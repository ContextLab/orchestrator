---
id: 003
title: LLM Quality Review Infrastructure
epic: validate-all-example-pipelines-with-manual-checks
created: 2025-08-25T18:50:00Z
updated: 2025-08-29T12:41:56Zgithub: https://github.com/ContextLab/orchestrator/issues/301status: completed
priority: high
estimated_hours: 12
depends_on: [275, 276]
parallel: false
week: 2
github_issues: []
---

# LLM Quality Review Infrastructure

Build automated LLM-powered quality assessment system using Claude Sonnet 4 or ChatGPT-5 with vision capabilities to systematically review all pipeline outputs for production quality.

## Problem Statement

Currently there is no systematic quality assurance for pipeline outputs:

1. **No Automated Quality Review**: Manual inspection is time-intensive and inconsistent
2. **No Template Artifact Detection**: Unrendered `{{variables}}` slip through unnoticed
3. **No Content Quality Assessment**: Poor AI responses, debug text, incomplete content not caught
4. **No Visual Output Validation**: Images and charts not systematically reviewed
5. **No Production Standards**: No objective criteria for "production-quality" outputs

## Solution Architecture

### LLM Integration with Existing Credentials
**Leverage toolbox's existing credential management system**

```python
class LLMQualityReviewer:
    def __init__(self):
        # Use existing credential management (.env, GitHub secrets)
        self.credentials = self._load_from_existing_system()
        self.claude_client = self._initialize_claude_sonnet_4()
        self.gpt_client = self._initialize_chatgpt_5() 
        self.fallback_models = ['claude-3-sonnet', 'gpt-4-vision-preview']
        
    def _load_from_existing_system(self) -> dict:
        """Load API keys from toolbox's existing credential management"""
        # Integration point with existing .env and GitHub secrets
        return load_credentials_from_toolbox()
```

### Vision-Enabled Quality Assessment
**Support for both text and visual content review**

```python
def review_pipeline_outputs(self, pipeline_name: str) -> QualityReview:
    """Comprehensive quality review with vision support"""
    outputs_path = f"examples/outputs/{pipeline_name}/"
    
    # 1. Scan all output files
    files = self._scan_output_directory(outputs_path)
    
    # 2. Categorize by type (text, image, data)
    text_files = [f for f in files if f.endswith(('.md', '.txt', '.csv', '.json'))]
    image_files = [f for f in files if f.endswith(('.png', '.jpg', '.jpeg', '.gif'))]
    
    # 3. Review each category with appropriate model capabilities
    text_reviews = self._review_text_files(text_files)
    visual_reviews = self._review_visual_files(image_files)  # Vision model
    
    return self._compile_quality_report(text_reviews, visual_reviews)
```

## Quality Assessment Framework

### Critical Quality Checks

#### 1. Template Resolution Validation
```python
def check_template_artifacts(self, content: str) -> List[Issue]:
    """Detect unrendered template variables"""
    issues = []
    
    # Check for unrendered Jinja2 templates
    jinja_pattern = r'\{\{[^}]+\}\}'
    matches = re.findall(jinja_pattern, content)
    for match in matches:
        issues.append(Issue(
            type="CRITICAL",
            category="template_artifact", 
            description=f"Unrendered template variable: {match}",
            severity="critical"
        ))
    
    # Check for other template systems
    # {{variable}}, ${variable}, %{variable}%, etc.
    
    return issues
```

#### 2. Content Quality Assessment  
```python
def assess_content_quality(self, content: str, file_path: str) -> ContentQuality:
    """LLM-powered content quality assessment"""
    
    prompt = f"""
    Review this content for production quality:
    
    FILE: {file_path}
    CONTENT: {content}
    
    Assess for:
    1. CRITICAL ISSUES (must fix):
       - Unrendered templates ({{variable}})
       - Debug/conversational text ("Certainly!", "Here's the...")
       - Incomplete content (cut-off text, partial responses)
       - Inaccurate or hallucinated information
       - Generic placeholder text
    
    2. PROFESSIONAL STANDARDS:
       - Clear, professional formatting
       - Complete and accurate information
       - Appropriate for showcasing platform capabilities
       - Demonstrates intended functionality clearly
    
    Rate: CRITICAL / MAJOR / MINOR / ACCEPTABLE
    Provide specific feedback for any issues found.
    """
    
    return self.llm_client.assess_quality(prompt)
```

#### 3. File Organization Validation
```python
def validate_file_organization(self, pipeline_name: str) -> OrganizationReview:
    """Validate file locations and naming conventions"""
    
    expected_path = f"examples/outputs/{pipeline_name}/"
    issues = []
    
    # Check file locations
    if not os.path.exists(expected_path):
        issues.append(Issue(
            type="CRITICAL",
            description=f"Output directory missing: {expected_path}"
        ))
        
    # Check naming conventions  
    for file in os.listdir(expected_path):
        if file.startswith('output') or file.startswith('result'):
            issues.append(Issue(
                type="MAJOR", 
                description=f"Generic filename: {file} should be input-specific"
            ))
            
    return OrganizationReview(issues)
```

#### 4. Visual Content Assessment (Vision Models)
```python
def assess_visual_content(self, image_path: str) -> VisualQuality:
    """Vision model assessment of images and charts"""
    
    prompt = """
    Analyze this image for production quality:
    
    QUALITY CHECKS:
    1. Image renders correctly (no corruption, clear visibility)
    2. Charts have proper labels, legends, readable text
    3. Visual quality is professional-grade
    4. Content matches expected visualization type
    5. Colors and styling are appropriate
    6. No artifacts or rendering errors
    
    RATE: EXCELLENT / GOOD / NEEDS_IMPROVEMENT / POOR
    Provide specific feedback on visual quality issues.
    """
    
    # Use vision-capable model (Claude Sonnet 4 or GPT-5 with vision)
    return self.vision_model.analyze_image(image_path, prompt)
```

## Quality Review Prompt Templates

### Comprehensive Quality Assessment Prompt
```python
QUALITY_REVIEW_PROMPT = """
You are reviewing outputs from the {pipeline_name} pipeline for production quality.

Review ALL files in examples/outputs/{pipeline_name}/ and assess:

CRITICAL ISSUES (must be fixed):
- Unrendered templates: {{variable_name}} or similar artifacts
- Debug/conversational text: "Certainly!", "Here's the...", "I'll help you..."
- Incomplete content: Cut-off text, partial responses, truncated output
- Incorrect locations: Files not in examples/outputs/{pipeline_name}/
- Generic naming: Files named "output.csv", "result.txt" instead of input-specific names
- Poor quality content: Inaccurate, hallucinated, or incomplete information
- Missing expected outputs: Files that should exist but don't

PRODUCTION QUALITY ASSESSMENT:
- Professional formatting and presentation
- Accurate and complete content
- Clear demonstration of intended functionality
- Appropriate for showcasing platform capabilities
- Proper file organization and naming
- No debugging artifacts or temporary content

For visual outputs (images, charts):
- Images render correctly without corruption
- Charts are readable with proper labels and legends
- Visual quality is professional-grade
- Content matches expected visualization type
- Colors and styling are appropriate

ANALYSIS REQUIREMENTS:
- Examine EVERY file in the output directory
- Provide specific feedback on each file
- Rate each file: CRITICAL / MAJOR / MINOR / ACCEPTABLE
- Give overall pipeline quality score: 0-100
- List specific fixes needed for any issues

OUTPUT FORMAT:
```json
{
  "pipeline_name": "{pipeline_name}",
  "overall_score": 85,
  "files_reviewed": ["file1.csv", "file2.md", ...],
  "critical_issues": [...],
  "major_issues": [...], 
  "minor_issues": [...],
  "recommendations": [...],
  "production_ready": true/false
}
```
"""
```

## Implementation Tasks

### Phase 1: Core Infrastructure
- [ ] **Credential Integration**: Connect to existing toolbox credential management
- [ ] **Model Client Setup**: Initialize Claude Sonnet 4 and ChatGPT-5 clients
- [ ] **Vision Model Integration**: Set up image analysis capabilities
- [ ] **Quality Framework**: Build core quality assessment classes

### Phase 2: Quality Check Implementation  
- [ ] **Template Artifact Detection**: Implement unrendered template detection
- [ ] **Content Quality Assessment**: Build LLM-powered content review
- [ ] **File Organization Validation**: Check locations and naming conventions
- [ ] **Visual Quality Assessment**: Implement vision model image review

### Phase 3: Review Pipeline Integration
- [ ] **Batch Review System**: Process all files in pipeline output directory
- [ ] **Report Generation**: Create structured quality reports
- [ ] **Scoring System**: Implement 0-100 quality scoring
- [ ] **Issue Classification**: Categorize issues by severity

### Phase 4: Automation and Integration
- [ ] **API Integration**: Connect to existing test framework
- [ ] **Parallel Processing**: Review multiple pipelines concurrently  
- [ ] **Caching System**: Cache results to avoid duplicate reviews
- [ ] **Error Handling**: Robust handling of API failures and edge cases

## Testing Strategy

### Model Integration Testing
```python
def test_llm_quality_review():
    """Test LLM quality review with known good/bad examples"""
    
    # Test with content containing template artifacts
    bad_content = "Processing {{filename}} with {{model_name}}"
    review = reviewer.assess_content_quality(bad_content, "test.md")
    assert review.has_critical_issues()
    assert "template_artifact" in review.issue_types
    
    # Test with good content
    good_content = "Processing sample_data.csv with Claude Sonnet 4"
    review = reviewer.assess_content_quality(good_content, "test.md")
    assert not review.has_critical_issues()
```

### Vision Model Testing
```python
def test_visual_quality_assessment():
    """Test vision model image quality assessment"""
    
    # Test with sample chart image
    chart_path = "examples/test_images/sample_chart.png"
    visual_review = reviewer.assess_visual_content(chart_path)
    
    assert visual_review.rating in ["EXCELLENT", "GOOD", "NEEDS_IMPROVEMENT", "POOR"]
    assert visual_review.has_feedback()
```

### Integration Testing  
```python
def test_full_pipeline_review():
    """Test complete pipeline quality review"""
    
    pipeline_name = "simple_data_processing"
    review = reviewer.review_pipeline_outputs(pipeline_name)
    
    assert review.overall_score >= 0 and review.overall_score <= 100
    assert len(review.files_reviewed) > 0
    assert review.production_ready in [True, False]
```

## Quality Metrics and Scoring

### Scoring Algorithm
```python
def calculate_quality_score(self, issues: List[Issue]) -> int:
    """Calculate 0-100 quality score based on issues found"""
    
    base_score = 100
    
    # Deduct points by severity
    for issue in issues:
        if issue.severity == "critical":
            base_score -= 25
        elif issue.severity == "major": 
            base_score -= 10
        elif issue.severity == "minor":
            base_score -= 3
            
    return max(0, base_score)
```

### Production Readiness Criteria
- **Score >= 90**: Production ready, no issues
- **Score 80-89**: Minor issues, acceptable for showcase
- **Score 70-79**: Some issues, needs improvement before release
- **Score 60-69**: Major issues, significant work needed
- **Score < 60**: Not suitable for production, critical fixes required

## Integration Points

### Existing System Integration
- **Credential Management**: Use existing .env and GitHub secrets handling
- **Test Framework**: Integrate quality reviews with pipeline tests
- **Template System**: Coordinate with template resolution fixes (Task 001)
- **Repository Structure**: Work with cleaned repository organization (Task 002)

### API and Model Configuration
- **Primary Models**: Claude Sonnet 4 (text + vision), ChatGPT-5 (text + vision)
- **Fallback Models**: Claude 3 Sonnet, GPT-4 Vision Preview
- **Rate Limiting**: Respect API rate limits, implement backoff strategies
- **Cost Management**: Monitor API usage, optimize prompt efficiency

## Success Criteria

### Infrastructure Success
- ✅ **Model Integration**: Claude Sonnet 4 and ChatGPT-5 operational via existing credentials
- ✅ **Vision Capabilities**: Image and chart analysis working
- ✅ **Quality Framework**: Comprehensive quality assessment operational
- ✅ **Batch Processing**: Can review multiple pipelines efficiently

### Quality Assessment Success  
- ✅ **Template Detection**: 100% accuracy detecting unrendered templates
- ✅ **Content Quality**: Reliable identification of debug artifacts and poor content
- ✅ **Visual Quality**: Effective assessment of images and charts
- ✅ **File Organization**: Accurate validation of locations and naming

### Production Integration Success
- ✅ **API Reliability**: Stable integration with LLM services
- ✅ **Report Generation**: Clear, actionable quality reports
- ✅ **Scoring Accuracy**: Quality scores correlate with human assessment
- ✅ **Performance**: Reviews complete in reasonable time (<5 min per pipeline)

## Expected Impact

### Quality Improvement
- **Objective Assessment**: Consistent, systematic quality evaluation
- **Early Detection**: Catch quality issues before users see them
- **Professional Standards**: Ensure all outputs meet production quality
- **Continuous Monitoring**: Automated quality assurance prevents regression

### Development Efficiency
- **Automated Review**: Reduce manual quality inspection time
- **Clear Feedback**: Specific, actionable improvement recommendations  
- **Quality Metrics**: Objective measures of example quality
- **Release Confidence**: High-quality examples enhance platform credibility

This infrastructure enables systematic, automated quality assurance for all pipeline outputs while integrating seamlessly with existing toolbox systems.
