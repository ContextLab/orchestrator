---
id: 004
title: Pipeline Testing Infrastructure
epic: validate-all-example-pipelines-with-manual-checks
created: 2025-08-25T18:55:00Z
updated: 2025-08-25T23:45:46Z
github: https://github.com/ContextLab/orchestrator/issues/281
status: pending
priority: high
estimated_hours: 10
depends_on: [275, 276, 277]
parallel: false
week: 2
github_issues: []
---

# Pipeline Testing Infrastructure

Build automated testing infrastructure for all example pipelines, integrating with existing test framework to enable systematic validation and regression testing before releases.

## Problem Statement

Currently there is no systematic testing framework for example pipelines:

1. **No Automated Pipeline Testing**: Example pipelines not included in test suite
2. **No Regression Protection**: Changes can break examples without detection
3. **No Release Validation**: No requirement to test examples before releases
4. **No Performance Monitoring**: Pipeline execution times not tracked
5. **No Integration with CI/CD**: Pipeline tests not part of automated workflows

## Solution Architecture

### Integration with Existing Test Framework
**Build on current test infrastructure rather than creating new systems**

```python
class PipelineTestSuite:
    def __init__(self):
        # Integrate with existing test framework
        self.test_runner = self._get_existing_test_runner()
        self.quality_reviewer = LLMQualityReviewer()  # From Task 003
        self.template_resolver = UnifiedTemplateResolver()  # From Task 001
        
    def run_pipeline_tests(self, pipeline_list: List[str] = None) -> TestResults:
        """Run comprehensive pipeline test suite"""
        pipelines = pipeline_list or self._discover_all_pipelines()
        
        results = {}
        for pipeline in pipelines:
            results[pipeline] = self._test_pipeline_comprehensive(pipeline)
            
        return TestResults(results)
```

### Comprehensive Pipeline Testing
**Multi-layered validation approach**

```python
def _test_pipeline_comprehensive(self, pipeline_name: str) -> PipelineTestResult:
    """Comprehensive pipeline testing with multiple validation layers"""
    
    # 1. Execution Testing
    execution_result = self._test_pipeline_execution(pipeline_name)
    
    # 2. Template Resolution Validation  
    template_result = self._test_template_resolution(pipeline_name)
    
    # 3. Output Quality Review (LLM-powered from Task 003)
    quality_result = self._test_output_quality(pipeline_name)
    
    # 4. File Organization Validation
    organization_result = self._test_file_organization(pipeline_name)
    
    # 5. Performance Monitoring
    performance_result = self._test_performance_metrics(pipeline_name)
    
    return PipelineTestResult(
        pipeline_name=pipeline_name,
        execution=execution_result,
        templates=template_result, 
        quality=quality_result,
        organization=organization_result,
        performance=performance_result
    )
```

## Testing Framework Components

### 1. Pipeline Execution Testing
```python
def _test_pipeline_execution(self, pipeline_name: str) -> ExecutionResult:
    """Test pipeline executes without errors"""
    
    pipeline_path = f"examples/{pipeline_name}.yaml"
    
    try:
        # Execute pipeline with proper inputs
        start_time = time.time()
        result = self._execute_pipeline_with_inputs(pipeline_path)
        execution_time = time.time() - start_time
        
        return ExecutionResult(
            success=True,
            execution_time=execution_time,
            outputs_generated=result.outputs,
            error=None
        )
        
    except Exception as e:
        return ExecutionResult(
            success=False,
            execution_time=None,
            outputs_generated=[],
            error=str(e)
        )
```

### 2. Template Resolution Testing
```python
def _test_template_resolution(self, pipeline_name: str) -> TemplateResult:
    """Test all templates resolve correctly (integrates with Task 001)"""
    
    # Check for unresolved templates in outputs
    output_path = f"examples/outputs/{pipeline_name}/"
    template_issues = []
    
    for file in self._get_output_files(output_path):
        content = self._read_file(file)
        
        # Check for template artifacts
        if '{{' in content or '}}' in content:
            template_issues.append(f"Unresolved template in {file}")
            
    return TemplateResult(
        resolved_correctly=len(template_issues) == 0,
        issues=template_issues
    )
```

### 3. Output Quality Testing (Integration with Task 003)
```python
def _test_output_quality(self, pipeline_name: str) -> QualityResult:
    """Test output quality using LLM review system"""
    
    # Use LLM Quality Reviewer from Task 003
    quality_review = self.quality_reviewer.review_pipeline_outputs(pipeline_name)
    
    return QualityResult(
        overall_score=quality_review.overall_score,
        production_ready=quality_review.production_ready,
        critical_issues=quality_review.critical_issues,
        recommendations=quality_review.recommendations
    )
```

### 4. File Organization Testing
```python
def _test_file_organization(self, pipeline_name: str) -> OrganizationResult:
    """Test file locations and naming conventions"""
    
    expected_output_dir = f"examples/outputs/{pipeline_name}/"
    issues = []
    
    # Check output directory exists
    if not os.path.exists(expected_output_dir):
        issues.append(f"Missing output directory: {expected_output_dir}")
        return OrganizationResult(valid=False, issues=issues)
    
    # Check file naming conventions
    for file in os.listdir(expected_output_dir):
        if file.startswith(('output', 'result', 'temp')):
            issues.append(f"Generic filename: {file} (should be input-specific)")
            
    return OrganizationResult(
        valid=len(issues) == 0,
        issues=issues
    )
```

### 5. Performance Monitoring
```python
def _test_performance_metrics(self, pipeline_name: str) -> PerformanceResult:
    """Monitor pipeline execution performance"""
    
    # Track execution metrics
    metrics = {
        'execution_time': self._measure_execution_time(pipeline_name),
        'memory_usage': self._measure_memory_usage(pipeline_name),
        'api_calls': self._count_api_calls(pipeline_name),
        'output_size': self._measure_output_size(pipeline_name)
    }
    
    # Compare against historical data
    historical = self._get_historical_metrics(pipeline_name)
    performance_regression = self._detect_regression(metrics, historical)
    
    return PerformanceResult(
        metrics=metrics,
        regression_detected=performance_regression,
        historical_comparison=historical
    )
```

## Test Configuration and Discovery

### Pipeline Discovery System
```python
def _discover_all_pipelines(self) -> List[str]:
    """Discover all example pipelines automatically"""
    
    example_dir = "examples/"
    pipelines = []
    
    for file in os.listdir(example_dir):
        if file.endswith('.yaml') and not file.startswith('test_'):
            pipeline_name = file.replace('.yaml', '')
            pipelines.append(pipeline_name)
            
    return sorted(pipelines)
```

### Test Input Management
```python
def _get_test_inputs_for_pipeline(self, pipeline_name: str) -> dict:
    """Get appropriate test inputs for each pipeline type"""
    
    # Default test inputs by pipeline category
    test_inputs = {
        # Data processing pipelines
        'simple_data_processing': {'input_file': 'examples/data/sample_data.csv'},
        'data_processing_pipeline': {'data_file': 'examples/data/sales_data.csv'},
        'statistical_analysis': {'dataset': 'examples/data/test_data.csv'},
        
        # Research pipelines  
        'research_minimal': {'topic': 'artificial intelligence'},
        'research_basic': {'research_query': 'quantum computing applications'},
        'web_research_pipeline': {'search_terms': ['machine learning', 'neural networks']},
        
        # Creative pipelines
        'creative_image_pipeline': {'description': 'futuristic city with flying cars'},
        'multimodal_processing': {'input_image': 'examples/data/test_image.jpg'},
        
        # Control flow pipelines
        'control_flow_conditional': {'threshold': 50},
        'control_flow_for_loop': {'items': ['file1.txt', 'file2.txt', 'file3.txt']},
        'control_flow_while_loop': {'target_number': 42}
    }
    
    return test_inputs.get(pipeline_name, {})
```

## Test Suite Integration

### Integration with Existing Test Framework
```python
# Add to existing test suite
class TestExamplePipelines(unittest.TestCase):
    
    def setUp(self):
        self.pipeline_tester = PipelineTestSuite()
        
    def test_all_pipelines_execute_successfully(self):
        """Test all example pipelines execute without errors"""
        results = self.pipeline_tester.run_pipeline_tests()
        
        failed_pipelines = [name for name, result in results.items() 
                           if not result.execution.success]
        
        self.assertEqual([], failed_pipelines, 
                        f"Pipelines failed execution: {failed_pipelines}")
    
    def test_all_pipelines_have_quality_outputs(self):
        """Test all pipelines produce quality outputs (LLM review)"""
        results = self.pipeline_tester.run_pipeline_tests()
        
        low_quality = [name for name, result in results.items()
                      if result.quality.overall_score < 80]
        
        self.assertEqual([], low_quality,
                        f"Pipelines with low quality scores: {low_quality}")
    
    def test_no_template_artifacts_in_outputs(self):
        """Test no unresolved templates in any outputs"""  
        results = self.pipeline_tester.run_pipeline_tests()
        
        template_issues = [name for name, result in results.items()
                          if not result.templates.resolved_correctly]
        
        self.assertEqual([], template_issues,
                        f"Pipelines with template issues: {template_issues}")
```

### Test Execution Modes

#### 1. Full Test Suite (Release Testing)
```python
def run_full_pipeline_test_suite():
    """Run comprehensive tests on all pipelines"""
    tester = PipelineTestSuite()
    return tester.run_pipeline_tests()  # All pipelines
```

#### 2. Quick Validation (Development)  
```python
def run_quick_pipeline_tests():
    """Run basic tests on core pipelines"""
    core_pipelines = [
        'simple_data_processing',
        'research_minimal', 
        'control_flow_conditional',
        'creative_image_pipeline'
    ]
    
    tester = PipelineTestSuite()
    return tester.run_pipeline_tests(core_pipelines)
```

#### 3. Single Pipeline Testing
```python  
def test_single_pipeline(pipeline_name: str):
    """Test specific pipeline in detail"""
    tester = PipelineTestSuite()
    return tester.run_pipeline_tests([pipeline_name])
```

## CI/CD Integration

### Test Automation Configuration
```python
# pytest configuration for pipeline tests
class PipelineTestConfig:
    """Configuration for automated pipeline testing"""
    
    # Test markers for different execution modes
    MARKERS = {
        'pipeline_full': 'Run all pipeline tests (slow)',
        'pipeline_core': 'Run core pipeline tests (medium)', 
        'pipeline_quick': 'Run basic pipeline validation (fast)'
    }
    
    # Test timeouts
    TIMEOUTS = {
        'full_suite': 3600,  # 1 hour for all pipelines
        'core_tests': 1800,  # 30 minutes for core pipelines
        'quick_tests': 300   # 5 minutes for quick validation
    }
```

### Release Testing Requirements
```python
def pre_release_pipeline_validation():
    """Required pipeline tests before any release"""
    
    # 1. All pipelines must execute successfully
    execution_results = test_all_pipeline_execution()
    assert execution_results.all_passed(), "Pipeline execution failures detected"
    
    # 2. All outputs must pass quality review
    quality_results = test_all_pipeline_quality() 
    assert quality_results.average_score >= 85, "Quality standards not met"
    
    # 3. No template resolution issues
    template_results = test_all_template_resolution()
    assert template_results.all_resolved(), "Template resolution issues detected"
    
    # 4. File organization compliance
    organization_results = test_all_file_organization()
    assert organization_results.all_compliant(), "File organization issues detected"
    
    return "All pipeline tests passed - ready for release"
```

## Testing Implementation Tasks

### Phase 1: Core Test Framework
- [ ] **Test Discovery**: Automatic pipeline discovery and categorization
- [ ] **Execution Testing**: Basic pipeline execution validation
- [ ] **Integration Setup**: Connect to existing test framework
- [ ] **Test Configuration**: Set up test inputs and parameters

### Phase 2: Quality Integration
- [ ] **LLM Quality Integration**: Connect to quality review system (Task 003)
- [ ] **Template Testing**: Integrate template resolution validation (Task 001)
- [ ] **Organization Testing**: File location and naming validation
- [ ] **Performance Monitoring**: Execution time and resource tracking

### Phase 3: Test Suite Implementation
- [ ] **Full Test Suite**: Comprehensive testing of all pipelines
- [ ] **Core Test Suite**: Essential pipeline validation
- [ ] **Quick Test Suite**: Fast development validation
- [ ] **Single Pipeline Testing**: Detailed individual pipeline testing

### Phase 4: CI/CD Integration
- [ ] **Automated Testing**: Integration with existing CI/CD pipeline
- [ ] **Release Requirements**: Pre-release testing requirements
- [ ] **Performance Tracking**: Historical performance monitoring
- [ ] **Regression Detection**: Automated performance regression detection

## Success Criteria

### Testing Infrastructure Success
- ✅ **All Pipelines Discovered**: Automatic discovery of all 37+ example pipelines
- ✅ **Execution Testing**: 100% of pipelines tested for successful execution
- ✅ **Quality Integration**: LLM quality reviews integrated into test suite
- ✅ **Template Validation**: All template resolution issues detected

### CI/CD Integration Success
- ✅ **Automated Execution**: Pipeline tests run automatically in CI/CD
- ✅ **Release Blocking**: Failed pipeline tests block releases
- ✅ **Performance Monitoring**: Execution metrics tracked over time
- ✅ **Regression Detection**: Performance regressions detected automatically

### Quality Assurance Success  
- ✅ **No Failed Executions**: 100% pipeline execution success rate
- ✅ **Quality Standards Met**: Average quality score >= 85
- ✅ **No Template Issues**: Zero unresolved template artifacts
- ✅ **Organization Compliance**: 100% file organization compliance

## Expected Impact

### Development Quality
- **Catch Regressions Early**: Pipeline issues detected before users encounter them
- **Release Confidence**: Systematic validation ensures example quality
- **Performance Monitoring**: Track and optimize pipeline execution over time
- **Automated Validation**: Reduce manual testing time and human error

### User Experience  
- **Reliable Examples**: Users can count on examples working correctly
- **Professional Quality**: All outputs meet production standards
- **Consistent Performance**: Predictable execution times and resource usage
- **Better Documentation**: Test results inform tutorial quality

This testing infrastructure ensures all example pipelines remain functional and high-quality through automated validation and continuous monitoring.
