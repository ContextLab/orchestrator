"""
Real User Journey Tests - Complete End-to-End Testing

This test suite validates the complete user experience with real infrastructure,
exactly as documented and used by actual users.
"""

import pytest
import os
import sys
import asyncio
import tempfile
import subprocess
from pathlib import Path

from orchestrator import Orchestrator, init_models


class TestNewUserSetupToExecution:
    """Test complete user journeys with real infrastructure."""
    
    async def test_new_user_complete_setup(self):
        """Test complete new user experience with real setup."""
        
        # Ensure we have at least one API key
        available_keys = [k for k in ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'GOOGLE_API_KEY'] if os.getenv(k)]
        if not available_keys:
            pytest.skip("No API keys available for user journey test")
        
        # Test user following documentation exactly
        test_code = '''
import os
import asyncio
from orchestrator import Orchestrator, init_models

async def main():
    print("Step 1: Initializing models (as per docs)...")
    # Exactly what docs tell users to do
    registry = init_models()
    print(f"Models available: {registry.list_models()}")
    
    if not registry.list_models():
        print("ERROR: No models available")
        return 1
    
    print("Step 2: Creating orchestrator...")
    orchestrator = Orchestrator(model_registry=registry)
    
    print("Step 3: Defining pipeline (from quickstart guide)...")
    pipeline_yaml = """
    name: User Test Pipeline
    parameters:
      topic: "quantum computing"
    steps:
      - id: research
        action: generate_text
        parameters:
          prompt: "Explain {{ topic }} in simple terms"
          max_tokens: 100
      - id: save
        tool: filesystem
        action: write
        parameters:
          path: "/tmp/user_test_{{ topic | slugify }}.md"
          content: |
            # {{ topic | title }}
            
            Generated: {{ execution.timestamp }}
            
            ## Overview
            {{ research.result }}
            
            ---
            *Report generated by Orchestrator*
    """
    
    print("Step 4: Executing pipeline...")
    result = await orchestrator.execute_yaml(pipeline_yaml)
    
    print("Step 5: Validating output...")
    if not result:
        print("ERROR: Pipeline returned no result")
        return 1
    
    # Check output file
    output_file = "/tmp/user_test_quantum-computing.md"
    if not os.path.exists(output_file):
        print(f"ERROR: Output file not created: {output_file}")
        return 1
    
    with open(output_file, 'r') as f:
        content = f.read()
    
    # Validate template rendering
    if '{{' in content:
        print(f"ERROR: Unrendered templates found: {content}")
        return 1
    
    if 'quantum computing' not in content.lower():
        print(f"ERROR: Expected content not found: {content}")
        return 1
    
    print("SUCCESS: User journey completed successfully!")
    print(f"Output file created: {output_file}")
    print(f"Content length: {len(content)} characters")
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))
        '''
        
        # Write test code to temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(test_code)
            test_file = f.name
        
        try:
            # Execute as separate process (simulates new user)
            proc = await asyncio.create_subprocess_exec(
                sys.executable, test_file,
                env={**os.environ, 'PYTHONPATH': str(Path.cwd() / 'src')},
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=Path.cwd()
            )
            
            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=120)
            
            # Validate success
            if proc.returncode != 0:
                pytest.fail(f"User journey failed: {stdout.decode()} {stderr.decode()}")
            
            assert b"SUCCESS: User journey completed" in stdout
            
            # Validate output file exists and is correct
            output_file = Path("/tmp/user_test_quantum-computing.md")
            assert output_file.exists(), "User test output file not created"
            
            content = output_file.read_text()
            assert '{{' not in content, "Templates not rendered in user output"
            assert 'quantum computing' in content.lower(), "Expected content missing"
            assert 'Generated:' in content, "Execution timestamp missing"
            
            # Cleanup
            output_file.unlink()
            
        finally:
            # Cleanup temp file
            Path(test_file).unlink()

    async def test_cli_user_workflow_complete(self):
        """Test complete CLI workflow that users follow."""
        
        # Test pipeline files that should exist
        test_pipelines = [
            "examples/research_minimal.yaml",
            "examples/research_basic.yaml"
        ]
        
        available_pipeline = None
        for pipeline in test_pipelines:
            if Path(pipeline).exists():
                available_pipeline = pipeline
                break
        
        if not available_pipeline:
            pytest.skip(f"No test pipelines found: {test_pipelines}")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            print(f"Testing CLI with pipeline: {available_pipeline}")
            
            # Execute CLI command exactly as users do
            cli_commands = [
                # Basic execution
                [
                    sys.executable, "scripts/run_pipeline.py", 
                    available_pipeline,
                    "-i", "topic=CLI workflow testing",
                    "-o", temp_dir
                ],
                # With input file
                # TODO: Add input file test when we have example input files
            ]
            
            for cmd in cli_commands:
                print(f"Running: {' '.join(cmd)}")
                
                proc = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env={**os.environ, 'PYTHONPATH': str(Path.cwd() / 'src')},
                    cwd=Path.cwd()
                )
                
                try:
                    stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=180)
                except asyncio.TimeoutError:
                    proc.kill()
                    pytest.fail(f"CLI command timed out: {' '.join(cmd)}")
                
                # Validate CLI success
                if proc.returncode != 0:
                    error_msg = f"CLI command failed: {' '.join(cmd)}\n"
                    error_msg += f"STDOUT: {stdout.decode()}\n"
                    error_msg += f"STDERR: {stderr.decode()}"
                    pytest.fail(error_msg)
                
                # Validate output was created
                output_files = list(Path(temp_dir).glob("*.md"))
                assert len(output_files) > 0, f"No output files created by CLI in {temp_dir}"
                
                # Validate template rendering
                for output_file in output_files:
                    content = output_file.read_text()
                    assert '{{' not in content, f"Unrendered templates in CLI output: {output_file}"
                    assert 'cli workflow testing' in content.lower(), f"Expected content missing in {output_file}"

    async def test_error_scenarios_real_user_experience(self):
        """Test error scenarios that real users encounter."""
        
        # Test 1: Missing API keys
        original_keys = {}
        api_keys = ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'GOOGLE_API_KEY']
        
        # Temporarily remove all API keys
        for key in api_keys:
            original_keys[key] = os.environ.pop(key, None)
        
        try:
            # This should produce user-friendly error
            error_test_code = '''
import asyncio
from orchestrator import init_models, Orchestrator

async def main():
    try:
        registry = init_models()
        if not registry.list_models():
            print("ERROR: No models available - likely missing API keys")
            return 1
        
        orchestrator = Orchestrator(model_registry=registry)
        result = await orchestrator.execute_yaml("""
        steps:
          - id: test
            action: generate_text
            parameters:
              prompt: "test"
        """)
        
        print("UNEXPECTED: Should have failed with missing API keys")
        return 1
        
    except Exception as e:
        print(f"EXPECTED_ERROR: {e}")
        return 0

if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))
            '''
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(error_test_code)
                test_file = f.name
            
            try:
                proc = await asyncio.create_subprocess_exec(
                    sys.executable, test_file,
                    env={**os.environ, 'PYTHONPATH': str(Path.cwd() / 'src')},
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd=Path.cwd()
                )
                
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=30)
                
                # Should succeed with expected error
                output = stdout.decode() + stderr.decode()
                assert "No models available" in output or "EXPECTED_ERROR" in output
                
            finally:
                Path(test_file).unlink()
            
        finally:
            # Restore original API keys
            for key, value in original_keys.items():
                if value is not None:
                    os.environ[key] = value

    async def test_real_pipeline_stress_test(self):
        """Stress test with multiple real pipeline executions."""
        
        # Find available pipeline
        test_pipeline = "examples/research_minimal.yaml"
        if not Path(test_pipeline).exists():
            pytest.skip(f"Test pipeline not found: {test_pipeline}")
        
        # Run multiple executions sequentially (to avoid rate limits)
        topics = [
            "artificial intelligence",
            "renewable energy", 
            "space exploration"
        ]
        
        with tempfile.TemporaryDirectory() as temp_dir:
            for i, topic in enumerate(topics):
                output_subdir = Path(temp_dir) / f"test_{i}"
                output_subdir.mkdir()
                
                print(f"Running stress test {i+1}/{len(topics)}: {topic}")
                
                proc = await asyncio.create_subprocess_exec(
                    sys.executable, "scripts/run_pipeline.py",
                    test_pipeline,
                    "-i", f"topic={topic}",
                    "-o", str(output_subdir),
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    env={**os.environ, 'PYTHONPATH': str(Path.cwd() / 'src')},
                    cwd=Path.cwd()
                )
                
                try:
                    stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=120)
                except asyncio.TimeoutError:
                    proc.kill()
                    pytest.fail(f"Stress test {i} timed out for topic: {topic}")
                
                if proc.returncode != 0:
                    pytest.fail(f"Stress test {i} failed for {topic}: {stderr.decode()}")
                
                # Validate output
                output_files = list(output_subdir.glob("*.md"))
                assert len(output_files) > 0, f"No output for stress test {i}"
                
                for output_file in output_files:
                    content = output_file.read_text()
                    assert '{{' not in content, f"Unrendered templates in stress test {i}"
                    assert topic.lower() in content.lower(), f"Topic missing in stress test {i}"
                
                # Small delay to be nice to APIs
                await asyncio.sleep(2)


class TestRealInfrastructureValidation:
    """Validate real infrastructure components."""
    
    async def test_model_registry_real_initialization(self):
        """Test that model registry initializes with real models."""
        registry = init_models()
        
        models = registry.list_models()
        print(f"Available models: {models}")
        
        # Should have at least one model if API keys are configured
        if any(os.getenv(key) for key in ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'GOOGLE_API_KEY']):
            assert len(models) > 0, "No models available despite having API keys"
        
        # Test model selection
        if models:
            selected = registry.select_model(['generate'])
            assert selected is not None, "Could not select model for generation task"
            print(f"Selected model: {selected.name}")

    async def test_orchestrator_real_initialization(self):
        """Test orchestrator initialization with real models."""
        registry = init_models()
        
        if not registry.list_models():
            pytest.skip("No models available for orchestrator test")
        
        orchestrator = Orchestrator(model_registry=registry)
        assert orchestrator is not None
        
        # Test simple execution
        simple_yaml = '''
        steps:
          - id: test_init
            action: generate_text
            parameters:
              prompt: "Say 'initialization test successful'"
              max_tokens: 10
        '''
        
        result = await orchestrator.execute_yaml(simple_yaml)
        assert result is not None
        assert 'test_init' in result

    async def test_file_system_operations_real(self):
        """Test filesystem operations with real infrastructure."""
        registry = init_models()
        
        if not registry.list_models():
            pytest.skip("No models available for filesystem test")
        
        orchestrator = Orchestrator(model_registry=registry)
        
        with tempfile.TemporaryDirectory() as temp_dir:
            pipeline_yaml = f'''
            steps:
              - id: create_content
                action: generate_text
                parameters:
                  prompt: "Write a short test message about filesystem operations"
                  max_tokens: 50
                  
              - id: save_content
                tool: filesystem
                action: write
                parameters:
                  path: "{temp_dir}/filesystem_test.md"
                  content: |
                    # Filesystem Test
                    
                    Generated: {{{{ execution.timestamp }}}}
                    
                    Content: {{{{ create_content.result }}}}
            '''
            
            result = await orchestrator.execute_yaml(pipeline_yaml)
            assert result is not None
            
            # Check file was created
            output_file = Path(temp_dir) / "filesystem_test.md"
            assert output_file.exists(), "Filesystem operation failed to create file"
            
            content = output_file.read_text()
            assert "# Filesystem Test" in content
            assert "Generated:" in content
            assert "Content:" in content
            assert '{{' not in content, "Templates not rendered in filesystem output"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])