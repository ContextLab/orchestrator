"""Test code snippets from documentation to ensure they work correctly."""

import os
import tempfile
import pytest
import yaml


# Add the project root to the path
class TestReadmeCodeSnippets:
    """Test code snippets from README.md."""

    def test_hello_world_yaml(self):
        """Test the hello_world.yaml example from README."""
        yaml_content = """id: hello_world
name: Hello World Pipeline
description: A simple example pipeline

steps:
  - id: greet
    action: generate_text
    parameters:
      prompt: "Say hello to the world in a creative way!"

  - id: translate
    action: generate_text
    parameters:
      prompt: "Translate this greeting to Spanish: {{ greet.result }}"
    dependencies: [greet]

outputs:
  greeting: "{{ greet.result }}"
  spanish: "{{ translate.result }}"
"""

        # Validate YAML syntax
        parsed = yaml.safe_load(yaml_content)
        assert parsed["id"] == "hello_world"
        assert len(parsed["steps"]) == 2
        assert parsed["steps"][0]["id"] == "greet"
        assert parsed["steps"][1]["dependencies"] == ["greet"]
        assert "outputs" in parsed

    @pytest.mark.asyncio
    async def test_programmatic_usage(self, populated_model_registry):
        """Test the programmatic usage example from README."""
        # Create a temporary YAML file
        yaml_content = """id: hello_world
name: Hello World Pipeline
description: A simple example pipeline

steps:
  - id: greet
    action: generate_text
    parameters:
      prompt: "Say hello to the world in a creative way!"
      model: <AUTO>Choose a fast, creative model</AUTO>
      max_tokens: 100
"""

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(yaml_content)
            yaml_file = f.name

        try:
            # Test the import and basic usage
            import orchestrator as orc

            # The populated_model_registry fixture already initializes models
            # so we don't need to call orc.init_models()

            # Compile pipeline (use async version since we're in async function)
            pipeline = await orc.compile_async(yaml_file)
            assert pipeline is not None
            assert pipeline.pipeline.name == "Hello World Pipeline"

            # Run pipeline (with real model)
            result = await pipeline.run_async()
            assert result is not None
            assert isinstance(result, dict)

        finally:
            # Clean up
            os.unlink(yaml_file)

    def test_research_pipeline_yaml(self):
        """Test the research pipeline YAML example from README."""
        yaml_content = """id: research_report
name: Research Report Pipeline
description: Generate a comprehensive research report on any topic

steps:
  - id: research_topic
    action: web_search
    parameters:
      query: "{{ topic }} latest developments 2024"
      num_results: 10

  - id: analyze_results
    action: analyze_text
    parameters:
      text: "{{ research_topic.results }}"
      analysis_type: "key_findings"
      focus: "{{ instructions }}"
    dependencies: [research_topic]

  - id: generate_outline
    action: generate_text
    parameters:
      prompt: |
        Based on these key findings: {{ analyze_results.summary }}
        Create a detailed outline for a research report on {{ topic }}.
        Focus on: {{ instructions }}
      model: <AUTO>Choose best model for structured writing</AUTO>
    dependencies: [analyze_results]

  - id: write_sections
    action: generate_text
    parameters:
      prompt: |
        Write detailed content for this research report outline:
        {{ generate_outline.result }}

        Include citations and maintain academic tone.
      model: <AUTO>Choose best model for long-form writing</AUTO>
      max_tokens: 4000
    dependencies: [generate_outline]

  - id: write_report
    action: write_file
    parameters:
      path: "{{ output_dir }}/research_report_{{ topic | slugify }}.md"
      content: |
        # Research Report: {{ topic }}

        {{ write_sections.result }}

        ---
        Generated by Orchestrator Research Pipeline
    dependencies: [write_sections]

outputs:
  report_path: "{{ write_report.path }}"
  summary: "{{ analyze_results.summary }}"
"""

        # Validate YAML syntax
        parsed = yaml.safe_load(yaml_content)
        assert parsed["id"] == "research_report"
        assert len(parsed["steps"]) == 5

        # Check step dependencies
        assert "dependencies" not in parsed["steps"][0]  # research_topic has no deps
        assert parsed["steps"][1]["dependencies"] == ["research_topic"]
        assert parsed["steps"][2]["dependencies"] == ["analyze_results"]
        assert parsed["steps"][3]["dependencies"] == ["generate_outline"]
        assert parsed["steps"][4]["dependencies"] == ["write_sections"]

        # Check AUTO tags are present
        assert "<AUTO>" in parsed["steps"][2]["parameters"]["model"]
        assert "<AUTO>" in parsed["steps"][3]["parameters"]["model"]

    @pytest.mark.asyncio
    async def test_research_pipeline_execution(self, populated_model_registry):
        """Test executing the research pipeline example."""
        yaml_content = """id: simple_research
name: Simple Research Pipeline
description: Simplified research pipeline for testing

steps:
  - id: generate_summary
    action: generate_text
    parameters:
      prompt: |
        Write a brief summary about: {{ topic }}
        Focus on: {{ instructions }}
      model: <AUTO>Choose a fast model</AUTO>
      max_tokens: 200

outputs:
  summary: "{{ generate_summary.result }}"
"""

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(yaml_content)
            yaml_file = f.name

        try:
            import orchestrator as orc

            # Compile pipeline (use async version since we're in async function)
            pipeline = await orc.compile_async(yaml_file)

            # Run with inputs
            result = await pipeline.run_async(
                topic="quantum computing applications in medicine",
                instructions="Focus on recent breakthroughs and future potential")

            assert result is not None
            assert isinstance(result, dict)
            # Check if outputs are present and summary is in outputs
            assert "outputs" in result
            assert "summary" in result["outputs"]
            assert result["outputs"]["summary"]  # Ensure it's not empty

        finally:
            os.unlink(yaml_file)


class TestDesignDocExamples:
    """Test code examples from design.md."""

    def test_control_flow_yaml(self):
        """Test control flow YAML examples from design documentation."""
        yaml_content = """id: conditional_example
name: Conditional Execution Example

steps:
  - id: check_condition
    action: evaluate
    parameters:
      expression: "{{ input_value > 100 }}"

  - id: high_value_task
    action: generate_text
    parameters:
      prompt: "Process high value: {{ input_value }}"
    if: "{{ check_condition.result == true }}"

  - id: low_value_task
    action: generate_text
    parameters:
      prompt: "Process low value: {{ input_value }}"
    if: "{{ check_condition.result == false }}"
"""

        # Validate YAML syntax
        parsed = yaml.safe_load(yaml_content)
        assert parsed["id"] == "conditional_example"
        assert len(parsed["steps"]) == 3

        # Check conditional steps
        assert "if" in parsed["steps"][1]
        assert "if" in parsed["steps"][2]

    def test_loop_yaml(self):
        """Test loop YAML examples from design documentation."""
        yaml_content = """id: loop_example
name: Loop Execution Example

steps:
  - id: process_items
    action: for_each
    parameters:
      items: "{{ input_items }}"
      steps:
        - id: process_item
          action: generate_text
          parameters:
            prompt: "Process item: {{ item }}"
"""

        # Validate YAML syntax
        parsed = yaml.safe_load(yaml_content)
        assert parsed["id"] == "loop_example"
        assert parsed["steps"][0]["action"] == "for_each"
        assert "items" in parsed["steps"][0]["parameters"]
        assert "steps" in parsed["steps"][0]["parameters"]


class TestAutoTagExamples:
    """Test AUTO tag examples from documentation."""

    def test_auto_tag_in_yaml(self):
        """Test AUTO tag usage in YAML."""
        from src.orchestrator.compiler.auto_tag_yaml_parser import parse_yaml_with_auto_tags

        yaml_content = """id: auto_tag_example
name: AUTO Tag Example

steps:
  - id: analyze
    action: analyze
    parameters:
      method: <AUTO>Choose best analysis method for this data type</AUTO>
      depth: <AUTO>Determine analysis depth based on data complexity</AUTO>
      format: <AUTO>Select output format: json, yaml, or markdown</AUTO>
"""

        # Parse with AUTO tag support
        parsed = parse_yaml_with_auto_tags(yaml_content)

        assert parsed["id"] == "auto_tag_example"
        assert "<AUTO>" in parsed["steps"][0]["parameters"]["method"]
        assert "</AUTO>" in parsed["steps"][0]["parameters"]["method"]
        assert (
            "Choose best analysis method" in parsed["steps"][0]["parameters"]["method"]
        )

        # Verify all AUTO tags are preserved
        for param in ["method", "depth", "format"]:
            value = parsed["steps"][0]["parameters"][param]
            assert value.startswith("<AUTO>")
            assert value.endswith("</AUTO>")


class TestModelUsageExamples:
    """Test model usage examples from documentation."""

    @pytest.mark.asyncio
    async def test_model_selection(self, populated_model_registry):
        """Test model selection examples."""

        # Test selecting a model by requirements
        model = await populated_model_registry.select_model(
            {"tasks": ["generate"], "min_size": "1b", "expertise": ["general"]}
        )

        assert model is not None
        assert hasattr(model, "generate")
        assert model.capabilities.supported_tasks

    @pytest.mark.asyncio
    async def test_model_usage(self, populated_model_registry):
        """Test direct model usage examples."""
        # Get any available model
        models = populated_model_registry.list_models()
        if not models:
            pytest.skip("No models available")

        # Parse the model key to get provider and model name
        model_key = models[0]
        if ":" in model_key:
            provider, model_name = model_key.split(":", 1)
            model = populated_model_registry.get_model(model_name, provider)
        else:
            model = populated_model_registry.get_model(model_key)

        # Test generation
        result = await model.generate("Hello, world!", temperature=0.7, max_tokens=50)

        assert isinstance(result, str)
        assert len(result) > 0


class TestToolUsageExamples:
    """Test tool usage examples from documentation."""

    def test_tool_yaml_definition(self):
        """Test tool definition in YAML."""
        yaml_content = """id: tool_usage_example
name: Tool Usage Example

steps:
  - id: search_web
    action: web_search
    parameters:
      query: "latest AI developments"
      num_results: 5

  - id: analyze_image
    action: image_analysis
    parameters:
      image: "{{ image_path }}"
      analysis_type: "describe"

  - id: validate_data
    action: validate
    parameters:
      data: "{{ input_data }}"
      schema:
        type: object
        properties:
          name:
            type: string
          age:
            type: integer
"""

        # Validate YAML syntax
        parsed = yaml.safe_load(yaml_content)
        assert len(parsed["steps"]) == 3

        # Check tool actions
        assert parsed["steps"][0]["action"] == "web_search"
        assert parsed["steps"][1]["action"] == "image_analysis"
        assert parsed["steps"][2]["action"] == "validate"

        # Check parameters
        assert "query" in parsed["steps"][0]["parameters"]
        assert "analysis_type" in parsed["steps"][1]["parameters"]
        assert "schema" in parsed["steps"][2]["parameters"]


class TestDesignDocCodeSnippets:
    """Test Python code snippets from design.md."""

    def test_task_abstraction_interface(self):
        """Test that Task abstraction interface is correctly implemented."""
        from src.orchestrator.core.task import Task, TaskStatus

        # Create a task instance
        task = Task(
            id="test_task",
            name="Test Task",
            action="generate_text",
            parameters={"prompt": "Hello"},
            dependencies=[])

        # Verify interface
        assert hasattr(task, "id")
        assert hasattr(task, "name")
        assert hasattr(task, "action")
        assert hasattr(task, "parameters")
        assert hasattr(task, "dependencies")
        assert hasattr(task, "status")

        # Check initial status
        assert task.status == TaskStatus.PENDING

    def test_pipeline_abstraction_interface(self):
        """Test that Pipeline abstraction interface is correctly implemented."""
        from src.orchestrator.core.pipeline import Pipeline

        # Verify Pipeline class exists and has expected methods
        assert hasattr(Pipeline, "add_task")
        assert hasattr(Pipeline, "is_valid")  # Changed from 'validate'
        assert hasattr(Pipeline, "get_execution_order")

    def test_model_interface(self):
        """Test that Model interface is correctly implemented."""
        from src.orchestrator.core.model import Model, ModelCapabilities

        # Verify Model class has required methods
        assert hasattr(Model, "generate")
        assert hasattr(Model, "health_check")
        assert hasattr(Model, "is_available")  # Changed from 'validate_parameters'

        # Verify ModelCapabilities structure
        caps = ModelCapabilities(
            supported_tasks=["generate"]
        )  # Must have at least one task
        assert hasattr(caps, "supported_tasks")
        assert hasattr(caps, "max_tokens")
        assert hasattr(caps, "supports_streaming")
        assert hasattr(caps, "supports_structured_output")

    def test_yaml_compiler_interface(self):
        """Test that YAMLCompiler interface is correctly implemented."""
        from src.orchestrator.compiler.yaml_compiler import YAMLCompiler

        # Verify YAMLCompiler has expected methods
        assert hasattr(YAMLCompiler, "compile")
        assert hasattr(YAMLCompiler, "validate_yaml")  # Changed from 'validate'
        assert hasattr(
            YAMLCompiler, "detect_auto_tags"
        )  # Changed from '_resolve_ambiguities'

    def test_error_handling_hierarchy(self):
        """Test error handling class hierarchy from design."""
        from src.orchestrator.core.exceptions import (
            OrchestratorError,
            PipelineError,
            TaskError,
            ModelError,
            ValidationError,
            ResourceError,
            StateError,
            ToolError,
            ControlSystemError,
            CompilationError,
            NetworkError,
            ConfigurationError,
            get_error_hierarchy
        )
        
        # Test that base error exists and is an Exception
        assert issubclass(OrchestratorError, Exception)
        
        # Test main error categories exist and inherit from base
        error_categories = [
            PipelineError, TaskError, ModelError, ValidationError,
            ResourceError, StateError, ToolError, ControlSystemError,
            CompilationError, NetworkError, ConfigurationError
        ]
        
        for error_class in error_categories:
            assert issubclass(error_class, OrchestratorError)
            
        # Test that we can create and raise errors
        try:
            raise PipelineError("Test pipeline error")
        except OrchestratorError as e:
            assert str(e) == "Test pipeline error"
            
        # Test error hierarchy helper function
        hierarchy = get_error_hierarchy()
        assert isinstance(hierarchy, dict)
        assert "OrchestratorError" in hierarchy

    @pytest.mark.asyncio
    async def test_model_registry_interface(self, populated_model_registry):
        """Test ModelRegistry interface from design."""

        registry = populated_model_registry

        # Verify interface methods
        assert hasattr(registry, "register_model")
        assert hasattr(registry, "get_model")
        assert hasattr(registry, "select_model")
        assert hasattr(registry, "list_models")
        assert hasattr(registry, "update_model_performance")

        # Test selection
        model = await registry.select_model({"tasks": ["generate"]})
        assert model is not None


if __name__ == "__main__":
    # Run tests
    pytest.main([__file__, "-v"])
