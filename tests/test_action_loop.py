"""Comprehensive test suite for action_loop functionality - NO MOCKS, real functionality only."""

import asyncio
import json
import tempfile
from pathlib import Path
from typing import Any, Dict, List

import pytest

from src.orchestrator.core.action_loop_task import ActionLoopTask
from src.orchestrator.core.action_loop_context import ActionResult, EnhancedLoopContext
from src.orchestrator.control_flow.action_loop_handler import ActionLoopHandler
from src.orchestrator.tools.base import default_registry
from src.orchestrator.control_flow.auto_resolver import ControlFlowAutoResolver
from src.orchestrator.core.template_manager import TemplateManager
from src.orchestrator.models import get_model_registry
from src.orchestrator.compiler.control_flow_compiler import ControlFlowCompiler
from src.orchestrator.control_systems.hybrid_control_system import HybridControlSystem

from tests.test_infrastructure import create_test_orchestrator, TestModel, TestProvider


class TestActionLoopTask:
    """Test ActionLoopTask model with real validation and functionality."""

    def test_create_basic_action_loop_task(self):
        """Test creating a basic action loop task."""
        task = ActionLoopTask(
            id="test-loop",
            name="Test Action Loop",
            action="action_loop",
            action_loop=[
                {"action": "echo hello", "name": "greeting"},
                {"action": "echo world", "name": "target"}
            ],
            until="greeting.result == 'hello' and target.result == 'world'",
            max_iterations=5
        )
        
        assert task.id == "test-loop"
        assert len(task.action_loop) == 2
        assert task.until == "greeting.result == 'hello' and target.result == 'world'"
        assert task.max_iterations == 5
        assert task.current_iteration == 0
        assert task.terminated_by is None

    def test_action_loop_task_validation(self):
        """Test validation of action loop task fields."""
        # Test missing action_loop
        with pytest.raises(ValueError, match="action_loop cannot be empty"):
            ActionLoopTask(
                id="test",
                name="Test",
                action="action_loop",
                action_loop=[],
                until="true"
            )
        
        # Test missing termination condition
        with pytest.raises(ValueError, match="Either 'until' or 'while_condition' must be specified"):
            ActionLoopTask(
                id="test",
                name="Test", 
                action="action_loop",
                action_loop=[{"action": "test"}]
            )
        
        # Test both termination conditions
        with pytest.raises(ValueError, match="Cannot specify both 'until' and 'while_condition'"):
            ActionLoopTask(
                id="test",
                name="Test",
                action="action_loop", 
                action_loop=[{"action": "test"}],
                until="true",
                while_condition="false"
            )

    def test_action_loop_task_statistics(self):
        """Test action loop task statistics tracking."""
        task = ActionLoopTask(
            id="test-loop",
            name="Test Loop",
            action="action_loop",
            action_loop=[{"action": "test"}],
            until="true"
        )
        
        # Record some tool executions
        task.record_tool_execution("filesystem", True)
        task.record_tool_execution("web-search", True) 
        task.record_tool_execution("filesystem", False, "Permission denied")
        
        stats = task.get_loop_statistics()
        assert stats["total_tool_executions"] == 3
        assert stats["total_errors"] == 1
        assert stats["tool_usage"]["filesystem"] == 2
        assert stats["tool_usage"]["web-search"] == 1
        assert stats["success_rate"] == 2/3

    def test_action_loop_task_serialization(self):
        """Test converting action loop task to/from dictionary."""
        original = ActionLoopTask(
            id="test-loop",
            name="Test Loop", 
            action="action_loop",
            action_loop=[
                {"action": "test", "name": "step1"},
                {"action": "validate", "parameters": {"data": "test"}}
            ],
            until="step1.result == 'success'",
            max_iterations=10,
            break_on_error=True
        )
        
        # Convert to dict
        task_dict = original.to_dict()
        assert task_dict["id"] == "test-loop"
        assert task_dict["action_loop"] == original.action_loop
        assert task_dict["until"] == original.until
        assert task_dict["max_iterations"] == 10
        assert task_dict["break_on_error"] is True
        
        # Convert back from dict
        restored = ActionLoopTask.from_task_definition(task_dict)
        assert restored.id == original.id
        assert restored.action_loop == original.action_loop
        assert restored.until == original.until
        assert restored.max_iterations == original.max_iterations


class TestActionLoopHandler:
    """Test ActionLoopHandler with real tool integration and AUTO resolution."""

    @pytest.fixture
    def handler(self):
        """Create ActionLoopHandler with real components."""
        model_registry = get_model_registry()
        auto_resolver = ControlFlowAutoResolver(model_registry)
        template_manager = TemplateManager()
        
        return ActionLoopHandler(
            tool_registry=default_registry,
            auto_resolver=auto_resolver,
            template_manager=template_manager
        )

    @pytest.fixture
    def temp_dir(self):
        """Create temporary directory for filesystem tests."""
        with tempfile.TemporaryDirectory() as tmp_dir:
            yield Path(tmp_dir)

    @pytest.mark.asyncio
    async def test_simple_iteration_count_loop(self, handler):
        """Test simple action loop with basic iteration counting."""
        task = ActionLoopTask(
            id="count-loop",
            name="Count Loop",
            action="action_loop",
            action_loop=[
                {"action": "echo counting", "name": "status"}
            ],
            until="$iteration >= 3",  # Simple condition without templates
            max_iterations=10
        )
        
        context = {}
        result = await handler.execute_action_loop(task, context)
        
        assert result["success"] is True
        print(f"Actual iterations: {result['iterations_completed']}")
        print(f"Terminated by: {result['terminated_by']}")
        print(f"Condition evaluations: {len(result.get('condition_evaluations', []))}")
        
        # Should terminate by condition or max iterations
        assert result["terminated_by"] in ["condition", "max_iterations"]
        assert result["iterations_completed"] >= 3

    @pytest.mark.asyncio 
    async def test_filesystem_tool_loop(self, handler, temp_dir):
        """Test action loop with real filesystem operations."""
        # Create test files
        for i in range(3):
            (temp_dir / f"input_{i}.txt").write_text(f"Content {i}")
        
        task = ActionLoopTask(
            id="file-loop",
            name="File Processing Loop",
            action="action_loop",
            action_loop=[
                {
                    "tool": "filesystem",
                    "action": "list",
                    "parameters": {"path": str(temp_dir)},
                    "name": "files"
                },
                {
                    "tool": "filesystem",
                    "action": "read", 
                    "parameters": {"path": f"{temp_dir}/input_0.txt"},
                    "name": "content"
                },
                {
                    "tool": "filesystem",
                    "action": "write",
                    "parameters": {
                        "path": str(temp_dir) + "/processed_{{ iteration }}.txt",
                        "content": "Processed: {{ content.result.content }}"
                    },
                    "name": "written"
                }
            ],
            until="$iteration >= 2",
            max_iterations=5
        )
        
        result = await handler.execute_action_loop(task, {})
        
        assert result["success"] is True
        assert result["terminated_by"] == "condition"
        assert result["iterations_completed"] == 2
        
        # Check that processed files were created (iterations 0 and 1)
        for i in range(2):
            processed_file = temp_dir / f"processed_{i}.txt"
            assert processed_file.exists()
            assert "Processed: Content 0" in processed_file.read_text()
        
        # Check tool statistics
        tool_stats = result["tool_statistics"]
        assert "filesystem" in tool_stats
        assert tool_stats["filesystem"]["executions"] == 6  # 3 actions Ã— 2 iterations

    @pytest.mark.asyncio
    async def test_conditional_termination_loop(self, handler, temp_dir):
        """Test action loop with condition-based termination."""
        # Create test files to track iterations
        task = ActionLoopTask(
            id="counter-loop",
            name="Counter Loop",
            action="action_loop",
            action_loop=[
                {
                    "tool": "filesystem",
                    "action": "write",
                    "parameters": {
                        "path": str(temp_dir) + "/iteration_{{ iteration }}.txt",
                        "content": "Iteration {{ iteration }}"
                    },
                    "name": "write_file"
                }
            ],
            until="$iteration >= 3", 
            max_iterations=10
        )
        
        result = await handler.execute_action_loop(task, {})
        
        assert result["success"] is True
        assert result["terminated_by"] == "condition"
        
        # Check that iteration files were created (should be 4 iterations: 0, 1, 2, 3)
        expected_iterations = result["iterations_completed"]
        for i in range(expected_iterations):
            iteration_file = temp_dir / f"iteration_{i}.txt"
            assert iteration_file.exists()
            assert f"Iteration {i}" in iteration_file.read_text()
        
        # Check condition evaluations occurred
        condition_evaluations = result["condition_evaluations"]
        assert len(condition_evaluations) > 0

    @pytest.mark.asyncio
    async def test_max_iterations_termination(self, handler):
        """Test action loop terminated by max iterations."""
        task = ActionLoopTask(
            id="infinite-loop",
            name="Infinite Loop Test",
            action="action_loop",
            action_loop=[
                {"action": "echo continuing", "name": "status"}
            ],
            until="false",  # Never terminate naturally
            max_iterations=3
        )
        
        result = await handler.execute_action_loop(task, {})
        
        assert result["success"] is True
        assert result["iterations_completed"] == 3
        assert result["terminated_by"] == "max_iterations"
        assert len(result["all_results"]) == 3

    @pytest.mark.asyncio
    async def test_break_on_error_loop(self, handler):
        """Test action loop with break on error enabled."""
        task = ActionLoopTask(
            id="error-loop",
            name="Error Loop Test", 
            action="action_loop",
            action_loop=[
                {"action": "echo step 1", "name": "step1"},
                {
                    "tool": "filesystem",
                    "action": "read",
                    "parameters": {"path": "/nonexistent/file.txt"},
                    "name": "error_step"
                },
                {"action": "echo step 3", "name": "step3"}
            ],
            until="{{ $iteration >= 5 }}",
            max_iterations=10,
            break_on_error=True
        )
        
        result = await handler.execute_action_loop(task, {})
        
        assert result["success"] is True  # Loop completed successfully even with errors
        assert result["terminated_by"] == "error"
        assert result["iterations_completed"] == 1  # Stopped after first error
        
        # Check that error was recorded
        tool_stats = result["tool_statistics"]
        assert "filesystem" in tool_stats
        assert tool_stats["filesystem"]["errors"] == 1

    @pytest.mark.asyncio
    async def test_web_search_loop(self, handler):
        """Test action loop with web search tool."""
        # Note: This test uses real web search, may be slow
        task = ActionLoopTask(
            id="search-loop",
            name="Web Search Loop",
            action="action_loop",
            action_loop=[
                {
                    "tool": "web-search",
                    "action": "search",
                    "parameters": {
                        "query": "Python testing iteration {{ iteration }}",
                        "max_results": 2
                    },
                    "name": "search_results"
                }
            ],
            until="$iteration >= 2",
            max_iterations=3
        )
        
        result = await handler.execute_action_loop(task, {})
        
        assert result["success"] is True
        assert result["terminated_by"] in ["condition", "max_iterations"]
        
        # Check that search results were obtained
        if result["all_results"]:
            first_result = result["all_results"][0]["results"]
            assert "search_results" in first_result
            assert isinstance(first_result["search_results"], dict)

    @pytest.mark.asyncio
    async def test_data_processing_loop(self, handler):
        """Test action loop with data processing tool."""
        test_data = ["apple", "banana", "cherry", "date"]
        
        task = ActionLoopTask(
            id="data-loop",
            name="Data Processing Loop",
            action="action_loop",
            action_loop=[
                {
                    "tool": "data-processing",
                    "action": "transform",
                    "parameters": {
                        "data": json.dumps(test_data),
                        "transform_spec": {
                            "item_count": "len(json.loads(data))",
                            "first_item": "json.loads(data)[0]",
                            "uppercased": "[item.upper() for item in json.loads(data)]"
                        }
                    },
                    "name": "processed"
                }
            ],
            until="processed.processed_data.item_count == 4",
            max_iterations=3
        )
        
        result = await handler.execute_action_loop(task, {})
        
        assert result["success"] is True
        assert result["terminated_by"] in ["condition", "max_iterations"]
        
        # Check processed data exists (format may vary)
        final_result = result["final_results"]
        assert "processed" in final_result
        # The data processing tool may return in different formats, just verify some processing occurred
        assert final_result["processed"] is not None

    @pytest.mark.asyncio
    async def test_nested_template_resolution(self, handler, temp_dir):
        """Test complex template resolution within action loops."""
        config_file = temp_dir / "config.json"
        config_file.write_text('{"threshold": 3, "message": "Hello World"}')
        
        task = ActionLoopTask(
            id="template-loop",
            name="Template Resolution Loop",
            action="action_loop", 
            action_loop=[
                {
                    "tool": "filesystem",
                    "action": "read",
                    "parameters": {"path": str(config_file)},
                    "name": "config"
                },
                {
                    "tool": "data-processing",
                    "action": "transform",
                    "parameters": {
                        "data": "{{ config.content }}",
                        "transform_spec": {
                            "parsed": "json.loads(data)",
                            "threshold": "json.loads(data)['threshold']",
                            "iteration_check": "{{ iteration }} >= json.loads(data)['threshold']"
                        }
                    },
                    "name": "parsed_config"
                }
            ],
            until="$iteration >= 3",
            max_iterations=10
        )
        
        result = await handler.execute_action_loop(task, {})
        
        assert result["success"] is True
        assert result["terminated_by"] == "condition"
        assert result["iterations_completed"] >= 3  # Should reach threshold


class TestActionLoopCompilation:
    """Test YAML compilation of action loops."""

    @pytest.fixture
    def compiler(self):
        """Create control flow compiler."""
        from tests.test_infrastructure import TestProvider
        from src.orchestrator.models.registry import ModelRegistry
        
        # Create registry with test infrastructure
        model_registry = ModelRegistry()
        test_provider = TestProvider()
        model_registry.register_provider(test_provider)
        
        return ControlFlowCompiler(model_registry=model_registry)

    @pytest.mark.asyncio
    async def test_compile_basic_action_loop(self, compiler):
        """Test compiling a basic action loop from YAML."""
        yaml_content = """
        name: Test Action Loop Pipeline
        description: Test pipeline with action loop
        inputs:
          message: Hello
        steps:
          - id: loop-test
            name: Basic Action Loop
            action_loop:
              - action: "echo {{ message }}"
                name: greeting
              - action: "echo world"
                name: target
            until: "{{ greeting.result }} == 'Hello' and {{ target.result }} == 'world'"
            max_iterations: 5
        """
        
        pipeline = await compiler.compile(yaml_content)
        assert pipeline is not None
        
        # Handle different task storage structures
        assert len(pipeline.tasks) == 1
        if isinstance(pipeline.tasks, dict):
            # Tasks is a dict with task IDs as keys
            task = list(pipeline.tasks.values())[0]
        else:
            # Tasks is a list
            task = pipeline.tasks[0]
        assert isinstance(task, ActionLoopTask)
        assert task.id == "loop-test"
        assert len(task.action_loop) == 2
        assert task.until is not None
        assert task.max_iterations == 5

    @pytest.mark.asyncio 
    async def test_compile_action_loop_with_tools(self, compiler):
        """Test compiling action loop with tool specifications."""
        yaml_content = """
        name: File Processing Loop
        description: Process files in a loop
        inputs:
          directory: /tmp
        steps:
          - id: file-processor
            name: File Processing Loop
            action_loop:
              - tool: filesystem
                action: list
                parameters:
                  path: "{{ directory }}"
                name: files
              - tool: filesystem
                action: read
                parameters:
                  path: "{{ directory }}/test.txt"
                name: content
              - action: "echo Processed: {{ content.result.content }}"
                name: processed
            until: "{{ $iteration >= 3 }}"
            max_iterations: 10
            break_on_error: true
        """
        
        pipeline = await compiler.compile(yaml_content)
        
        # Handle different task storage structures
        if isinstance(pipeline.tasks, dict):
            task = list(pipeline.tasks.values())[0]
        else:
            task = pipeline.tasks[0]
        
        assert isinstance(task, ActionLoopTask)
        assert len(task.action_loop) == 3
        assert task.action_loop[0]["tool"] == "filesystem"
        assert task.action_loop[1]["tool"] == "filesystem" 
        assert "echo Processed:" in task.action_loop[2]["action"]
        assert task.break_on_error is True


class TestActionLoopIntegration:
    """Test integration of action loops with the orchestrator system."""

    @pytest.fixture
    def control_system(self):
        """Create hybrid control system."""
        model_registry = get_model_registry()
        return HybridControlSystem(model_registry=model_registry)

    @pytest.mark.asyncio
    async def test_execute_action_loop_via_control_system(self, control_system, tmp_path):
        """Test executing action loop through HybridControlSystem."""
        # Create ActionLoopTask
        task = ActionLoopTask(
            id="integration-test",
            name="Integration Test Loop",
            action="action_loop",
            action_loop=[
                {
                    "tool": "filesystem",
                    "action": "write",
                    "parameters": {
                        "path": str(tmp_path) + "/test_{{ iteration }}.txt",
                        "content": "Test iteration {{ iteration }}"
                    },
                    "name": "written"
                }
            ],
            until="$iteration >= 2",
            max_iterations=5
        )
        
        # Execute through control system
        context = {"test_mode": True}
        result = await control_system._execute_task_impl(task, context)
        
        assert result["success"] is True
        assert result["terminated_by"] == "condition"
        
        # Check that files were created for actual iterations
        expected_iterations = result["iterations_completed"]  
        for i in range(expected_iterations):
            test_file = tmp_path / f"test_{i}.txt"
            assert test_file.exists()
            assert f"Test iteration {i}" in test_file.read_text()

    @pytest.mark.asyncio
    async def test_full_pipeline_with_action_loop(self, tmp_path):
        """Test complete pipeline execution with action loop."""
        # Create test YAML pipeline
        yaml_content = f"""
        name: Full Pipeline Test
        description: Complete test with action loop
        inputs:
          base_dir: {tmp_path}
        steps:
          - id: setup
            action: "echo Setting up test"
            name: setup
            
          - id: file-loop
            name: File Creation Loop
            action_loop:
              - tool: filesystem
                action: write
                parameters:
                  path: "{{{{ base_dir }}}}/file_{{{{ $iteration }}}}.txt"
                  content: "File {{{{ $iteration }}}} created"
                name: file_created
            until: "{{{{ $iteration >= 2 }}}}"
            max_iterations: 5
            depends_on: [setup]
            
          - id: summary
            action: "echo Completed file creation"
            name: summary
            depends_on: [file-loop]
        """
        
        # Compile and execute pipeline
        from tests.test_infrastructure import TestProvider
        from src.orchestrator.models.registry import ModelRegistry
        
        # Create registry with test infrastructure
        model_registry = ModelRegistry()
        test_provider = TestProvider()
        model_registry.register_provider(test_provider)
        
        compiler = ControlFlowCompiler(model_registry=model_registry)
        pipeline = await compiler.compile(yaml_content)
        
        # Execute pipeline (would need full orchestrator setup for real execution)
        assert len(pipeline.tasks) == 3
        
        # Find action loop task - handle dict structure
        loop_task = None
        if isinstance(pipeline.tasks, dict):
            tasks = list(pipeline.tasks.values())
        else:
            tasks = pipeline.tasks
            
        for task in tasks:
            if isinstance(task, ActionLoopTask):
                loop_task = task
                break
        
        assert loop_task is not None
        assert loop_task.id == "file-loop"
        assert len(loop_task.action_loop) == 1


# Performance and stress tests
class TestActionLoopPerformance:
    """Test action loop performance and resource management."""

    @pytest.mark.asyncio
    async def test_large_iteration_count(self):
        """Test action loop with large number of iterations."""
        handler = ActionLoopHandler()
        
        task = ActionLoopTask(
            id="performance-test",
            name="Performance Test Loop",
            action="action_loop",
            action_loop=[
                {"action": "echo {{ $iteration }}", "name": "counter"}
            ],
            until="{{ $iteration >= 100 }}",
            max_iterations=100
        )
        
        import time
        start_time = time.time()
        result = await handler.execute_action_loop(task, {})
        execution_time = time.time() - start_time
        
        assert result["success"] is True
        assert result["iterations_completed"] == 100  # Max iterations reached
        assert result["terminated_by"] == "max_iterations"
        
        # Performance check - should complete in reasonable time
        assert execution_time < 30.0  # 30 seconds max
        
        # Memory usage check - final results should be manageable
        assert len(str(result)) < 1000000  # Less than 1MB when serialized

    @pytest.mark.asyncio
    async def test_tool_error_recovery(self, tmp_path):
        """Test action loop error handling and recovery."""
        handler = ActionLoopHandler()
        
        # Create a file that will be deleted mid-loop
        test_file = tmp_path / "temp_file.txt"
        test_file.write_text("initial content")
        
        task = ActionLoopTask(
            id="error-recovery-test", 
            name="Error Recovery Test",
            action="action_loop",
            action_loop=[
                {
                    "tool": "filesystem",
                    "action": "read",
                    "parameters": {"path": str(test_file)},
                    "name": "file_read"
                },
                {
                    "action": "echo Read: {{ file_read.result.content if file_read.result.get('success', False) else 'Error reading file' }}",
                    "name": "status"
                }
            ],
            until="{{ $iteration >= 3 }}",
            max_iterations=5,
            break_on_error=False  # Continue despite errors
        )
        
        # Start execution in background
        async def run_loop():
            return await handler.execute_action_loop(task, {})
        
        # Delete file after first iteration to cause errors
        loop_task = asyncio.create_task(run_loop())
        await asyncio.sleep(0.1)  # Let first iteration complete
        if test_file.exists():
            test_file.unlink()
        
        result = await loop_task
        
        assert result["success"] is True
        assert result["iterations_completed"] == 5  # Includes error iterations
        assert result["terminated_by"] == "max_iterations"
        
        # Check that loop completed successfully despite errors
        # (The fact that we reached here means error recovery worked)


if __name__ == "__main__":
    # Run tests directly
    pytest.main([__file__, "-v", "--tb=short"])