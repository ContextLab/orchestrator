# Trend Analysis Sub-Pipeline
# Performs time series analysis and trend detection
id: trend_analysis
name: Trend Analysis Sub-Pipeline
description: Analyze trends, patterns, and generate simple forecasts
version: "1.0.0"

parameters:
  data:
    type: object
    description: Input data for trend analysis
  time_column:
    type: string
    default: "timestamp"
    description: Column containing time/date information
  value_columns:
    type: array
    default: ["sales", "revenue"]
    description: Columns to analyze for trends

steps:
  - id: prepare_time_series
    tool: python-executor
    parameters:
      code: |
        import pandas as pd
        import numpy as np
        import json
        from datetime import datetime, timedelta
        
        # Get parameters
        data = context.get('data', {})
        time_column = context.get('time_column', 'timestamp')
        value_columns = context.get('value_columns', ['sales', 'revenue'])
        
        # Convert to DataFrame
        if isinstance(data, str):
            try:
                data = json.loads(data)
            except:
                import io
                df = pd.read_csv(io.StringIO(data))
        elif isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            df = pd.DataFrame([data]) if not any(isinstance(v, (list, dict)) for v in data.values()) else pd.DataFrame(data)
        else:
            df = pd.DataFrame(data)
        
        # Try to find time column
        if time_column not in df.columns:
            # Look for date-like columns
            date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]
            if date_cols:
                time_column = date_cols[0]
            else:
                # Create a synthetic time series
                df[time_column] = pd.date_range(start='2024-01-01', periods=len(df), freq='D')
        
        # Convert time column to datetime
        try:
            df[time_column] = pd.to_datetime(df[time_column])
        except:
            # If conversion fails, create synthetic dates
            df[time_column] = pd.date_range(start='2024-01-01', periods=len(df), freq='D')
        
        # Sort by time
        df = df.sort_values(time_column)
        
        # Identify numeric columns if value_columns not in data
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        actual_value_columns = [col for col in value_columns if col in df.columns]
        
        if not actual_value_columns and numeric_cols:
            actual_value_columns = numeric_cols[:3]  # Take first 3 numeric columns
        
        # Store prepared data
        context['time_series_df'] = df
        context['time_column'] = time_column
        context['value_columns'] = actual_value_columns
        
        result = {
            'rows': len(df),
            'time_column': time_column,
            'value_columns': actual_value_columns,
            'date_range': {
                'start': str(df[time_column].min()),
                'end': str(df[time_column].max())
            }
        }
    
  - id: calculate_moving_averages
    tool: python-executor
    parameters:
      code: |
        import pandas as pd
        import numpy as np
        
        df = context.get('time_series_df', pd.DataFrame())
        time_column = context.get('time_column', 'timestamp')
        value_columns = context.get('value_columns', [])
        
        moving_averages = {}
        
        for col in value_columns:
            if col in df.columns:
                series = df[col].fillna(method='ffill').fillna(method='bfill')
                
                # Calculate different moving averages
                ma_7 = series.rolling(window=min(7, len(series))).mean()
                ma_30 = series.rolling(window=min(30, len(series))).mean()
                ma_90 = series.rolling(window=min(90, len(series))).mean()
                
                moving_averages[col] = {
                    '7_day_ma': float(ma_7.iloc[-1]) if len(ma_7) > 0 and not pd.isna(ma_7.iloc[-1]) else None,
                    '30_day_ma': float(ma_30.iloc[-1]) if len(ma_30) > 0 and not pd.isna(ma_30.iloc[-1]) else None,
                    '90_day_ma': float(ma_90.iloc[-1]) if len(ma_90) > 0 and not pd.isna(ma_90.iloc[-1]) else None,
                    'current_value': float(series.iloc[-1]) if len(series) > 0 and not pd.isna(series.iloc[-1]) else None
                }
                
                # Store for visualization
                df[f'{col}_ma7'] = ma_7
                df[f'{col}_ma30'] = ma_30
        
        context['df_with_ma'] = df
        result = moving_averages
    dependencies:
      - prepare_time_series
    
  - id: detect_trends
    tool: python-executor
    parameters:
      code: |
        import pandas as pd
        import numpy as np
        from scipy import stats
        
        df = context.get('time_series_df', pd.DataFrame())
        value_columns = context.get('value_columns', [])
        
        trend_analysis = {}
        
        for col in value_columns:
            if col in df.columns:
                series = df[col].dropna()
                if len(series) > 1:
                    # Create time index for regression
                    x = np.arange(len(series))
                    y = series.values
                    
                    # Linear regression for trend
                    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
                    
                    # Determine trend direction
                    if p_value < 0.05:  # Statistically significant
                        if slope > 0:
                            trend = 'upward'
                        elif slope < 0:
                            trend = 'downward'
                        else:
                            trend = 'stable'
                    else:
                        trend = 'no_significant_trend'
                    
                    # Calculate percent change
                    if len(series) > 0 and series.iloc[0] != 0:
                        pct_change = ((series.iloc[-1] - series.iloc[0]) / series.iloc[0]) * 100
                    else:
                        pct_change = 0
                    
                    # Detect volatility
                    volatility = series.std() / series.mean() if series.mean() != 0 else 0
                    
                    trend_analysis[col] = {
                        'trend_direction': trend,
                        'slope': float(slope),
                        'r_squared': float(r_value ** 2),
                        'p_value': float(p_value),
                        'percent_change': float(pct_change),
                        'volatility': float(volatility),
                        'interpretation': f"{trend.replace('_', ' ').title()} trend with {abs(pct_change):.1f}% change"
                    }
        
        result = trend_analysis
        context['trends'] = trend_analysis
    dependencies:
      - prepare_time_series
    
  - id: detect_seasonality
    tool: python-executor
    parameters:
      code: |
        import pandas as pd
        import numpy as np
        
        df = context.get('time_series_df', pd.DataFrame())
        time_column = context.get('time_column', 'timestamp')
        value_columns = context.get('value_columns', [])
        
        seasonality_analysis = {}
        
        for col in value_columns:
            if col in df.columns and time_column in df.columns:
                # Add time-based features
                df['month'] = pd.to_datetime(df[time_column]).dt.month
                df['day_of_week'] = pd.to_datetime(df[time_column]).dt.dayofweek
                df['quarter'] = pd.to_datetime(df[time_column]).dt.quarter
                
                seasonal_patterns = {}
                
                # Monthly seasonality
                if 'month' in df.columns:
                    monthly_avg = df.groupby('month')[col].mean()
                    if len(monthly_avg) > 0:
                        seasonal_patterns['monthly'] = {
                            'peak_month': int(monthly_avg.idxmax()) if not monthly_avg.empty else None,
                            'low_month': int(monthly_avg.idxmin()) if not monthly_avg.empty else None,
                            'variation': float((monthly_avg.max() - monthly_avg.min()) / monthly_avg.mean()) if monthly_avg.mean() != 0 else 0
                        }
                
                # Day of week seasonality
                if 'day_of_week' in df.columns:
                    daily_avg = df.groupby('day_of_week')[col].mean()
                    if len(daily_avg) > 0:
                        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
                        peak_day_idx = daily_avg.idxmax() if not daily_avg.empty else 0
                        low_day_idx = daily_avg.idxmin() if not daily_avg.empty else 0
                        seasonal_patterns['weekly'] = {
                            'peak_day': days[peak_day_idx] if peak_day_idx < len(days) else 'Unknown',
                            'low_day': days[low_day_idx] if low_day_idx < len(days) else 'Unknown',
                            'weekend_vs_weekday': float(daily_avg[5:].mean() / daily_avg[:5].mean()) if daily_avg[:5].mean() != 0 else 1
                        }
                
                # Quarterly seasonality
                if 'quarter' in df.columns:
                    quarterly_avg = df.groupby('quarter')[col].mean()
                    if len(quarterly_avg) > 0:
                        seasonal_patterns['quarterly'] = {
                            'peak_quarter': f"Q{int(quarterly_avg.idxmax())}" if not quarterly_avg.empty else None,
                            'low_quarter': f"Q{int(quarterly_avg.idxmin())}" if not quarterly_avg.empty else None
                        }
                
                seasonality_analysis[col] = seasonal_patterns
        
        result = seasonality_analysis
        context['seasonality'] = seasonality_analysis
    dependencies:
      - prepare_time_series
    
  - id: generate_forecast
    tool: python-executor
    parameters:
      code: |
        import pandas as pd
        import numpy as np
        from scipy import stats
        
        df = context.get('time_series_df', pd.DataFrame())
        value_columns = context.get('value_columns', [])
        trends = context.get('trends', {})
        
        forecasts = {}
        
        for col in value_columns:
            if col in df.columns and col in trends:
                series = df[col].dropna()
                if len(series) > 1:
                    # Simple linear projection
                    x = np.arange(len(series))
                    y = series.values
                    
                    # Get trend line parameters
                    slope = trends[col].get('slope', 0)
                    _, intercept, _, _, _ = stats.linregress(x, y)
                    
                    # Project forward (10% of data length)
                    forecast_periods = max(1, min(30, len(series) // 10))
                    future_x = np.arange(len(series), len(series) + forecast_periods)
                    forecast_values = slope * future_x + intercept
                    
                    # Add some uncertainty based on historical volatility
                    volatility = series.std()
                    
                    forecasts[col] = {
                        'forecast_periods': int(forecast_periods),
                        'forecast_values': [float(v) for v in forecast_values[:5]],  # First 5 forecasts
                        'forecast_mean': float(forecast_values.mean()),
                        'confidence_interval': {
                            'lower': float(forecast_values.mean() - 1.96 * volatility),
                            'upper': float(forecast_values.mean() + 1.96 * volatility)
                        },
                        'expected_change': float(slope * forecast_periods),
                        'method': 'linear_projection'
                    }
        
        result = forecasts
        context['forecasts'] = forecasts
    dependencies:
      - detect_trends
    
  - id: generate_trend_summary
    action: generate_text
    parameters:
      prompt: |
        Based on the trend analysis results, provide a comprehensive summary:
        
        Trend Analysis:
        {{ detect_trends.result | json }}
        
        Moving Averages:
        {{ calculate_moving_averages.result | json }}
        
        Seasonality Patterns:
        {{ detect_seasonality.result | json }}
        
        Forecasts:
        {{ generate_forecast.result | json }}
        
        Write a 4-5 sentence summary that includes:
        1. Overall trend direction for key metrics
        2. Any seasonal patterns detected
        3. Short-term forecast expectations
        4. Key insights or recommendations based on the trends
        
        Use specific numbers and percentages from the analysis.
      model: <AUTO>
    dependencies:
      - detect_trends
      - calculate_moving_averages
      - detect_seasonality
      - generate_forecast

outputs:
  trends: "{{ detect_trends.result }}"
  moving_averages: "{{ calculate_moving_averages.result }}"
  seasonality: "{{ detect_seasonality.result }}"
  forecasts: "{{ generate_forecast.result }}"
  trend_summary: "{{ generate_trend_summary.result }}"