# Sentiment Analysis Sub-Pipeline
# Performs sentiment analysis on text data using LLM APIs
id: sentiment_analysis
name: Sentiment Analysis Sub-Pipeline
description: Analyze sentiment, extract entities, and identify key themes
version: "1.0.0"

parameters:
  data:
    type: object
    description: Input data containing text to analyze
  text_column:
    type: string
    default: "comments"
    description: Name of the column containing text data

steps:
  - id: prepare_text_data
    tool: python-executor
    parameters:
      code: |
        import pandas as pd
        import json
        
        # Get input data
        data = context.get('data', {})
        text_column = context.get('text_column', 'comments')
        
        # Convert to DataFrame
        if isinstance(data, str):
            try:
                data = json.loads(data)
            except:
                import io
                df = pd.read_csv(io.StringIO(data))
        elif isinstance(data, list):
            df = pd.DataFrame(data)
        elif isinstance(data, dict):
            df = pd.DataFrame([data]) if not any(isinstance(v, (list, dict)) for v in data.values()) else pd.DataFrame(data)
        else:
            df = pd.DataFrame(data)
        
        # Extract text data
        if text_column in df.columns:
            texts = df[text_column].dropna().tolist()
        else:
            # Try to find any text column
            text_cols = df.select_dtypes(include=['object']).columns
            if len(text_cols) > 0:
                texts = df[text_cols[0]].dropna().tolist()
                text_column = text_cols[0]
            else:
                texts = []
        
        # Limit to reasonable number for API calls
        texts = texts[:100]  # Process up to 100 texts
        
        context['texts'] = texts
        context['text_column_used'] = text_column
        
        result = {
            'text_count': len(texts),
            'text_column': text_column,
            'sample_texts': texts[:3] if texts else []
        }
    
  - id: batch_sentiment_analysis
    tool: task-delegation
    action: batch_process
    parameters:
      tasks: |
        {% set texts = prepare_text_data.result.get('sample_texts', []) %}
        {% if texts %}
        [
          {% for text in texts[:20] %}
          {
            "task": "Analyze the sentiment of this text and provide a score from -1 (very negative) to 1 (very positive), along with the primary emotion detected. Respond with JSON only: {\"score\": <number>, \"emotion\": \"<emotion>\", \"confidence\": <0-1>}",
            "context": "{{ text | truncate(500) | replace('\"', '\\\"') }}"
          }{% if not loop.last %},{% endif %}
          {% endfor %}
        ]
        {% else %}
        [
          {
            "task": "Analyze sentiment",
            "context": "No text data available for analysis"
          }
        ]
        {% endif %}
      model: <AUTO>Select fast model for sentiment analysis</AUTO>
      max_concurrent: 5
    dependencies:
      - prepare_text_data
    
  - id: entity_extraction
    action: generate_text
    parameters:
      prompt: |
        Extract key entities from the following text samples:
        
        {% for text in prepare_text_data.result.sample_texts[:10] %}
        Text {{ loop.index }}: {{ text | truncate(200) }}
        {% endfor %}
        
        Identify and list:
        1. People/Names mentioned
        2. Organizations/Companies
        3. Products/Services
        4. Locations
        5. Key topics/themes
        
        Provide the results in JSON format:
        {
          "people": [...],
          "organizations": [...],
          "products": [...],
          "locations": [...],
          "topics": [...]
        }
      model: <AUTO>
      response_format: json
    dependencies:
      - prepare_text_data
    condition: "{{ prepare_text_data.result.text_count > 0 }}"
    
  - id: keyword_analysis
    tool: python-executor
    parameters:
      code: |
        import re
        from collections import Counter
        
        texts = context.get('texts', [])
        
        # Simple keyword extraction
        all_words = []
        for text in texts:
            if isinstance(text, str):
                # Extract words (simple tokenization)
                words = re.findall(r'\b[a-z]+\b', text.lower())
                all_words.extend(words)
        
        # Remove common stop words
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                     'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
                     'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                     'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these',
                     'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'which'}
        
        filtered_words = [w for w in all_words if w not in stop_words and len(w) > 3]
        
        # Count frequencies
        word_freq = Counter(filtered_words)
        top_keywords = word_freq.most_common(20)
        
        # Create bigrams (two-word phrases)
        bigrams = []
        for text in texts[:50]:  # Limit for performance
            if isinstance(text, str):
                words = re.findall(r'\b[a-z]+\b', text.lower())
                words = [w for w in words if w not in stop_words]
                for i in range(len(words) - 1):
                    bigrams.append(f"{words[i]} {words[i+1]}")
        
        bigram_freq = Counter(bigrams)
        top_phrases = bigram_freq.most_common(10)
        
        result = {
            'top_keywords': [{'word': word, 'count': count} for word, count in top_keywords],
            'top_phrases': [{'phrase': phrase, 'count': count} for phrase, count in top_phrases],
            'total_words_analyzed': len(all_words),
            'unique_words': len(set(all_words))
        }
    dependencies:
      - prepare_text_data
    
  - id: aggregate_sentiment_scores
    tool: python-executor
    parameters:
      code: |
        import json
        import numpy as np
        
        # Get batch sentiment results
        batch_results = context.get('batch_sentiment_analysis', {}).get('result', {}).get('results', [])
        
        scores = []
        emotions = []
        
        for result in batch_results:
            if isinstance(result, str):
                try:
                    parsed = json.loads(result)
                    if 'score' in parsed:
                        scores.append(float(parsed['score']))
                    if 'emotion' in parsed:
                        emotions.append(parsed['emotion'])
                except:
                    pass
            elif isinstance(result, dict):
                if 'score' in result:
                    scores.append(float(result['score']))
                if 'emotion' in result:
                    emotions.append(result['emotion'])
        
        # Calculate aggregate metrics
        if scores:
            sentiment_stats = {
                'mean_sentiment': float(np.mean(scores)),
                'median_sentiment': float(np.median(scores)),
                'std_sentiment': float(np.std(scores)),
                'min_sentiment': float(np.min(scores)),
                'max_sentiment': float(np.max(scores)),
                'positive_ratio': float(sum(1 for s in scores if s > 0.1) / len(scores)),
                'negative_ratio': float(sum(1 for s in scores if s < -0.1) / len(scores)),
                'neutral_ratio': float(sum(1 for s in scores if -0.1 <= s <= 0.1) / len(scores))
            }
        else:
            sentiment_stats = {
                'mean_sentiment': 0,
                'median_sentiment': 0,
                'positive_ratio': 0,
                'negative_ratio': 0,
                'neutral_ratio': 1
            }
        
        # Count emotions
        from collections import Counter
        emotion_counts = Counter(emotions)
        
        result = {
            'sentiment_statistics': sentiment_stats,
            'emotion_distribution': dict(emotion_counts),
            'dominant_emotion': emotion_counts.most_common(1)[0][0] if emotion_counts else 'neutral',
            'sample_count': len(scores)
        }
    dependencies:
      - batch_sentiment_analysis
    
  - id: generate_sentiment_summary
    action: generate_text
    parameters:
      prompt: |
        Based on the sentiment analysis results, provide a concise summary:
        
        Sentiment Statistics:
        {{ aggregate_sentiment_scores.result.sentiment_statistics | json }}
        
        Emotion Distribution:
        {{ aggregate_sentiment_scores.result.emotion_distribution | json }}
        
        Top Keywords:
        {{ keyword_analysis.result.top_keywords[:10] | json }}
        
        Extracted Entities:
        {{ entity_extraction.result | default('No entities extracted') }}
        
        Write a 3-4 sentence summary that includes:
        1. Overall sentiment trend (positive/negative/neutral)
        2. Primary emotions detected
        3. Key topics or themes identified
        4. Any notable patterns or insights
      model: <AUTO>
    dependencies:
      - aggregate_sentiment_scores
      - keyword_analysis
      - entity_extraction

outputs:
  sentiment_scores: "{{ aggregate_sentiment_scores.result }}"
  keywords: "{{ keyword_analysis.result }}"
  entities: "{{ entity_extraction.result | default({}) }}"
  overall_sentiment: "{{ aggregate_sentiment_scores.result.sentiment_statistics.mean_sentiment | default(0) }}"
  sentiment_summary: "{{ generate_sentiment_summary.result }}"