id: auto-tags-demo
# AUTO Tags Demonstration Pipeline
# This example showcases various AUTO tag features for dynamic intelligence

name: auto-tags-demonstration
description: Demonstrates AUTO tags for dynamic parameter resolution
version: "1.0.0"

# Input parameters that will be used by AUTO tags
inputs:
  data_file: "sales_data.csv"
  analysis_goal: "identify trends and anomalies"
  output_format_preference: "executive_summary"

steps:
  # Step 1: Analyze data characteristics
  - id: analyze_data_type
    tool: filesystem
    action: read
    parameters:
      path: "{{ data_file }}"
      
  # Step 2: Choose analysis method dynamically
  - id: select_analysis
    action: generate_text
    parameters:
      prompt: |
        Given this data sample:
        {{ analyze_data_type.content | truncate(500) }}
        
        The analysis goal is: {{ analysis_goal }}
        
        Based on the data structure and analysis goal, determine:
        1. The best analysis method: 'statistical', 'ml_clustering', 'time_series', or 'correlation'
        2. An appropriate confidence threshold (0.0-1.0)
      model: <AUTO>Choose an appropriate model for analysis planning</AUTO>
      
  # Step 3: Determine processing requirements
  - id: assess_complexity
    action: analyze_text
    parameters:
      text: "{{ analyze_data_type.content }}"
      analysis_type: "complexity"
      prompt: |
        Analyze this data and determine:
        1. Processing power needed: 'light', 'standard', or 'heavy'
        2. Optimal number of parallel tasks (1-10)
      model: <AUTO>Choose a model for complexity assessment</AUTO>
      
  # Step 4: Dynamic error handling
  - id: process_data
    tool: data-processing
    action: transform
    parameters:
      input_data: "{{ analyze_data_type.content }}"
      input_format: "csv"
      output_format: "json"
      error_strategy: <AUTO>For financial data, should we 'skip_errors', 'fix_errors', or 'fail_on_error'?</AUTO>
      validation_level: "<AUTO>What validation strictness is appropriate: 'permissive', 'moderate', or 'strict'?</AUTO>"
      
  # Step 5: Generate insights with dynamic depth
  - id: generate_insights
    action: analyze_text
    parameters:
      text: "{{ process_data.result }}"
      analysis_type: "insights"
      prompt: |
        Analyze this processed data for {{ analysis_goal }}.
        
        Provide:
        1. Key insights (depth should match executive summary preference)
        2. Whether visualizations would enhance the analysis (yes/no)
        3. List 3-5 key metrics to highlight
      model: <AUTO>Choose a model for insight generation</AUTO>
      
  # Step 6: Conditional visualization
  - id: create_charts
    tool: report-generator
    action: generate
    condition: "{{ generate_insights.visualization_needed == 'yes' }}"
    parameters:
      title: "Data Visualizations"
      format: "html"
      chart_types: "<AUTO>What chart types best represent this data? List up to 3 from: 'line', 'bar', 'scatter', 'pie', 'heatmap'</AUTO>"
      color_scheme: "<AUTO>Choose appropriate color scheme: 'professional', 'colorful', 'monochrome'</AUTO>"
      
  # Step 7: Format output based on preference
  - id: create_report
    tool: report-generator
    action: generate
    parameters:
      title: "Analysis Report: {{ analysis_goal }}"
      format: "<AUTO>Given output preference '{{ output_format_preference }}', choose format: 'markdown', 'pdf', or 'html'</AUTO>"
      sections: <AUTO>What sections should this report include? Provide as comma-separated list</AUTO>
      tone: "<AUTO>What tone is appropriate for an executive summary: 'formal', 'conversational', or 'technical'?</AUTO>"
      length: "<AUTO>Target length in words for executive summary: choose between 200-1000</AUTO>"
      content: |
        # Analysis Results
        
        ## Summary
        Method: {{ select_analysis.analysis_method }}
        Confidence: {{ select_analysis.confidence_threshold }}
        
        ## Key Insights
        {{ generate_insights.result }}
        
        ## Metrics
        {{ generate_insights.key_metrics }}
        
        {% if create_charts.result %}
        ## Visualizations
        {{ create_charts.result }}
        {% endif %}
        
  # Step 8: Determine distribution list
  - id: distribution_plan
    action: generate_text
    parameters:
      prompt: |
        Based on the analysis of {{ analysis_goal }}, determine:
        1. Who should receive this report (list stakeholder groups)
        2. Priority level: 'urgent', 'high', 'normal', or 'low'
        3. Recommended follow-up actions (list 2-3 items)
      model: <AUTO>Choose a model for distribution planning</AUTO>
      
  # Step 9: Save the final report
  - id: save_report
    tool: filesystem
    action: write
    parameters:
      path: "examples/outputs/auto_tags_demo/analysis_report_{{ execution.timestamp | slugify }}.md"
      content: |
        # AUTO Tags Demo Analysis Report
        
        **Generated:** {{ execution.timestamp }}
        **Analysis Goal:** {{ analysis_goal }}
        
        ## Executive Summary
        
        {{ create_report.markdown }}
        
        ## Distribution Plan
        
        {{ distribution_plan.result }}
        
        ---
        *Generated by AUTO Tags Demonstration Pipeline*
    dependencies:
      - create_report
      - distribution_plan

# Output definition
outputs:
  report_path: "{{ save_report.filepath }}"
  analysis_results: "{{ generate_insights.result }}"
  distribution_plan: "{{ distribution_plan.result }}"