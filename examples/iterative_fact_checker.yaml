# Iterative Fact-Checking Pipeline
# Uses while loops to iteratively verify and add references to documents
id: iterative_fact_checker
name: Iterative Fact Checker
description: Iteratively processes documents to ensure all claims have proper references
version: "2.0.0"

parameters:
  input_document:
    type: string
    default: "test_climate_document.md"
    description: Path to the document to fact-check
  quality_threshold:
    type: number
    default: 0.95
    description: Minimum percentage of claims that must have references
  max_iterations:
    type: integer
    default: 5
    description: Maximum number of improvement iterations

steps:
  # Initialize with the input document
  - id: initialize_vars
    action: generate
    parameters:
      prompt: "Initialize fact-checking process for document: {{ parameters.input_document }}"
      model: "claude-sonnet-4-20250514"
      max_tokens: 50
    
  # Load the initial document
  - id: load_initial_doc
    tool: filesystem
    action: read
    parameters:
      path: "{{ parameters.input_document }}"
    dependencies:
      - initialize_vars
    
  # Main iterative fact-checking loop
  - id: fact_check_loop
    while: "true"
    max_iterations: 3
    dependencies:
      - load_initial_doc
    steps:
      # Load current document (first iteration from initial, then from previous iteration)
      - id: load_document
        tool: filesystem
        action: read
        parameters:
          path: "{{ parameters.input_document }}"
    
      # Extract all claims and existing references
      - id: extract_claims
        action: generate
        parameters:
          prompt: |
            Analyze this document and extract all factual claims and their references.
            
            Document:
            {{ load_document.content }}
            
            For each claim, provide:
            1. The claim text
            2. Whether it has a reference/citation (true/false)
            3. If it has a reference, extract the reference URL
            
            Please respond in the following JSON format:
            {
              "claims": [
                {
                  "text": "claim text here",
                  "has_reference": true/false,
                  "reference_url": "URL if available"
                }
              ],
              "total_claims": number,
              "claims_with_references": number,
              "percentage_referenced": decimal_percentage
            }
            
            Return ONLY the JSON, no other text.
          model: "claude-sonnet-4-20250514"
          max_tokens: 2000
    
      # Verify existing references using headless browser (simplified for now)
      - id: verify_refs
        action: generate
        parameters:
          prompt: |
            Verify which of these reference URLs are valid and accessible:
            {% for claim in (extract_claims.result | from_json).claims %}
            {% if claim.has_reference and claim.reference_url %}
            - {{ claim.reference_url }}
            {% endif %}
            {% endfor %}
            
            For each URL, indicate if it's valid/accessible or broken.
          model: "claude-sonnet-4-20250514"
          max_tokens: 1000
    
      # Find citations for unreferenced claims
      - id: find_citations
        action: generate
        parameters:
          prompt: |
            Find reliable sources and citations for these unreferenced claims:
            
            {% for claim in (extract_claims.result | from_json).claims %}
            {% if not claim.has_reference %}
            - {{ claim.text }}
            {% endif %}
            {% endfor %}
            
            For each claim, provide:
            1. A reliable source URL
            2. The source title
            3. A brief explanation of why this source supports the claim
            
            Format as a list with clear citations.
          model: "claude-sonnet-4-20250514"
          max_tokens: 2000
    
      # Update document with new references and corrections
      - id: update_document
        action: generate
        parameters:
          prompt: |
            Update this document by:
            1. Adding citations for all unreferenced claims using the sources found
            2. Fixing any broken references (URLs that returned errors)
            3. Formatting all references consistently as footnotes at the end
            
            Original document:
            {{ load_document.content }}
            
            Claims analysis:
            {{ extract_claims }}
            
            Reference verification results:
            {{ verify_refs }}
            
            New citations to add:
            {{ find_citations.result }}
            
            Return the complete updated document with all improvements.
          model: "claude-sonnet-4-20250514"
          max_tokens: 5000
    
      # Save iteration document
      - id: save_iteration
        tool: filesystem
        action: write
        parameters:
          path: "{{ output_path }}/iteration_{{ $iteration }}_document.md"
          content: "{{ update_document.result }}"
      
      # Update quality score for reporting
      - id: update_score
        action: generate
        parameters:
          prompt: |
            The document now has {{ (extract_claims.result | from_json).claims_with_references }} out of {{ (extract_claims.result | from_json).total_claims }} claims with references.
            This is {{ (extract_claims.result | from_json).percentage_referenced }}% referenced.
            Output just the decimal percentage (e.g., 0.95 for 95%).
          model: "claude-sonnet-4-20250514"
          max_tokens: 10
    
  # Save the final verified document
  - id: save_final_document
    tool: filesystem
    action: write
    parameters:
      path: "{{ output_path }}/{{ parameters.input_document | basename | regex_replace('\\.md$', '') }}_verified.md"
      content: |
        {% if fact_check_loop.iterations %}
        {{ fact_check_loop.iterations[-1].update_document.result }}
        {% else %}
        {{ load_initial_doc.content }}
        {% endif %}
    dependencies:
      - fact_check_loop
    
  # Generate comprehensive fact-checking report
  - id: generate_report
    tool: filesystem
    action: write
    parameters:
      path: "{{ output_path }}/fact_checking_report.md"
      content: |
        # Fact-Checking Report
        
        ## Document Information
        - **Source Document**: {{ parameters.input_document }}
        - **Date Processed**: {{ execution.timestamp }}
        
        ## Processing Summary
        - **Total Iterations**: {{ fact_check_loop.iteration_count | default(1) }}
        - **Quality Threshold**: {{ parameters.quality_threshold * 100 }}%
        - **Final Quality Score**: {% if fact_check_loop.iterations %}{{ (fact_check_loop.iterations[-1].extract_claims.result | from_json).percentage_referenced }}%{% else %}N/A{% endif %}
        
        ## Iteration Details
        {% for iteration in fact_check_loop.iterations %}
        ### Iteration {{ loop.index }}
        - Claims analyzed: {{ (iteration.extract_claims.result | from_json).total_claims }}
        - Claims with references: {{ (iteration.extract_claims.result | from_json).claims_with_references }}
        - Percentage referenced: {{ (iteration.extract_claims.result | from_json).percentage_referenced }}%
        - New citations added: {{ iteration.find_citations.result | length | default(0) }}
        {% endfor %}
        
        ## Final Status
        {% if fact_check_loop.iterations and (fact_check_loop.iterations[-1].extract_claims.result | from_json).percentage_referenced >= parameters.quality_threshold * 100 %}
        ✅ **Quality threshold met**: All or most claims now have proper references.
        {% else %}
        ⚠️ **Maximum iterations reached**: Some claims may still lack references.
        {% endif %}
        
        ## Output Files
        - Verified document: `{{ parameters.input_document | basename | regex_replace('\\.md$', '') }}_verified.md`
        - Iteration documents: `iteration_*_document.md`
        
        ---
        *Generated by Iterative Fact Checker Pipeline v2.0*
    dependencies:
      - save_final_document

outputs:
  verified_document: "{{ save_final_document.path }}"
  report: "{{ generate_report.path }}"
  iterations_performed: "{{ fact_check_loop.iteration_count | default(0) }}"
  final_quality: "{% if fact_check_loop.iterations %}{{ (fact_check_loop.iterations[-1].extract_claims.result | from_json).percentage_referenced }}%{% else %}N/A{% endif %}"