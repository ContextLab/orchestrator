#!/usr/bin/env python3
"""Simplified research report pipeline."""

import asyncio
import sys
import os
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from orchestrator.orchestrator import Orchestrator
from orchestrator.core.control_system import MockControlSystem
from orchestrator.core.task import Task
from orchestrator.integrations.ollama_model import OllamaModel


class SimpleResearchControlSystem(MockControlSystem):
    """Simplified control system for research reports."""
    
    def __init__(self):
        super().__init__(name="simple-research")
        self._results = {}
    
    async def execute_task(self, task: Task, context: dict = None):
        """Execute research tasks."""
        # Handle $results references
        self._resolve_references(task)
        
        # Execute based on action
        if task.action == "search":
            result = await self._search(task)
        elif task.action == "compile":
            result = await self._compile(task)
        elif task.action == "generate_report":
            result = await self._generate_report(task)
        else:
            result = {"status": "completed", "message": f"Executed {task.action}"}
        
        self._results[task.id] = result
        return result
    
    def _resolve_references(self, task):
        """Resolve $results references."""
        for key, value in task.parameters.items():
            if isinstance(value, str) and value.startswith("$results."):
                parts = value.split(".")
                if len(parts) >= 2:
                    task_id = parts[1]
                    if task_id in self._results:
                        result = self._results[task_id]
                        for part in parts[2:]:
                            if isinstance(result, dict) and part in result:
                                result = result[part]
                            else:
                                result = None
                                break
                        task.parameters[key] = result
    
    async def _search(self, task):
        """Perform web search."""
        query = task.parameters.get("query", "AI agents")
        
        print(f"\nüîç [SEARCH] Query: '{query}'")
        
        # Simple search results
        return {
            "query": query,
            "results": [
                {
                    "title": "LangChain Agents Documentation",
                    "url": "https://python.langchain.com/docs/modules/agents/",
                    "snippet": "Agents use an LLM to determine which actions to take."
                },
                {
                    "title": "AutoGen - Multi-agent Framework",
                    "url": "https://microsoft.github.io/autogen/",
                    "snippet": "Framework for LLM applications using multiple agents."
                },
                {
                    "title": "CrewAI - AI Agent Teams",
                    "url": "https://github.com/joaomdmoura/crewAI",
                    "snippet": "Framework for orchestrating role-playing AI agents."
                }
            ]
        }
    
    async def _compile(self, task):
        """Compile search results."""
        data = task.parameters.get("data", {})
        
        print(f"\nüìö [COMPILE] Compiling results...")
        
        results = data.get("results", [])
        
        compiled = "# AI Agents Research\n\n"
        for r in results:
            compiled += f"## {r['title']}\n"
            compiled += f"- URL: {r['url']}\n"
            compiled += f"- {r['snippet']}\n\n"
        
        return {"content": compiled}
    
    async def _generate_report(self, task):
        """Generate final report."""
        content = task.parameters.get("content", {})
        topic = task.parameters.get("topic", "AI agents")
        
        print(f"\nüìù [GENERATE REPORT] Topic: {topic}")
        
        compiled_text = content.get("content", "") if isinstance(content, dict) else str(content)
        
        report = f"""# Research Report: {topic}

**Date**: {datetime.now().strftime('%Y-%m-%d')}

## Summary

This report explores AI agents, their frameworks, and implementation approaches.

## Research Findings

{compiled_text}

## Key Frameworks

1. **LangChain** - Comprehensive agent framework
2. **AutoGen** - Multi-agent conversations
3. **CrewAI** - Role-based agent teams

## Conclusion

AI agents enable autonomous task execution using LLMs. Popular Python frameworks include LangChain, AutoGen, and CrewAI.

---
*Generated by Orchestrator Framework*"""
        
        return {
            "content": report,
            "word_count": len(report.split())
        }


async def run_simple_research():
    """Run simplified research pipeline."""
    print("\nüöÄ SIMPLE RESEARCH REPORT PIPELINE")
    print("=" * 60)
    
    pipeline_yaml = """
name: simple_research_report
description: Simplified research report generation

steps:
  - id: search_web
    action: search
    parameters:
      query: <AUTO>AI agents frameworks Python</AUTO>

  - id: compile_data
    action: compile
    dependencies: [search_web]
    parameters:
      data: "$results.search_web"

  - id: create_report
    action: generate_report
    dependencies: [compile_data]
    parameters:
      content: "$results.compile_data"
      topic: "AI Agents"
"""
    
    # Set up orchestrator
    control_system = SimpleResearchControlSystem()
    orchestrator = Orchestrator(control_system=control_system)
    
    # Use real model if available
    model = OllamaModel(model_name="gemma2:27b", timeout=60)
    if model._is_available:
        print(f"‚úÖ Using model: {model.name}")
        orchestrator.yaml_compiler.ambiguity_resolver.model = model
    else:
        print("‚ö†Ô∏è  Using fallback model")
    
    # Execute
    print("\n‚öôÔ∏è  Executing pipeline...")
    
    try:
        results = await orchestrator.execute_yaml(pipeline_yaml, context={})
        
        print("\n‚úÖ Pipeline completed!")
        
        # Save report
        output_dir = Path("output/simple_research")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        report = results.get("create_report", {})
        if report and "content" in report:
            report_path = output_dir / "research_report.md"
            with open(report_path, "w") as f:
                f.write(report["content"])
            
            print(f"\nüíæ Report saved to: {report_path}")
            print(f"   Word count: {report.get('word_count', 0)}")
            
            # Show excerpt
            print("\nüìú Report Preview:")
            print("-" * 40)
            print(report["content"][:500] + "...")
            print("-" * 40)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def assess_report():
    """Assess the generated report quality."""
    print("\nüîç REPORT QUALITY ASSESSMENT")
    print("=" * 60)
    
    report_path = Path("output/simple_research/research_report.md")
    
    if not report_path.exists():
        print("‚ùå No report found.")
        return 0
    
    with open(report_path, "r") as f:
        content = f.read()
    
    print(f"üìÑ Report: {report_path}")
    print(f"   Size: {len(content)} chars")
    print(f"   Words: {len(content.split())}")
    
    # Check content
    score = 0
    checks = {
        "Has title": "Research Report:" in content,
        "Has date": "Date:" in content,
        "Has summary": "Summary" in content,
        "Has frameworks": any(f in content for f in ["LangChain", "AutoGen", "CrewAI"]),
        "Has conclusion": "Conclusion" in content
    }
    
    print("\nüìã Content Checks:")
    for check, passed in checks.items():
        if passed:
            score += 0.2
            print(f"   ‚úÖ {check}")
        else:
            print(f"   ‚ùå {check}")
    
    print(f"\nüéØ Quality Score: {score:.0%}")
    
    if score >= 0.8:
        print("   Grade: A - Good report")
    elif score >= 0.6:
        print("   Grade: B - Acceptable")
    else:
        print("   Grade: C - Needs improvement")
    
    return score


async def main():
    """Run simple research report demonstration."""
    print("üéØ SIMPLIFIED RESEARCH REPORT TEST")
    print("Demonstrating basic pipeline functionality")
    print("=" * 60)
    
    # Run pipeline
    success = await run_simple_research()
    
    if success:
        # Assess quality
        score = await assess_report()
        
        print("\n" + "=" * 60)
        print("üèÅ TEST COMPLETE")
        
        if score >= 0.6:
            print("\n‚úÖ Successfully generated research report")
            print("‚úÖ Basic pipeline functionality confirmed")
            print("‚úÖ Ready for more complex pipelines")
        else:
            print("\n‚ö†Ô∏è  Report generated with issues")
    else:
        print("\n‚ùå Pipeline failed")
    
    print("\nüí° For the full README pipeline, we need:")
    print("   - Larger models (7B+ parameters)")
    print("   - Real web search capability")
    print("   - PDF generation tools")
    print("   - More sophisticated quality checks")


if __name__ == "__main__":
    asyncio.run(main())