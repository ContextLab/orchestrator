name: "Data Processing Workflow"
description: "Scalable data processing pipeline with validation, transformation, and analysis"
model: "anthropic/claude-sonnet-4-20250514"

inputs:
  source:
    type: string
    description: "Data source path or pattern (file, S3, database URL)"
    required: true
  
  output_path:
    type: string
    description: "Output path for processed data"
    required: true
  
  output_format:
    type: string
    description: "Output format (parquet, csv, json, database)"
    default: "parquet"
  
  chunk_size:
    type: integer
    description: "Number of records to process in each chunk"
    default: 10000
  
  quality_threshold:
    type: float
    description: "Minimum data quality score (0-1)"
    default: 0.8
  
  parallel_workers:
    type: integer
    description: "Number of parallel processing workers"
    default: 4

steps:
  # Step 1: Discover and validate data sources
  - id: discover_sources
    action: |
      discover all data sources matching pattern {{source}} and gather metadata including:
      1. File/table names and sizes
      2. Data formats and schemas
      3. Creation/modification dates
      4. Access permissions
      Return list of valid sources with metadata
    timeout: 30.0
    
    tags: ["discovery", "metadata"]

  # Step 2: Validate data schema
  - id: validate_schema
    action: |
      analyze the schema of each data source and:
      1. Detect column types and formats
      2. Identify primary keys and relationships
      3. Check for required fields
      4. Validate data types consistency
      Return schema validation report
    depends_on: [discover_sources]
    
    condition: "{{discover_sources.result.count}} > 0"
    tags: ["validation", "schema"]

  # Step 3: Profile data quality
  - id: profile_data
    action: |
      profile data quality for each source:
      1. Calculate completeness (null/missing values)
      2. Check data type consistency
      3. Identify outliers and anomalies
      4. Measure uniqueness and cardinality
      5. Detect potential duplicates
      Return quality metrics and issues
    depends_on: [validate_schema]
    tags: ["profiling", "quality"]

  # Step 4: Clean and transform data
  - id: clean_data
    action: |
      clean and transform data based on profiling results:
      1. Handle missing values (impute/remove based on context)
      2. Standardize formats (dates, phone numbers, addresses)
      3. Remove duplicates
      4. Fix data type inconsistencies
      5. Apply business rules and constraints
      Process in chunks of {{chunk_size}} records
    depends_on: [profile_data]
    condition: "{{profile_data.result.average_quality_score}} >= {{quality_threshold}}"
    on_error:
      action: |
        log problematic records and continue with valid data
      continue_on_error: true
      retry_count: 2
    tags: ["cleaning", "transformation"]

  # Step 5: Enrich data
  - id: enrich_data
    action: |
      enrich cleaned data by:
      1. Adding calculated fields and derived metrics
      2. Joining with reference data if available
      3. Applying ML predictions or classifications
      4. Adding timestamps and processing metadata
      Return enriched dataset
    depends_on: [clean_data]
    
    condition: "{{clean_data.result.success}} == true"
    tags: ["enrichment", "feature-engineering"]

  # Step 6: Validate processed data
  - id: validate_output
    action: |
      validate the processed data:
      1. Verify all business rules are satisfied
      2. Check referential integrity
      3. Validate calculated fields
      4. Ensure output schema compliance
      5. Compare record counts and totals
      Return validation report
    depends_on: [enrich_data]
    
    tags: ["validation", "quality-assurance"]

  # Step 7: Generate quality report
  - id: generate_report
    action: |
      create comprehensive data quality report including:
      1. Source data statistics
      2. Data quality metrics before/after
      3. Transformation summary
      4. Issues found and resolved
      5. Validation results
      6. Processing performance metrics
      Format as markdown report
    depends_on: [validate_output]
    
    tags: ["reporting", "documentation"]

  # Step 8: Export processed data
  - id: export_data
    action: |
      export processed data to {{output_path}} in {{output_format}} format:
      1. Optimize for the target format (compression, partitioning)
      2. Include metadata and schema information
      3. Create indexes if applicable
      4. Set appropriate permissions
      Return export details and location
    depends_on: [validate_output]
    
    condition: "{{validate_output.result.passed}} == true"
    parallel: true  # Export different partitions in parallel
    timeout: 300.0  # 5 minutes for large exports
    on_error:
      action: |
        retry with smaller chunk size or fallback format
        retry_count: 3
      tags: ["export", "output"]

  # Step 9: Data lineage tracking
  - id: track_lineage
    action: |
      document complete data lineage:
      1. Source to target field mappings
      2. Transformation rules applied
      3. Data quality changes
      4. Processing timestamps
      5. Version information
      Save lineage metadata
    depends_on: [export_data]
    
    tags: ["lineage", "metadata"]

  # Step 10: Monitoring and alerts
  - id: monitor_pipeline
    action: |
      analyze pipeline execution and:
      1. Calculate processing time per stage
      2. Measure resource utilization
      3. Identify bottlenecks
      4. Check against SLAs
      5. Generate alerts for anomalies
      Return monitoring summary
    depends_on: [track_lineage]
    
    tags: ["monitoring", "performance"]

  # Step 11: Save processing report
  - id: save_processing_report
    action: |
      Write comprehensive data processing report to:
      examples/output/data_processing_{{source | regex_replace('[^a-zA-Z0-9]', '_')}}.md
      # Data Processing Report
      *Generated on: {{execution.timestamp}}*
      *Model: Ollama Llama2*
      ## Processing Summary
      - **Source**: {{source}}
      - **Output Path**: {{output_path}}
      - **Output Format**: {{output_format}}
      - **Total Records**: {{export_data.result.record_count | default('N/A')}}
      - **Processing Time**: {{monitor_pipeline.result.total_time | default('N/A')}}
      ## Data Profiling Results
      {{profile_data.result}}
      ## Validation Summary
      - **Total Issues Found**: {{profile_data.result.total_issues | default('0')}}
      - **Issues Resolved**: {{clean_data.result.issues_fixed | default('0')}}
      - **Data Quality Score**: {{validate_output.result.quality_score | default('N/A')}}
      ## Data Transformations Applied
      {{transform_data.result}}
      ## Quality Report
      {{generate_report.result}}
      ## Performance Metrics
      {{monitor_pipeline.result}}
      ## Data Lineage
      {{track_lineage.result}}
      ## Processing Configuration
      - **Chunk Size**: {{chunk_size}}
      - **Parallel Workers**: {{parallel_workers}}
      - **Validation Rules**: {{validation_rules | length}} rules applied
      - **Transformations**: {{transformations | length}} transformations
      ---
      *This report was generated by the Orchestrator Data Processing Workflow*
    depends_on: [monitor_pipeline, generate_report]
    tags: ["output", "file-save"]


  - id: save_output
    action: |
      Save the following content to examples/output/data_processing_workflow.md:
      # Data Processing Workflow
      *Generated on: {{execution.timestamp}}*
      *Pipeline: data_processing_workflow*
      ## Output
      {{generate_report.result}}
      ---
      *Generated by Orchestrator Data Processing Workflow Pipeline*
    depends_on: [save_processing_report]
outputs:
  processed_data_path: "{{export_data.result.output_path}}"
  total_records_processed: "{{export_data.result.record_count}}"
  data_quality_score: "{{validate_output.result.quality_score}}"
  quality_report: "{{generate_report.result}}"
  processing_time: "{{monitor_pipeline.result.total_time}}"
  lineage_metadata: "{{track_lineage.result}}"
  issues_found: "{{profile_data.result.total_issues}}"
  issues_resolved: "{{clean_data.result.issues_fixed}}"