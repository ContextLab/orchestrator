# Intelligent Fact-Checker Pipeline
# Demonstrates AUTO tags resolving to lists and runtime for_each expansion

id: intelligent-fact-checker
name: Intelligent Fact-Checker
description: Verifies claims and sources using AUTO tag list generation and runtime parallel processing
version: "3.0.0"

inputs:
  document_source:
    type: string
    description: Path to document file or URL to analyze
    required: true
  strictness:
    type: string
    description: How strict should fact-checking be (lenient/moderate/strict)
    default: "moderate"
  output_path:
    type: string
    description: Path where fact-check report should be saved
    required: false

outputs:
  fact_check_report:
    type: string
    value: "{{ output_path | default('examples/outputs/fact_checker/fact_check_report.md') }}"

steps:
  # Step 1: Load document using filesystem tool
  - id: load_document
    tool: filesystem
    action: read
    parameters:
      path: "{{ document_source }}"
    produces: text
    
  # Step 2: Extract sources as a list using AUTO tag
  - id: extract_sources_list
    dependencies:
      - load_document
    action: generate-structured
    parameters:
      prompt: |
        Analyze this document and extract all sources, citations, and references.
        
        Document content:
        {{ load_document.result }}
        
        Extract each source with its name, URL, and type (journal/report/website/other).
      schema:
        type: object
        properties:
          sources:
            type: array
            items:
              type: object
              properties:
                name:
                  type: string
                url:
                  type: string
                type:
                  type: string
                  enum: ["journal", "report", "website", "other"]
        required: [sources]
      max_completion_tokens: 1000
      model: "claude-sonnet-4-20250514"
    produces: json
    
  # Step 3: Extract claims as a list using AUTO tag
  - id: extract_claims_list
    dependencies:
      - load_document
    action: generate-structured
    parameters:
      prompt: |
        Extract all verifiable factual claims from this document.
        
        Document content:
        {{ load_document.result }}
        
        List each claim as a clear, concise statement that can be fact-checked.
      schema:
        type: object
        properties:
          claims:
            type: array
            items:
              type: string
        required: [claims]
      max_completion_tokens: 1000
      model: "claude-sonnet-4-20250514"
    produces: json
    
  # Step 4: Process sources in parallel using for_each with runtime expansion
  - id: verify_sources
    for_each: "{{ extract_sources_list.result.sources }}"
    max_parallel: 2
    add_completion_task: true
    steps:
      - id: verify_source
        action: generate-text
        parameters:
          prompt: |
            Verify this source for authenticity and accuracy:
            Name: {{ $item.name }}
            URL: {{ $item.url }}
            Type: {{ $item.type }}
            
            Check:
            1. Is the source real and accessible?
            2. Is it correctly cited?
            3. Is it a reputable source for its type?
            
            Provide structured analysis:
            - SOURCE: {{ $item.name }} ({{ $item.url }})
            - STATUS: Valid/Invalid/Questionable
            - CREDIBILITY: High/Medium/Low
            - NOTES: Brief observations
          max_completion_tokens: 300
          model: "claude-sonnet-4-20250514"
    dependencies:
      - extract_sources_list
    
  # Step 5: Process claims in parallel using for_each with runtime expansion
  - id: verify_claims
    for_each: "{{ extract_claims_list.result.claims }}"
    max_parallel: 3
    add_completion_task: true
    steps:
      - id: verify_claim
        action: generate-text
        parameters:
          prompt: |
            Verify this specific claim: {{ $item }}
            Claim number: {{ $index }}
            
            Using sources from document for verification.
            
            Provide professional fact-checking analysis:
            - CLAIM {{ $index }}: {{ $item }}
            - SUPPORT: Yes/Partial/No
            - RELIABILITY: High/Medium/Low
            - EVIDENCE: Brief summary of supporting/contradicting evidence
            - STATUS: Verified/Unverified/Disputed
          max_completion_tokens: 400
          model: "claude-sonnet-4-20250514"
    dependencies:
      - extract_claims_list
    
  # Step 6: Generate final report
  - id: generate_report
    dependencies:
      - verify_sources  # The ForEachTask itself
      - verify_claims   # The ForEachTask itself
      - extract_sources_list
      - extract_claims_list
    action: generate-text
    parameters:
      prompt: |
        # FACT-CHECKING REPORT: {{ document_source }}
        
        ## Executive Summary
        
        Document analyzed: {{ document_source }}
        Strictness level: {{ strictness }}
        
        Total sources identified: {{ extract_sources_list.result.sources | length }}
        Total claims identified: {{ extract_claims_list.result.claims | length }}
        
        ## Key Information
        
        ### Sources Found
        {% for source in extract_sources_list.result.sources %}
        - {{ source.name }} ({{ source.type }}): {{ source.url }}
        {% endfor %}
        
        ### Claims Analyzed
        {% for claim in extract_claims_list.result.claims %}
        {{ loop.index }}. {{ claim }}
        {% endfor %}
        
        ## Final Assessment
        
        Based on the parallel verification process:
        - Overall credibility rating: [Provide High/Medium/Low based on analysis]
        - Key findings: [2-3 bullet points summarizing verification results]
        - Recommendations: [Specific actionable recommendations]
        
        ---
        *This report demonstrates AUTO tag list generation and runtime parallel for_each processing.*
      max_completion_tokens: 2000
      model: "claude-sonnet-4-20250514"
    produces: text
    
  # Step 7: Save the report
  - id: save_report
    dependencies:
      - generate_report
    tool: filesystem
    action: write
    parameters:
      path: "{{ output_path | default('examples/outputs/fact_checker/fact_check_report.md') }}"
      content: |
        # Intelligent Fact-Checking Report
        
        **Document Analyzed:** {{ document_source }}
        **Strictness Level:** {{ strictness }}
        **Generated:** {{ execution.timestamp }}
        
        ---
        
        {{ generate_report.result }}
        
        ---
        
        ## Technical Details
        
        ### AUTO Tag List Generation (Runtime)
        
        This pipeline demonstrates AUTO tags resolving to lists at runtime:
        - **Sources extracted:** {{ extract_sources_list.result.sources | length }} sources
        - **Claims extracted:** {{ extract_claims_list.result.claims | length }} claims
        
        ### Runtime Parallel Processing with for_each
        
        The pipeline uses runtime `for_each` expansion with `max_parallel`:
        - **Sources verified in parallel:** max_parallel=2
        - **Claims verified in parallel:** max_parallel=3
        
        ### Extracted Data
        
        #### Sources (from AUTO tag)
        {% for source in extract_sources_list.result.sources %}
        {{ loop.index }}. {{ source.name }} ({{ source.type }}): {{ source.url }}
        {% endfor %}
        
        #### Claims (from AUTO tag)
        {% for claim in extract_claims_list.result.claims %}
        {{ loop.index }}. {{ claim }}
        {% endfor %}
        
        ---
        
        *Report generated using orchestrator framework with runtime for_each expansion*
        *Features: AUTO tag list generation, runtime parallel loop expansion*