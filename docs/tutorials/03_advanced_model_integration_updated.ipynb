{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Model Integration Tutorial\n",
        "\n",
        "This tutorial covers advanced techniques for integrating multiple AI models and providers with the Orchestrator framework.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The Orchestrator framework supports:\n",
        "\n",
        "- **Multiple Providers**: OpenAI, Anthropic, Google, local models\n",
        "- **Dynamic Model Selection**: Automatic best-fit model selection\n",
        "- **Model Capabilities**: Understanding what each model can do\n",
        "- **Fallback Strategies**: Automatic failover between models\n",
        "- **Cost Optimization**: Choosing models based on cost and performance\n",
        "- **Streaming Support**: Real-time response streaming\n",
        "\n",
        "## Setup\n",
        "\n",
        "Let's start by importing the necessary components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "from orchestrator.models.openai_model import OpenAIModel\n",
        "from orchestrator.models.anthropic_model import AnthropicModel\n",
        "from orchestrator.utils.api_keys import load_api_keys\n",
        "from orchestrator.models.model_registry import ModelRegistry\n",
        "from orchestrator.core.task import Task\n",
        "from orchestrator.core.pipeline import Pipeline\n",
        "from orchestrator.orchestrator import Orchestrator\n",
        "from orchestrator.state.state_manager import InMemoryStateManager\n",
        "\n",
        "# Load API keys\n",
        "load_api_keys()\n",
        "\n",
        "print(\"✅ Advanced model integration components imported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with Multiple Real Models\n",
        "\n",
        "Let's set up different models from various providers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models from different providers\n",
        "# Note: You need to have the appropriate API keys set as environment variables\n",
        "\n",
        "# OpenAI Models\n",
        "gpt4_model = OpenAIModel(\n",
        "    name=\"gpt-4\",\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "gpt35_model = OpenAIModel(\n",
        "    name=\"gpt-3.5-turbo\",\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "# Anthropic Model (if you have an API key)\n",
        "if os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
        "    claude_model = AnthropicModel(\n",
        "        name=\"claude-2\",\n",
        "        api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
        "    )\n",
        "    print(\"✅ Anthropic Claude model configured\")\n",
        "else:\n",
        "    claude_model = None\n",
        "    print(\"ℹ️ Anthropic API key not found, skipping Claude model\")\n",
        "\n",
        "print(f\"✅ OpenAI models configured: GPT-4 and GPT-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Model Registry\n",
        "\n",
        "The model registry helps manage multiple models and their selection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model registry and register models\n",
        "registry = ModelRegistry()\n",
        "\n",
        "# Register OpenAI models\n",
        "registry.register_model(gpt4_model)\n",
        "registry.register_model(gpt35_model)\n",
        "\n",
        "# Register Anthropic model if available\n",
        "if claude_model:\n",
        "    registry.register_model(claude_model)\n",
        "\n",
        "print(f\"🗂️ Model Registry created with {len(registry.list_models())} models:\")\n",
        "for model_name in registry.list_models():\n",
        "    model = registry.get_model(model_name)\n",
        "    print(f\"   📱 {model.name} - Ready for use\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection Based on Task Requirements\n",
        "\n",
        "Different models have different strengths. Let's create tasks that benefit from specific models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a pipeline with model-specific tasks\n",
        "multi_model_pipeline = Pipeline(\n",
        "    id=\"multi_model_demo\",\n",
        "    name=\"Multi-Model Demonstration Pipeline\",\n",
        "    description=\"Demonstrates using different models for different tasks\"\n",
        ")\n",
        "\n",
        "# Task 1: Complex reasoning (best suited for GPT-4)\n",
        "reasoning_task = Task(\n",
        "    id=\"complex_reasoning\",\n",
        "    name=\"Complex Reasoning Task\",\n",
        "    action=\"generate\",\n",
        "    parameters={\n",
        "        \"prompt\": \"Solve this logic puzzle: Three friends (Alice, Bob, Charlie) have different colored hats (red, blue, green). Alice's hat is not red. Bob's hat is not green. Charlie's hat is not blue. If Alice's hat is green, what color is each person's hat?\",\n",
        "        \"max_tokens\": 200\n",
        "    },\n",
        "    metadata={\n",
        "        \"preferred_model\": \"gpt-4\",  # Complex reasoning benefits from GPT-4\n",
        "        \"requirements\": {\n",
        "            \"reasoning_capability\": \"high\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Task 2: Quick summary (efficient with GPT-3.5)\n",
        "summary_task = Task(\n",
        "    id=\"quick_summary\",\n",
        "    name=\"Quick Summary\",\n",
        "    action=\"generate\",\n",
        "    parameters={\n",
        "        \"prompt\": \"Summarize the solution in one sentence: {complex_reasoning}\",\n",
        "        \"max_tokens\": 50\n",
        "    },\n",
        "    dependencies=[\"complex_reasoning\"],\n",
        "    metadata={\n",
        "        \"preferred_model\": \"gpt-3.5-turbo\",  # Simple task, use efficient model\n",
        "        \"requirements\": {\n",
        "            \"speed\": \"fast\",\n",
        "            \"cost\": \"low\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Task 3: Creative expansion (could use any capable model)\n",
        "creative_task = Task(\n",
        "    id=\"creative_expansion\",\n",
        "    name=\"Creative Expansion\",\n",
        "    action=\"generate\",\n",
        "    parameters={\n",
        "        \"prompt\": \"Write a short creative story about people wearing the hats from this puzzle: {complex_reasoning}\",\n",
        "        \"max_tokens\": 300\n",
        "    },\n",
        "    dependencies=[\"complex_reasoning\"],\n",
        "    metadata={\n",
        "        \"preferred_model\": \"any\",  # Any model can handle creative tasks\n",
        "        \"requirements\": {\n",
        "            \"creativity\": \"high\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add tasks to pipeline\n",
        "multi_model_pipeline.add_task(reasoning_task)\n",
        "multi_model_pipeline.add_task(summary_task)\n",
        "multi_model_pipeline.add_task(creative_task)\n",
        "\n",
        "print(f\"🧠 Multi-model pipeline created with {len(multi_model_pipeline)} tasks\")\n",
        "print(f\"📋 Execution order: {multi_model_pipeline.get_execution_order()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Smart Orchestrator with Model Assignment\n",
        "\n",
        "Create an orchestrator that can intelligently assign models to tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SmartOrchestrator(Orchestrator):\n",
        "    \"\"\"Enhanced orchestrator with intelligent model assignment.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_manager, model_registry: ModelRegistry):\n",
        "        super().__init__(state_manager)\n",
        "        self.model_registry = model_registry\n",
        "        \n",
        "        # Register all models from registry\n",
        "        for model_name in model_registry.list_models():\n",
        "            model = model_registry.get_model(model_name)\n",
        "            self.register_model(model)\n",
        "    \n",
        "    def select_model_for_task(self, task: Task) -> str:\n",
        "        \"\"\"Select the best model for a given task.\"\"\"\n",
        "        # Check if task has a preferred model\n",
        "        preferred = task.metadata.get(\"preferred_model\")\n",
        "        \n",
        "        if preferred and preferred != \"any\":\n",
        "            # Use preferred model if available\n",
        "            if preferred in self.models:\n",
        "                return preferred\n",
        "        \n",
        "        # Otherwise, select based on requirements\n",
        "        requirements = task.metadata.get(\"requirements\", {})\n",
        "        \n",
        "        # Simple heuristic for model selection\n",
        "        if requirements.get(\"reasoning_capability\") == \"high\":\n",
        "            return \"gpt-4\" if \"gpt-4\" in self.models else \"gpt-3.5-turbo\"\n",
        "        elif requirements.get(\"speed\") == \"fast\" or requirements.get(\"cost\") == \"low\":\n",
        "            return \"gpt-3.5-turbo\"\n",
        "        else:\n",
        "            # Default to first available model\n",
        "            return list(self.models.keys())[0]\n",
        "    \n",
        "    async def execute_pipeline_with_smart_assignment(self, pipeline: Pipeline) -> Dict[str, Any]:\n",
        "        \"\"\"Execute pipeline with automatic model assignment.\"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        print(f\"🧠 Executing pipeline with smart model assignment\\n\")\n",
        "        \n",
        "        for task_id in pipeline.get_execution_order()[0]:  # Simple sequential execution\n",
        "            task = pipeline.get_task(task_id)\n",
        "            \n",
        "            # Select best model for this task\n",
        "            selected_model_name = self.select_model_for_task(task)\n",
        "            selected_model = self.models[selected_model_name]\n",
        "            \n",
        "            print(f\"📋 Task: {task.name}\")\n",
        "            print(f\"   🤖 Selected Model: {selected_model_name}\")\n",
        "            \n",
        "            # Execute task\n",
        "            task.start()\n",
        "            \n",
        "            try:\n",
        "                # Prepare prompt with dependencies\n",
        "                prompt = task.parameters[\"prompt\"]\n",
        "                for dep in task.dependencies:\n",
        "                    if dep in results:\n",
        "                        prompt = prompt.replace(f\"{{{dep}}}\", results[dep])\n",
        "                \n",
        "                # Execute with selected model\n",
        "                result = await selected_model.generate(\n",
        "                    prompt,\n",
        "                    max_tokens=task.parameters.get(\"max_tokens\", 100)\n",
        "                )\n",
        "                \n",
        "                task.complete(result)\n",
        "                results[task_id] = result\n",
        "                print(f\"   ✅ Completed successfully\\n\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                task.fail(e)\n",
        "                print(f\"   ❌ Failed: {e}\\n\")\n",
        "                raise\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Create smart orchestrator\n",
        "state_manager = InMemoryStateManager()\n",
        "smart_orchestrator = SmartOrchestrator(state_manager, registry)\n",
        "\n",
        "print(f\"🧠 Smart orchestrator created with {len(smart_orchestrator.models)} models\")\n",
        "print(f\"📊 Available models: {list(smart_orchestrator.models.keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute the Multi-Model Pipeline\n",
        "\n",
        "Now let's run our pipeline with real AI models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_multi_model_pipeline():\n",
        "    \"\"\"Execute the multi-model pipeline.\"\"\"\n",
        "    print(\"🚀 Starting multi-model pipeline execution...\\n\")\n",
        "    \n",
        "    try:\n",
        "        # Execute with smart model assignment\n",
        "        results = await smart_orchestrator.execute_pipeline_with_smart_assignment(multi_model_pipeline)\n",
        "        \n",
        "        print(\"\\n📊 Pipeline Results:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        for task_id, result in results.items():\n",
        "            task = multi_model_pipeline.get_task(task_id)\n",
        "            print(f\"\\n🔸 {task.name}:\")\n",
        "            print(f\"{result}\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pipeline execution failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Execute the pipeline\n",
        "# Note: This will make real API calls to OpenAI/Anthropic\n",
        "results = await run_multi_model_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Performance Comparison\n",
        "\n",
        "Let's compare how different models perform on the same task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def compare_models_on_task(prompt: str, max_tokens: int = 100):\n",
        "    \"\"\"Compare different models on the same task.\"\"\"\n",
        "    print(f\"🔬 Comparing models on task:\\n'{prompt}'\\n\")\n",
        "    \n",
        "    comparison_results = {}\n",
        "    \n",
        "    for model_name in smart_orchestrator.models:\n",
        "        model = smart_orchestrator.models[model_name]\n",
        "        print(f\"Testing {model_name}...\")\n",
        "        \n",
        "        try:\n",
        "            import time\n",
        "            start_time = time.time()\n",
        "            \n",
        "            result = await model.generate(prompt, max_tokens=max_tokens)\n",
        "            \n",
        "            end_time = time.time()\n",
        "            execution_time = end_time - start_time\n",
        "            \n",
        "            comparison_results[model_name] = {\n",
        "                \"response\": result,\n",
        "                \"execution_time\": execution_time,\n",
        "                \"tokens_approx\": len(result.split())  # Rough approximation\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            comparison_results[model_name] = {\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n📊 Comparison Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for model_name, result in comparison_results.items():\n",
        "        print(f\"\\n🤖 {model_name}:\")\n",
        "        if \"error\" in result:\n",
        "            print(f\"   ❌ Error: {result['error']}\")\n",
        "        else:\n",
        "            print(f\"   ⏱️ Time: {result['execution_time']:.2f}s\")\n",
        "            print(f\"   📝 Tokens (approx): {result['tokens_approx']}\")\n",
        "            print(f\"   Response: {result['response'][:200]}...\" if len(result['response']) > 200 else f\"   Response: {result['response']}\")\n",
        "    \n",
        "    return comparison_results\n",
        "\n",
        "# Compare models on a specific task\n",
        "comparison_prompt = \"Explain the concept of recursion in programming using a simple analogy.\"\n",
        "comparison = await compare_models_on_task(comparison_prompt, max_tokens=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost-Aware Pipeline Execution\n",
        "\n",
        "Monitor and optimize costs across different models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define approximate costs per 1K tokens (example values)\n",
        "MODEL_COSTS = {\n",
        "    \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
        "    \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002},\n",
        "    \"claude-2\": {\"input\": 0.008, \"output\": 0.024}\n",
        "}\n",
        "\n",
        "def estimate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
        "    \"\"\"Estimate cost for a model execution.\"\"\"\n",
        "    if model_name not in MODEL_COSTS:\n",
        "        return 0.0\n",
        "    \n",
        "    costs = MODEL_COSTS[model_name]\n",
        "    input_cost = (input_tokens / 1000) * costs[\"input\"]\n",
        "    output_cost = (output_tokens / 1000) * costs[\"output\"]\n",
        "    \n",
        "    return input_cost + output_cost\n",
        "\n",
        "# Create a cost-optimized pipeline\n",
        "cost_pipeline = Pipeline(\n",
        "    id=\"cost_optimized\",\n",
        "    name=\"Cost-Optimized Pipeline\",\n",
        "    description=\"Demonstrates cost-aware model selection\"\n",
        ")\n",
        "\n",
        "# Mix of tasks with different cost sensitivities\n",
        "tasks = [\n",
        "    Task(\n",
        "        id=\"expensive_analysis\",\n",
        "        name=\"Complex Analysis (Worth GPT-4)\",\n",
        "        action=\"generate\",\n",
        "        parameters={\n",
        "            \"prompt\": \"Analyze the economic implications of quantum computing on cybersecurity markets over the next decade.\",\n",
        "            \"max_tokens\": 300\n",
        "        },\n",
        "        metadata={\"cost_sensitivity\": \"low\", \"quality_requirement\": \"high\"}\n",
        "    ),\n",
        "    Task(\n",
        "        id=\"simple_summary\",\n",
        "        name=\"Simple Summary (Use Efficient Model)\",\n",
        "        action=\"generate\",\n",
        "        parameters={\n",
        "            \"prompt\": \"List three key points from: {expensive_analysis}\",\n",
        "            \"max_tokens\": 100\n",
        "        },\n",
        "        dependencies=[\"expensive_analysis\"],\n",
        "        metadata={\"cost_sensitivity\": \"high\", \"quality_requirement\": \"medium\"}\n",
        "    ),\n",
        "    Task(\n",
        "        id=\"translation\",\n",
        "        name=\"Translation (Any Model Works)\",\n",
        "        action=\"generate\",\n",
        "        parameters={\n",
        "            \"prompt\": \"Translate to Spanish: {simple_summary}\",\n",
        "            \"max_tokens\": 150\n",
        "        },\n",
        "        dependencies=[\"simple_summary\"],\n",
        "        metadata={\"cost_sensitivity\": \"medium\", \"quality_requirement\": \"medium\"}\n",
        "    )\n",
        "]\n",
        "\n",
        "for task in tasks:\n",
        "    cost_pipeline.add_task(task)\n",
        "\n",
        "print(\"💰 Cost-optimized pipeline created\")\n",
        "print(\"📊 Tasks with cost sensitivities:\")\n",
        "for task in tasks:\n",
        "    print(f\"   - {task.name}: {task.metadata['cost_sensitivity']} cost sensitivity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Fallback and Error Handling\n",
        "\n",
        "Implement robust error handling with model fallbacks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobustOrchestrator(SmartOrchestrator):\n",
        "    \"\"\"Orchestrator with fallback capabilities.\"\"\"\n",
        "    \n",
        "    async def execute_with_fallback(self, task: Task, primary_model: str) -> Any:\n",
        "        \"\"\"Execute task with fallback to other models if primary fails.\"\"\"\n",
        "        models_to_try = [primary_model]\n",
        "        \n",
        "        # Add other models as fallbacks\n",
        "        for model_name in self.models:\n",
        "            if model_name != primary_model:\n",
        "                models_to_try.append(model_name)\n",
        "        \n",
        "        last_error = None\n",
        "        \n",
        "        for model_name in models_to_try:\n",
        "            try:\n",
        "                print(f\"   🔄 Trying {model_name}...\")\n",
        "                model = self.models[model_name]\n",
        "                \n",
        "                result = await model.generate(\n",
        "                    task.parameters[\"prompt\"],\n",
        "                    max_tokens=task.parameters.get(\"max_tokens\", 100)\n",
        "                )\n",
        "                \n",
        "                print(f\"   ✅ Success with {model_name}\")\n",
        "                return result\n",
        "                \n",
        "            except Exception as e:\n",
        "                last_error = e\n",
        "                print(f\"   ⚠️ Failed with {model_name}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        # All models failed\n",
        "        raise Exception(f\"All models failed. Last error: {last_error}\")\n",
        "\n",
        "# Create robust orchestrator\n",
        "robust_orchestrator = RobustOrchestrator(state_manager, registry)\n",
        "\n",
        "print(\"🛡️ Robust orchestrator created with fallback capabilities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, you learned:\n",
        "\n",
        "1. **Real Model Integration**: Working with actual AI providers (OpenAI, Anthropic)\n",
        "2. **Model Registry**: Managing multiple models in a unified way\n",
        "3. **Smart Model Selection**: Choosing the right model for each task\n",
        "4. **Cost Optimization**: Balancing quality and cost across models\n",
        "5. **Performance Comparison**: Evaluating different models on the same tasks\n",
        "6. **Fallback Strategies**: Ensuring reliability with model fallbacks\n",
        "7. **Error Handling**: Graceful degradation when models fail\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **🎯 Match Models to Tasks**: Use powerful models for complex tasks, efficient models for simple ones\n",
        "- **💰 Monitor Costs**: Track API usage and optimize model selection\n",
        "- **🔄 Build Resilience**: Always have fallback options\n",
        "- **📊 Measure Performance**: Compare models to make informed decisions\n",
        "- **🧠 Smart Assignment**: Let the orchestrator choose models based on requirements\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "1. **Always Use Real API Keys**: Set up environment variables for each provider\n",
        "2. **Start Small**: Test with smaller models before using expensive ones\n",
        "3. **Monitor Usage**: Keep track of API calls and costs\n",
        "4. **Handle Errors**: Implement proper error handling and retries\n",
        "5. **Cache Results**: Avoid redundant API calls when possible\n",
        "6. **Set Limits**: Use max_tokens and other parameters to control costs\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Explore **Streaming Responses** for real-time generation\n",
        "- Learn about **Structured Output** with JSON schemas\n",
        "- Try **Fine-tuned Models** for specialized tasks\n",
        "- Implement **Custom Model Adapters** for proprietary models\n",
        "- Set up **Production Monitoring** for model performance\n",
        "\n",
        "---\n",
        "\n",
        "**Happy orchestrating with real AI models! 🚀🤖**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}