{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model Integration Tutorial\n",
    "\n",
    "This tutorial covers advanced techniques for integrating multiple AI models and providers with the Orchestrator framework.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Orchestrator framework supports:\n",
    "\n",
    "- **Multiple Providers**: OpenAI, Anthropic, Google, local models\n",
    "- **Dynamic Model Selection**: Automatic best-fit model selection\n",
    "- **Model Capabilities**: Understanding what each model can do\n",
    "- **Fallback Strategies**: Automatic failover between models\n",
    "- **Cost Optimization**: Choosing models based on cost and performance\n",
    "- **Streaming Support**: Real-time response streaming\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from orchestrator.core.model import Model, MockModel, ModelCapabilities\n",
    "from orchestrator.models.model_registry import ModelRegistry\n",
    "from orchestrator.core.task import Task\n",
    "from orchestrator.core.pipeline import Pipeline\n",
    "from orchestrator.orchestrator import Orchestrator\n",
    "from orchestrator.state.state_manager import InMemoryStateManager\n",
    "\n",
    "print(\"âœ… Advanced model integration components imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Capabilities and Requirements\n",
    "\n",
    "Understanding model capabilities is crucial for effective model selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define capabilities for different types of models\n",
    "\n",
    "# High-end creative model\n",
    "creative_capabilities = ModelCapabilities(\n",
    "    supported_tasks=[\"generate\", \"creative_writing\", \"storytelling\"],\n",
    "    max_tokens=4096,\n",
    "    supports_streaming=True,\n",
    "    supports_structured_output=False,\n",
    "    languages=[\"en\", \"es\", \"fr\", \"de\"],\n",
    "    response_formats=[\"text\", \"markdown\"],\n",
    "    cost_per_token=0.002,\n",
    "    latency_ms=1500\n",
    ")\n",
    "\n",
    "# Analytical model\n",
    "analytical_capabilities = ModelCapabilities(\n",
    "    supported_tasks=[\"analyze\", \"classify\", \"extract\", \"summarize\"],\n",
    "    max_tokens=2048,\n",
    "    supports_streaming=False,\n",
    "    supports_structured_output=True,\n",
    "    languages=[\"en\"],\n",
    "    response_formats=[\"text\", \"json\", \"structured\"],\n",
    "    cost_per_token=0.001,\n",
    "    latency_ms=800\n",
    ")\n",
    "\n",
    "# Fast, efficient model\n",
    "efficient_capabilities = ModelCapabilities(\n",
    "    supported_tasks=[\"generate\", \"translate\", \"simple_qa\"],\n",
    "    max_tokens=1024,\n",
    "    supports_streaming=True,\n",
    "    supports_structured_output=False,\n",
    "    languages=[\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"ja\", \"ko\", \"zh\"],\n",
    "    response_formats=[\"text\"],\n",
    "    cost_per_token=0.0005,\n",
    "    latency_ms=400\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ­ Creative Model Capabilities:\")\n",
    "print(f\"   Tasks: {creative_capabilities.supported_tasks}\")\n",
    "print(f\"   Max tokens: {creative_capabilities.max_tokens}\")\n",
    "print(f\"   Cost per token: ${creative_capabilities.cost_per_token}\")\n",
    "\n",
    "print(\"\\nðŸ” Analytical Model Capabilities:\")\n",
    "print(f\"   Tasks: {analytical_capabilities.supported_tasks}\")\n",
    "print(f\"   Structured output: {analytical_capabilities.supports_structured_output}\")\n",
    "print(f\"   Languages: {len(analytical_capabilities.languages)}\")\n",
    "\n",
    "print(\"\\nâš¡ Efficient Model Capabilities:\")\n",
    "print(f\"   Tasks: {efficient_capabilities.supported_tasks}\")\n",
    "print(f\"   Languages: {len(efficient_capabilities.languages)}\")\n",
    "print(f\"   Latency: {efficient_capabilities.latency_ms}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model Registry\n",
    "\n",
    "The model registry helps manage multiple models and their selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock models with different capabilities\n",
    "creative_model = MockModel(\n",
    "    name=\"creative-gpt\",\n",
    "    provider=\"openai\",\n",
    "    capabilities=creative_capabilities\n",
    ")\n",
    "\n",
    "analytical_model = MockModel(\n",
    "    name=\"claude-analytical\", \n",
    "    provider=\"anthropic\",\n",
    "    capabilities=analytical_capabilities\n",
    ")\n",
    "\n",
    "efficient_model = MockModel(\n",
    "    name=\"gemini-efficient\",\n",
    "    provider=\"google\",\n",
    "    capabilities=efficient_capabilities\n",
    ")\n",
    "\n",
    "# Create model registry and register models\n",
    "registry = ModelRegistry()\n",
    "registry.register_model(creative_model)\n",
    "registry.register_model(analytical_model)\n",
    "registry.register_model(efficient_model)\n",
    "\n",
    "print(f\"ðŸ—‚ï¸ Model Registry created with {len(registry.list_models())} models:\")\n",
    "for model_name in registry.list_models():\n",
    "    model = registry.get_model(model_name)\n",
    "    print(f\"   ðŸ“± {model.name} ({model.provider}) - {len(model.capabilities.supported_tasks)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent Model Selection\n",
    "\n",
    "The registry can automatically select the best model based on requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different selection criteria\n",
    "\n",
    "# 1. Select by task type\n",
    "creative_requirements = {\"tasks\": [\"creative_writing\"]}\n",
    "selected = registry.select_model(creative_requirements)\n",
    "print(f\"ðŸŽ¨ Best model for creative writing: {selected.name if selected else 'None'}\")\n",
    "\n",
    "# 2. Select by structured output capability\n",
    "structured_requirements = {\"supports_structured_output\": True}\n",
    "selected = registry.select_model(structured_requirements)\n",
    "print(f\"ðŸ“Š Best model for structured output: {selected.name if selected else 'None'}\")\n",
    "\n",
    "# 3. Select by cost optimization\n",
    "cost_requirements = {\"max_cost_per_token\": 0.001}\n",
    "selected = registry.select_model(cost_requirements)\n",
    "print(f\"ðŸ’° Most cost-effective model: {selected.name if selected else 'None'}\")\n",
    "\n",
    "# 4. Select by language support\n",
    "multilingual_requirements = {\"languages\": [\"zh\", \"ja\"]}\n",
    "selected = registry.select_model(multilingual_requirements)\n",
    "print(f\"ðŸŒ Best model for Chinese/Japanese: {selected.name if selected else 'None'}\")\n",
    "\n",
    "# 5. Select by performance requirements\n",
    "performance_requirements = {\"max_latency_ms\": 500}\n",
    "selected = registry.select_model(performance_requirements)\n",
    "print(f\"âš¡ Fastest model: {selected.name if selected else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Selection Strategies\n",
    "\n",
    "Let's implement some advanced selection strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedModelSelector:\n",
    "    \"\"\"Advanced model selection with custom strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, registry: ModelRegistry):\n",
    "        self.registry = registry\n",
    "    \n",
    "    def select_by_cost_performance_ratio(self, task_type: str) -> Model:\n",
    "        \"\"\"Select model with best cost/performance ratio for a task.\"\"\"\n",
    "        suitable_models = []\n",
    "        \n",
    "        for model_name in self.registry.list_models():\n",
    "            model = self.registry.get_model(model_name)\n",
    "            if task_type in model.capabilities.supported_tasks:\n",
    "                # Calculate cost-performance score (lower is better)\n",
    "                cost_score = model.capabilities.cost_per_token or 0.01\n",
    "                latency_score = (model.capabilities.latency_ms or 1000) / 1000\n",
    "                combined_score = cost_score * latency_score\n",
    "                \n",
    "                suitable_models.append((model, combined_score))\n",
    "        \n",
    "        if suitable_models:\n",
    "            # Return model with lowest cost-performance score\n",
    "            return min(suitable_models, key=lambda x: x[1])[0]\n",
    "        return None\n",
    "    \n",
    "    def select_with_fallback(self, requirements: Dict[str, Any]) -> List[Model]:\n",
    "        \"\"\"Select primary model with fallback options.\"\"\"\n",
    "        models = []\n",
    "        \n",
    "        # Primary: exact match\n",
    "        primary = self.registry.select_model(requirements)\n",
    "        if primary:\n",
    "            models.append(primary)\n",
    "        \n",
    "        # Fallback 1: relax some requirements\n",
    "        relaxed_requirements = requirements.copy()\n",
    "        if \"max_latency_ms\" in relaxed_requirements:\n",
    "            relaxed_requirements[\"max_latency_ms\"] *= 2\n",
    "        \n",
    "        fallback1 = self.registry.select_model(relaxed_requirements)\n",
    "        if fallback1 and fallback1 not in models:\n",
    "            models.append(fallback1)\n",
    "        \n",
    "        # Fallback 2: any model supporting the task\n",
    "        if \"tasks\" in requirements:\n",
    "            task_requirements = {\"tasks\": requirements[\"tasks\"]}\n",
    "            fallback2 = self.registry.select_model(task_requirements)\n",
    "            if fallback2 and fallback2 not in models:\n",
    "                models.append(fallback2)\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def select_by_load_balancing(self, task_type: str) -> Model:\n",
    "        \"\"\"Select model based on current load (simulated).\"\"\"\n",
    "        suitable_models = []\n",
    "        \n",
    "        for model_name in self.registry.list_models():\n",
    "            model = self.registry.get_model(model_name)\n",
    "            if task_type in model.capabilities.supported_tasks:\n",
    "                # Simulate current load (in real implementation, this would be actual metrics)\n",
    "                import random\n",
    "                simulated_load = random.uniform(0.1, 0.9)\n",
    "                suitable_models.append((model, simulated_load))\n",
    "        \n",
    "        if suitable_models:\n",
    "            # Return model with lowest simulated load\n",
    "            return min(suitable_models, key=lambda x: x[1])[0]\n",
    "        return None\n",
    "\n",
    "# Test advanced selection strategies\n",
    "selector = AdvancedModelSelector(registry)\n",
    "\n",
    "# Cost-performance optimization\n",
    "best_ratio_model = selector.select_by_cost_performance_ratio(\"generate\")\n",
    "print(f\"ðŸ’¡ Best cost/performance for generation: {best_ratio_model.name if best_ratio_model else 'None'}\")\n",
    "\n",
    "# Fallback strategy\n",
    "strict_requirements = {\n",
    "    \"tasks\": [\"analyze\"],\n",
    "    \"supports_structured_output\": True,\n",
    "    \"max_latency_ms\": 500\n",
    "}\n",
    "fallback_models = selector.select_with_fallback(strict_requirements)\n",
    "print(f\"ðŸ”„ Fallback chain: {[m.name for m in fallback_models]}\")\n",
    "\n",
    "# Load balancing\n",
    "load_balanced_model = selector.select_by_load_balancing(\"generate\")\n",
    "print(f\"âš–ï¸ Load balanced selection: {load_balanced_model.name if load_balanced_model else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Aware Pipeline Execution\n",
    "\n",
    "Create a pipeline that intelligently assigns tasks to appropriate models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with model-specific tasks\n",
    "smart_pipeline = Pipeline(\n",
    "    id=\"smart_model_pipeline\",\n",
    "    name=\"Smart Model Selection Pipeline\",\n",
    "    description=\"Demonstrates intelligent model assignment\"\n",
    ")\n",
    "\n",
    "# Task 1: Creative content generation (should use creative model)\n",
    "creative_task = Task(\n",
    "    id=\"create_story\",\n",
    "    name=\"Create Story\",\n",
    "    action=\"creative_writing\",\n",
    "    parameters={\n",
    "        \"prompt\": \"Write a short science fiction story about time travel\",\n",
    "        \"style\": \"dramatic\",\n",
    "        \"length\": \"medium\"\n",
    "    },\n",
    "    metadata={\n",
    "        \"preferred_model_type\": \"creative\",\n",
    "        \"requirements\": {\n",
    "            \"tasks\": [\"creative_writing\"],\n",
    "            \"min_tokens\": 2000\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Task 2: Analysis (should use analytical model)\n",
    "analysis_task = Task(\n",
    "    id=\"analyze_story\",\n",
    "    name=\"Analyze Story\",\n",
    "    action=\"analyze\",\n",
    "    parameters={\n",
    "        \"prompt\": \"Analyze the narrative structure and themes in: {create_story.result}\",\n",
    "        \"output_format\": \"structured\"\n",
    "    },\n",
    "    dependencies=[\"create_story\"],\n",
    "    metadata={\n",
    "        \"preferred_model_type\": \"analytical\",\n",
    "        \"requirements\": {\n",
    "            \"tasks\": [\"analyze\"],\n",
    "            \"supports_structured_output\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Task 3: Quick summary (should use efficient model)\n",
    "summary_task = Task(\n",
    "    id=\"summarize\",\n",
    "    name=\"Quick Summary\",\n",
    "    action=\"generate\",\n",
    "    parameters={\n",
    "        \"prompt\": \"Create a brief 50-word summary of: {create_story.result}\",\n",
    "        \"max_length\": 50\n",
    "    },\n",
    "    dependencies=[\"create_story\"],\n",
    "    metadata={\n",
    "        \"preferred_model_type\": \"efficient\",\n",
    "        \"requirements\": {\n",
    "            \"tasks\": [\"generate\"],\n",
    "            \"max_latency_ms\": 600,\n",
    "            \"max_cost_per_token\": 0.001\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add tasks to pipeline\n",
    "smart_pipeline.add_task(creative_task)\n",
    "smart_pipeline.add_task(analysis_task)\n",
    "smart_pipeline.add_task(summary_task)\n",
    "\n",
    "print(f\"ðŸ§  Smart pipeline created with {len(smart_pipeline)} tasks\")\n",
    "print(f\"ðŸ“‹ Execution order: {smart_pipeline.get_execution_order()}\")\n",
    "\n",
    "# Show model assignments\n",
    "print(\"\\nðŸŽ¯ Recommended model assignments:\")\n",
    "for task_id in smart_pipeline:\n",
    "    task = smart_pipeline.get_task(task_id)\n",
    "    requirements = task.metadata.get(\"requirements\", {})\n",
    "    recommended_models = selector.select_with_fallback(requirements)\n",
    "    \n",
    "    print(f\"   {task.name}:\")\n",
    "    if recommended_models:\n",
    "        primary = recommended_models[0]\n",
    "        print(f\"     Primary: {primary.name} ({primary.provider})\")\n",
    "        if len(recommended_models) > 1:\n",
    "            fallbacks = [f\"{m.name} ({m.provider})\" for m in recommended_models[1:]]\n",
    "            print(f\"     Fallbacks: {', '.join(fallbacks)}\")\n",
    "    else:\n",
    "        print(\"     No suitable model found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Mock Responses\n",
    "\n",
    "Configure our mock models with appropriate responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure creative model responses\n",
    "creative_story = \"\"\"\n",
    "Dr. Sarah Chen stared at the temporal displacement device, her life's work finally complete. \n",
    "The swirling vortex of blue energy hummed with possibilities. She had spent fifteen years \n",
    "perfecting the equations, but nothing could prepare her for what happened next.\n",
    "\n",
    "As she stepped through the portal, time fractured around her. She emerged in 1955, in her \n",
    "grandfather's laboratory. But something was wrongâ€”the equations on his blackboard were \n",
    "identical to hers. With growing horror, she realized the truth: she hadn't invented time \n",
    "travel. She had inherited it.\n",
    "\n",
    "The device in 1955 activated, sending her grandfather forward to 2024. In the present, \n",
    "an old man she'd never met knocked on her door, claiming to be her grandfather, \n",
    "carrying blueprints that looked exactly like her own.\n",
    "\n",
    "Time, it seemed, was a closed loopâ€”and she was both its beginning and its end.\n",
    "\"\"\".strip()\n",
    "\n",
    "creative_model.set_response(\n",
    "    \"Write a short science fiction story about time travel\",\n",
    "    creative_story\n",
    ")\n",
    "\n",
    "# Configure analytical model responses\n",
    "analysis_result = {\n",
    "    \"narrative_structure\": {\n",
    "        \"type\": \"circular_narrative\",\n",
    "        \"elements\": [\"setup\", \"discovery\", \"revelation\", \"closed_loop\"]\n",
    "    },\n",
    "    \"themes\": [\n",
    "        \"causality_paradox\",\n",
    "        \"scientific_inheritance\", \n",
    "        \"temporal_determinism\",\n",
    "        \"family_legacy\"\n",
    "    ],\n",
    "    \"literary_devices\": [\n",
    "        \"foreshadowing\",\n",
    "        \"dramatic_irony\",\n",
    "        \"temporal_symmetry\"\n",
    "    ],\n",
    "    \"tone\": \"mysterious_and_contemplative\",\n",
    "    \"character_development\": \"revelation_driven\"\n",
    "}\n",
    "\n",
    "analytical_model.set_response(\n",
    "    \"Analyze the narrative structure and themes\",\n",
    "    json.dumps(analysis_result, indent=2)\n",
    ")\n",
    "\n",
    "# Configure efficient model responses\n",
    "efficient_model.set_response(\n",
    "    \"Create a brief 50-word summary\",\n",
    "    \"Dr. Sarah Chen discovers her time travel device was inherited from her grandfather \"\n",
    "    \"through a temporal paradox. Her invention creates a closed loop where past and \"\n",
    "    \"future are connected, revealing that time travel was both her discovery and her legacy.\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Mock responses configured for all models\")\n",
    "print(f\"   ðŸ“ Creative story: {len(creative_story)} characters\")\n",
    "print(f\"   ðŸ“Š Analysis structure: {len(analysis_result)} sections\")\n",
    "print(\"   ðŸ“„ Summary: concise overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Orchestrator with Model Assignment\n",
    "\n",
    "Create an orchestrator that automatically assigns tasks to optimal models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartOrchestrator(Orchestrator):\n",
    "    \"\"\"Enhanced orchestrator with intelligent model assignment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_manager, model_registry: ModelRegistry):\n",
    "        super().__init__(state_manager)\n",
    "        self.model_registry = model_registry\n",
    "        self.selector = AdvancedModelSelector(model_registry)\n",
    "        \n",
    "        # Register all models from registry\n",
    "        for model_name in model_registry.list_models():\n",
    "            model = model_registry.get_model(model_name)\n",
    "            self.register_model(model)\n",
    "    \n",
    "    def assign_optimal_models(self, pipeline: Pipeline) -> Dict[str, str]:\n",
    "        \"\"\"Assign optimal models to pipeline tasks.\"\"\"\n",
    "        assignments = {}\n",
    "        \n",
    "        for task_id in pipeline:\n",
    "            task = pipeline.get_task(task_id)\n",
    "            requirements = task.metadata.get(\"requirements\", {})\n",
    "            \n",
    "            # Try to find optimal model\n",
    "            fallback_models = self.selector.select_with_fallback(requirements)\n",
    "            \n",
    "            if fallback_models:\n",
    "                assignments[task_id] = fallback_models[0].name\n",
    "                print(f\"âœ… Assigned {task.name} â†’ {fallback_models[0].name}\")\n",
    "                \n",
    "                # Store fallback options in task metadata\n",
    "                if len(fallback_models) > 1:\n",
    "                    fallback_names = [m.name for m in fallback_models[1:]]\n",
    "                    task.metadata[\"fallback_models\"] = fallback_names\n",
    "                    print(f\"   Fallbacks: {', '.join(fallback_names)}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ No suitable model found for {task.name}\")\n",
    "        \n",
    "        return assignments\n",
    "    \n",
    "    async def execute_with_smart_assignment(self, pipeline: Pipeline) -> bool:\n",
    "        \"\"\"Execute pipeline with automatic model assignment.\"\"\"\n",
    "        print(f\"ðŸ§  Executing smart pipeline: {pipeline.name}\")\n",
    "        \n",
    "        # Assign optimal models\n",
    "        assignments = self.assign_optimal_models(pipeline)\n",
    "        \n",
    "        # Execute with automatic failover\n",
    "        for task_id in pipeline:\n",
    "            task = pipeline.get_task(task_id)\n",
    "            \n",
    "            if task_id in assignments:\n",
    "                primary_model = assignments[task_id]\n",
    "                fallbacks = task.metadata.get(\"fallback_models\", [])\n",
    "                \n",
    "                # Try primary model first\n",
    "                success = await self._execute_task_with_model(task, primary_model)\n",
    "                \n",
    "                # Try fallbacks if primary fails\n",
    "                if not success:\n",
    "                    for fallback_model in fallbacks:\n",
    "                        print(f\"ðŸ”„ Trying fallback model: {fallback_model}\")\n",
    "                        success = await self._execute_task_with_model(task, fallback_model)\n",
    "                        if success:\n",
    "                            break\n",
    "                \n",
    "                if not success:\n",
    "                    print(f\"âŒ All models failed for task: {task.name}\")\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    async def _execute_task_with_model(self, task: Task, model_name: str) -> bool:\n",
    "        \"\"\"Execute a single task with specified model.\"\"\"\n",
    "        try:\n",
    "            model = self.models[model_name]\n",
    "            \n",
    "            # Simulate task execution\n",
    "            task.start()\n",
    "            \n",
    "            # Extract prompt from parameters\n",
    "            prompt = task.parameters.get(\"prompt\", \"\")\n",
    "            \n",
    "            # Execute based on action type\n",
    "            if task.action in [\"generate\", \"creative_writing\"]:\n",
    "                result = await model.generate(prompt, **task.parameters)\n",
    "            elif task.action == \"analyze\":\n",
    "                # For structured output, try generate_structured if available\n",
    "                if (hasattr(model, 'generate_structured') and \n",
    "                    model.capabilities.supports_structured_output):\n",
    "                    schema = {\"type\": \"object\"}  # Basic schema\n",
    "                    result = await model.generate_structured(prompt, schema)\n",
    "                else:\n",
    "                    result = await model.generate(prompt, **task.parameters)\n",
    "            else:\n",
    "                result = await model.generate(prompt, **task.parameters)\n",
    "            \n",
    "            task.complete(result)\n",
    "            print(f\"âœ… Completed {task.name} with {model_name}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed {task.name} with {model_name}: {e}\")\n",
    "            task.fail(e)\n",
    "            return False\n",
    "\n",
    "# Create smart orchestrator\n",
    "state_manager = InMemoryStateManager()\n",
    "smart_orchestrator = SmartOrchestrator(state_manager, registry)\n",
    "\n",
    "print(f\"ðŸ§  Smart orchestrator created with {len(smart_orchestrator.models)} models\")\n",
    "print(f\"ðŸ“Š Available models: {list(smart_orchestrator.models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Smart Pipeline\n",
    "\n",
    "Now let's execute our pipeline with intelligent model assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_smart_pipeline():\n",
    "    \"\"\"Execute the smart pipeline with automatic model assignment.\"\"\"\n",
    "    print(\"ðŸš€ Starting smart pipeline execution...\\n\")\n",
    "    \n",
    "    # Show initial pipeline state\n",
    "    print(f\"ðŸ“Š Pipeline: {smart_pipeline.name}\")\n",
    "    print(f\"ðŸ“‹ Tasks: {len(smart_pipeline)}\")\n",
    "    print(f\"âš¡ Execution order: {smart_pipeline.get_execution_order()}\")\n",
    "    print()\n",
    "    \n",
    "    # Execute with smart assignment\n",
    "    success = await smart_orchestrator.execute_with_smart_assignment(smart_pipeline)\n",
    "    \n",
    "    print(f\"\\n{'âœ…' if success else 'âŒ'} Pipeline execution {'completed' if success else 'failed'}\")\n",
    "    \n",
    "    # Show results\n",
    "    if success:\n",
    "        print(\"\\nðŸ“Š Task Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for task_id in smart_pipeline:\n",
    "            task = smart_pipeline.get_task(task_id)\n",
    "            print(f\"\\nðŸ”¸ {task.name}\")\n",
    "            print(f\"   Status: {task.status}\")\n",
    "            print(f\"   Action: {task.action}\")\n",
    "            \n",
    "            if task.result:\n",
    "                # Format result for display\n",
    "                result_text = str(task.result)\n",
    "                if len(result_text) > 300:\n",
    "                    result_text = result_text[:300] + \"...\"\n",
    "                print(f\"   Result: {result_text}\")\n",
    "            \n",
    "            # Show model assignment info\n",
    "            if \"fallback_models\" in task.metadata:\n",
    "                print(f\"   Fallbacks available: {len(task.metadata['fallback_models'])}\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Execute the smart pipeline\n",
    "result = await run_smart_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Monitoring\n",
    "\n",
    "Track model performance and costs across executions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate performance tracking\n",
    "class ModelPerformanceTracker:\n",
    "    \"\"\"Track model performance metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def record_execution(self, model_name: str, task_type: str, \n",
    "                        success: bool, latency_ms: int, tokens_used: int):\n",
    "        \"\"\"Record a model execution.\"\"\"\n",
    "        if model_name not in self.metrics:\n",
    "            self.metrics[model_name] = {\n",
    "                \"total_executions\": 0,\n",
    "                \"successful_executions\": 0,\n",
    "                \"total_latency_ms\": 0,\n",
    "                \"total_tokens\": 0,\n",
    "                \"task_types\": {}\n",
    "            }\n",
    "        \n",
    "        model_metrics = self.metrics[model_name]\n",
    "        model_metrics[\"total_executions\"] += 1\n",
    "        \n",
    "        if success:\n",
    "            model_metrics[\"successful_executions\"] += 1\n",
    "            model_metrics[\"total_latency_ms\"] += latency_ms\n",
    "            model_metrics[\"total_tokens\"] += tokens_used\n",
    "        \n",
    "        # Track by task type\n",
    "        if task_type not in model_metrics[\"task_types\"]:\n",
    "            model_metrics[\"task_types\"][task_type] = {\"count\": 0, \"success_rate\": 0}\n",
    "        \n",
    "        model_metrics[\"task_types\"][task_type][\"count\"] += 1\n",
    "        if success:\n",
    "            task_metrics = model_metrics[\"task_types\"][task_type]\n",
    "            task_metrics[\"success_rate\"] = (\n",
    "                (task_metrics[\"success_rate\"] * (task_metrics[\"count\"] - 1) + 1) /\n",
    "                task_metrics[\"count\"]\n",
    "            )\n",
    "    \n",
    "    def get_performance_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate performance report.\"\"\"\n",
    "        report = {}\n",
    "        \n",
    "        for model_name, metrics in self.metrics.items():\n",
    "            total_exec = metrics[\"total_executions\"]\n",
    "            successful_exec = metrics[\"successful_executions\"]\n",
    "            \n",
    "            success_rate = successful_exec / total_exec if total_exec > 0 else 0\n",
    "            avg_latency = (\n",
    "                metrics[\"total_latency_ms\"] / successful_exec \n",
    "                if successful_exec > 0 else 0\n",
    "            )\n",
    "            \n",
    "            report[model_name] = {\n",
    "                \"success_rate\": success_rate,\n",
    "                \"average_latency_ms\": avg_latency,\n",
    "                \"total_executions\": total_exec,\n",
    "                \"total_tokens\": metrics[\"total_tokens\"],\n",
    "                \"task_performance\": metrics[\"task_types\"]\n",
    "            }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Simulate some performance data\n",
    "tracker = ModelPerformanceTracker()\n",
    "\n",
    "# Simulate executions for our models\n",
    "import random\n",
    "\n",
    "# Creative model performance\n",
    "for _ in range(10):\n",
    "    tracker.record_execution(\n",
    "        \"creative-gpt\", \"creative_writing\", \n",
    "        True, random.randint(1000, 2000), random.randint(200, 500)\n",
    "    )\n",
    "\n",
    "# Analytical model performance  \n",
    "for _ in range(15):\n",
    "    tracker.record_execution(\n",
    "        \"claude-analytical\", \"analyze\",\n",
    "        random.choice([True, True, True, False]),  # 75% success rate\n",
    "        random.randint(600, 1200), random.randint(100, 300)\n",
    "    )\n",
    "\n",
    "# Efficient model performance\n",
    "for _ in range(20):\n",
    "    tracker.record_execution(\n",
    "        \"gemini-efficient\", \"generate\",\n",
    "        True, random.randint(200, 600), random.randint(50, 150)\n",
    "    )\n",
    "\n",
    "# Generate performance report\n",
    "performance_report = tracker.get_performance_report()\n",
    "\n",
    "print(\"ðŸ“Š Model Performance Report:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, metrics in performance_report.items():\n",
    "    print(f\"\\nðŸ¤– {model_name}:\")\n",
    "    print(f\"   Success Rate: {metrics['success_rate']:.1%}\")\n",
    "    print(f\"   Avg Latency: {metrics['average_latency_ms']:.0f}ms\")\n",
    "    print(f\"   Total Executions: {metrics['total_executions']}\")\n",
    "    print(f\"   Total Tokens: {metrics['total_tokens']:,}\")\n",
    "    \n",
    "    # Show task-specific performance\n",
    "    if metrics['task_performance']:\n",
    "        print(\"   Task Performance:\")\n",
    "        for task_type, task_metrics in metrics['task_performance'].items():\n",
    "            print(f\"     {task_type}: {task_metrics['success_rate']:.1%} ({task_metrics['count']} executions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis and Optimization\n",
    "\n",
    "Analyze costs across different models and execution strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostAnalyzer:\n",
    "    \"\"\"Analyze and optimize costs across models.\"\"\"\n",
    "    \n",
    "    def __init__(self, registry: ModelRegistry, tracker: ModelPerformanceTracker):\n",
    "        self.registry = registry\n",
    "        self.tracker = tracker\n",
    "    \n",
    "    def calculate_execution_costs(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate total costs per model.\"\"\"\n",
    "        costs = {}\n",
    "        \n",
    "        for model_name in self.registry.list_models():\n",
    "            model = self.registry.get_model(model_name)\n",
    "            cost_per_token = model.capabilities.cost_per_token or 0\n",
    "            \n",
    "            # Get token usage from performance tracker\n",
    "            performance = self.tracker.get_performance_report()\n",
    "            if model_name in performance:\n",
    "                total_tokens = performance[model_name][\"total_tokens\"]\n",
    "                total_cost = total_tokens * cost_per_token\n",
    "                costs[model_name] = total_cost\n",
    "            else:\n",
    "                costs[model_name] = 0.0\n",
    "        \n",
    "        return costs\n",
    "    \n",
    "    def analyze_cost_efficiency(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Analyze cost efficiency (cost per successful execution).\"\"\"\n",
    "        efficiency = {}\n",
    "        costs = self.calculate_execution_costs()\n",
    "        performance = self.tracker.get_performance_report()\n",
    "        \n",
    "        for model_name, total_cost in costs.items():\n",
    "            if model_name in performance:\n",
    "                perf = performance[model_name]\n",
    "                successful_exec = perf[\"total_executions\"] * perf[\"success_rate\"]\n",
    "                \n",
    "                if successful_exec > 0:\n",
    "                    cost_per_success = total_cost / successful_exec\n",
    "                    time_per_success = perf[\"average_latency_ms\"] / 1000  # Convert to seconds\n",
    "                    \n",
    "                    efficiency[model_name] = {\n",
    "                        \"cost_per_successful_execution\": cost_per_success,\n",
    "                        \"time_per_successful_execution\": time_per_success,\n",
    "                        \"cost_efficiency_score\": cost_per_success / max(time_per_success, 0.1)\n",
    "                    }\n",
    "        \n",
    "        return efficiency\n",
    "    \n",
    "    def recommend_cost_optimization(self) -> List[str]:\n",
    "        \"\"\"Provide cost optimization recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        efficiency = self.analyze_cost_efficiency()\n",
    "        \n",
    "        if not efficiency:\n",
    "            return [\"No performance data available for analysis\"]\n",
    "        \n",
    "        # Find most cost-efficient model\n",
    "        most_efficient = min(efficiency.items(), \n",
    "                           key=lambda x: x[1][\"cost_per_successful_execution\"])\n",
    "        \n",
    "        recommendations.append(\n",
    "            f\"Most cost-efficient model: {most_efficient[0]} \"\n",
    "            f\"(${most_efficient[1]['cost_per_successful_execution']:.4f} per success)\"\n",
    "        )\n",
    "        \n",
    "        # Find fastest model\n",
    "        fastest = min(efficiency.items(),\n",
    "                     key=lambda x: x[1][\"time_per_successful_execution\"])\n",
    "        \n",
    "        recommendations.append(\n",
    "            f\"Fastest model: {fastest[0]} \"\n",
    "            f\"({fastest[1]['time_per_successful_execution']:.2f}s per success)\"\n",
    "        )\n",
    "        \n",
    "        # Best overall efficiency (cost/time ratio)\n",
    "        best_overall = min(efficiency.items(),\n",
    "                          key=lambda x: x[1][\"cost_efficiency_score\"])\n",
    "        \n",
    "        recommendations.append(\n",
    "            f\"Best cost/time ratio: {best_overall[0]} \"\n",
    "            f\"(efficiency score: {best_overall[1]['cost_efficiency_score']:.4f})\"\n",
    "        )\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Perform cost analysis\n",
    "cost_analyzer = CostAnalyzer(registry, tracker)\n",
    "\n",
    "# Calculate costs\n",
    "execution_costs = cost_analyzer.calculate_execution_costs()\n",
    "cost_efficiency = cost_analyzer.analyze_cost_efficiency()\n",
    "recommendations = cost_analyzer.recommend_cost_optimization()\n",
    "\n",
    "print(\"ðŸ’° Cost Analysis Report:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nðŸ“Š Total Execution Costs:\")\n",
    "for model_name, cost in execution_costs.items():\n",
    "    print(f\"   {model_name}: ${cost:.4f}\")\n",
    "\n",
    "print(\"\\nâš¡ Cost Efficiency Analysis:\")\n",
    "for model_name, metrics in cost_efficiency.items():\n",
    "    print(f\"   {model_name}:\")\n",
    "    print(f\"     Cost per success: ${metrics['cost_per_successful_execution']:.4f}\")\n",
    "    print(f\"     Time per success: {metrics['time_per_successful_execution']:.2f}s\")\n",
    "    print(f\"     Efficiency score: {metrics['cost_efficiency_score']:.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Optimization Recommendations:\")\n",
    "for i, recommendation in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. **Model Capabilities**: Defining and understanding what each model can do\n",
    "2. **Model Registry**: Managing multiple models and their selection\n",
    "3. **Intelligent Selection**: Automatic model assignment based on requirements\n",
    "4. **Fallback Strategies**: Handling model failures gracefully\n",
    "5. **Performance Monitoring**: Tracking model execution metrics\n",
    "6. **Cost Optimization**: Analyzing and optimizing model usage costs\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "- **ðŸŽ¯ Optimal Performance**: Right model for each task\n",
    "- **ðŸ’° Cost Control**: Automatic cost optimization\n",
    "- **ðŸ”„ Reliability**: Fallback mechanisms prevent failures\n",
    "- **ðŸ“Š Insights**: Performance monitoring and analytics\n",
    "- **ðŸš€ Scalability**: Easy addition of new models and providers\n",
    "- **ðŸ§  Intelligence**: Automatic decision making\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Define Clear Capabilities**: Accurately specify what each model can do\n",
    "2. **Set Up Fallbacks**: Always have backup models for critical tasks\n",
    "3. **Monitor Performance**: Track success rates, latency, and costs\n",
    "4. **Optimize Regularly**: Use performance data to improve selections\n",
    "5. **Test Thoroughly**: Validate model assignments before production\n",
    "6. **Document Requirements**: Clear task requirements enable better selection\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore **Real Provider Integration** (OpenAI, Anthropic, Google)\n",
    "- Learn about **Custom Model Adapters** for proprietary models\n",
    "- Try **Advanced Orchestration Patterns** for complex workflows\n",
    "- Check out **Production Monitoring** and observability\n",
    "\n",
    "---\n",
    "\n",
    "**Smart orchestrating! ðŸ§ ðŸŽµ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}