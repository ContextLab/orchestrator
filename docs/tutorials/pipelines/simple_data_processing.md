# Pipeline Tutorial: simple_data_processing

## Overview

**Complexity Level**: Intermediate  
**Difficulty Score**: 30/100  
**Estimated Runtime**: 5-15 minutes  

### Purpose
This pipeline shows how to process and analyze data using orchestrator's data processing capabilities. It demonstrates csv_processing, template_variables for building robust data workflows.

### Use Cases
- Automated data processing workflows
- Business data analysis and reporting
- Data quality assessment and cleaning

### Prerequisites
- Basic understanding of YAML syntax
- Familiarity with template variables and data flow
- Understanding of basic control flow concepts

### Key Concepts
- Template variable substitution

## Pipeline Breakdown

### Configuration Analysis
- **input_section**: Defines the data inputs and parameters for the pipeline
- **steps_section**: Contains the sequence of operations to be executed
- **output_section**: Specifies how results are formatted and stored
- **template_usage**: Uses 4 template patterns for dynamic content
- **feature_highlights**: Demonstrates 2 key orchestrator features

### Data Flow
This pipeline processes input parameters through 2 steps to generate the specified outputs.

### Control Flow
Follows linear execution flow from first step to last step.

### Pipeline Configuration
```yaml
# Simple Data Processing Pipeline
# Uses real tools: filesystem and data-processing
id: simple_data_processing
name: Simple Data Processing Pipeline
description: Read a CSV file, process it, and save results
version: "1.0.0"

parameters:
  output_path:
    type: string
    default: "examples/outputs/simple_data_processing"
    description: Directory where output files will be saved

steps:
  - id: read_data
    tool: filesystem
    action: read
    parameters:
      path: "data/input.csv"
    
  - id: process_data
    tool: data-processing
    action: filter
    parameters:
      data: "{{ read_data.content }}"
      format: "csv"
      operation:
        criteria:
          status: "active"
    dependencies:
      - read_data
    
  - id: save_results
    tool: filesystem
    action: write
    parameters:
      path: "{{ output_path }}/output_{{ execution.timestamp | slugify }}.csv"
      content: "{{ process_data.processed_data }}"
    dependencies:
      - process_data
      
  - id: save_report
    tool: filesystem
    action: write
    parameters:
      path: "{{ output_path }}/report_{{ execution.timestamp | slugify }}.md"
      content: |
        # Simple Data Processing Report
        
        **Generated:** {{ execution.timestamp }}
        **Pipeline:** Simple Data Processing
        
        ## Processing Summary
        
        - **Input File:** data/input.csv
        - **Filter Applied:** status = "active"
        - **Output File:** {{ output_path }}/output_{{ execution.timestamp | slugify }}.csv
        
        ## Results
        
        The data processing pipeline successfully filtered the input CSV file to include only records with status="active".
        
        ### Filtered Data Preview:
        ```csv
        {{ process_data.processed_data | truncate(500) }}
        ```
        
        ---
        *Generated by Simple Data Processing Pipeline*
    dependencies:
      - process_data
```

## Customization Guide

### Input Modifications
- Modify input parameters to match your specific data sources
- Adjust file paths and data formats as needed for your environment

### Parameter Tuning
- Adjust step parameters to customize behavior for your needs

### Step Modifications
- Add new steps by following the same pattern as existing ones
- Remove steps that aren't needed for your specific use case
- Reorder steps if your workflow requires different sequencing
- Replace tool actions with alternatives that provide similar functionality

### Output Customization
- Change output file paths and formats to match your requirements
- Modify output templates to customize the structure and content
- This pipeline produces CSV data, Markdown documents, Reports - adjust output configuration accordingly

## Remixing Instructions

### Compatible Patterns
- Most basic pipelines can be combined with this pattern

### Extension Ideas
- Add iterative processing for continuous improvement
- Implement parallel processing for better performance
- Include advanced error recovery mechanisms

### Combination Examples
- Combine with research workflows to gather additional data
- Use with statistical analysis for comprehensive insights
- Integrate with visualization tools for data presentation

### Advanced Variations
- Scale to handle larger datasets and more complex processing
- Add real-time processing capabilities for streaming data
- Implement distributed processing across multiple systems

## Hands-On Exercise

### Execution Instructions
- 1. Navigate to your orchestrator project directory
- 2. Run: python scripts/run_pipeline.py examples/simple_data_processing.yaml
- 3. Monitor the output for progress and any error messages
- 4. Check the output directory for generated results

### Expected Outputs
- Generated CSV data in the specified output directory
- Generated Markdown documents in the specified output directory
- Generated Reports in the specified output directory
- Execution logs showing step-by-step progress
- Completion message with runtime statistics
- No error messages or warnings (successful execution)

### Troubleshooting
- **Template Resolution Errors**: Check that all input parameters are provided and template syntax is correct
- **General Execution Errors**: Check the logs for specific error messages and verify your orchestrator installation

### Verification Steps
- Check that the pipeline completed without errors
- Verify all expected output files were created
- Review the output content for quality and accuracy

---

*Tutorial generated on 2025-08-27T23:40:24.396588*
