# Pipeline Tutorial: research_minimal

## Overview

**Complexity Level**: Intermediate  
**Difficulty Score**: 45/100  
**Estimated Runtime**: 10-30 minutes  

### Purpose
This pipeline demonstrates how to build automated research workflows using the orchestrator toolbox. It showcases data_flow, interactive_workflows, llm_integration and provides a foundation for building more sophisticated research applications.

### Use Cases
- AI-powered content generation
- Academic research and literature review
- Fact-checking and information verification
- Information gathering and research
- Market research and competitive analysis

### Prerequisites
- Basic understanding of YAML syntax
- Familiarity with template variables and data flow
- Understanding of basic control flow concepts

### Key Concepts
- Data flow between pipeline steps
- Large language model integration
- Template variable substitution

## Pipeline Breakdown

### Configuration Analysis
- **input_section**: Defines the data inputs and parameters for the pipeline
- **steps_section**: Contains the sequence of operations to be executed
- **output_section**: Specifies how results are formatted and stored
- **template_usage**: Uses 8 template patterns for dynamic content
- **feature_highlights**: Demonstrates 5 key orchestrator features

### Data Flow
This pipeline processes input parameters through 5 steps to generate the specified outputs.

### Control Flow
Follows linear execution flow from first step to last step.

### Pipeline Configuration
```yaml
id: research_minimal
name: Minimal Research Pipeline
description: Simplest possible research - just search and summarize (requires only web-search tool)
version: "1.0.0"

parameters:
  topic:
    type: string
    required: true
    description: The research topic
  output_path:
    type: string
    default: "examples/outputs/research_minimal"
    description: Directory where output files will be saved

steps:
  - id: search_web
    tool: web-search
    action: search
    parameters:
      query: "{{topic}}"
      max_results: 5
  
  - id: summarize_results
    action: generate_text
    parameters:
      prompt: |
        Analyze these search results about {{topic}} and provide a structured response:
        
        {% for result in search_web.results %}
        {{loop.index}}. {{result.title}}
           {{result.snippet}}
           Source: {{result.url}}
        {% endfor %}
        
        Return a JSON object with the following structure:
        {
          "topic": "{{topic}}",
          "overview": "Brief one-sentence overview",
          "key_points": [
            "First main point (1-2 sentences)",
            "Second main point (1-2 sentences)",
            "Third main point (1-2 sentences)"
          ],
          "summary": "Concise 2-3 sentence summary of the main findings"
        }
        
        Ensure the response is valid JSON and contains only factual information from the sources.
      model: <AUTO task="summarize">Select appropriate model for summarization</AUTO>
      max_tokens: 500
      response_format: "json_object"
    dependencies:
      - search_web
  
  - id: save_summary
    tool: filesystem
    action: write
    parameters:
      path: "{{ output_path }}/{{topic | slugify}}_summary.md"
      content: |
        # Research Summary: {{topic}}
        **Date:** {{ execution.timestamp | date('%Y-%m-%d %H:%M:%S') }}
        **Sources Reviewed:** {{search_web.total_results}} sources reviewed

        ## Overview
        {% set summary_data = summarize_results.result | from_json %}
        {{summary_data.overview}}

        ## Key Findings
        {% for point in summary_data.key_points %}
        {{loop.index}}. {{point}}
        {% endfor %}

        ## Summary
        {{summary_data.summary}}

        ## Sources
        {% for result in search_web.results %}
        {{loop.index}}. [{{result.title}}]({{result.url}})
        {% endfor %}

        ---
        *Generated by Simple Research Pipeline*
    dependencies:
      - summarize_results

outputs:
  summary: "{{summarize_results.result}}"
  search_count: "{{search_web.total_results}}"
  output_file: "{{save_summary.filepath}}"
```

## Customization Guide

### Input Modifications
- Modify input parameters to match your specific data sources
- Adjust file paths and data formats as needed for your environment

### Parameter Tuning
- Adjust model parameters (temperature, max_tokens) for different output styles
- Modify prompts to change the tone and focus of generated content

### Step Modifications
- Add new steps by following the same pattern as existing ones
- Remove steps that aren't needed for your specific use case
- Reorder steps if your workflow requires different sequencing
- Replace tool actions with alternatives that provide similar functionality

### Output Customization
- Change output file paths and formats to match your requirements
- Modify output templates to customize the structure and content
- This pipeline produces JSON data, Markdown documents - adjust output configuration accordingly

## Remixing Instructions

### Compatible Patterns
- fact_checker.yaml - for content verification
- research workflows - for information gathering

### Extension Ideas
- Add iterative processing for continuous improvement
- Implement parallel processing for better performance
- Include advanced error recovery mechanisms

### Combination Examples
- Combine with fact_checker.yaml to verify research claims
- Use with creative_image_pipeline.yaml to generate visual research summaries
- Integrate with data_processing.yaml to analyze research data

### Advanced Variations
- Scale to handle larger datasets and more complex processing
- Add real-time processing capabilities for streaming data
- Implement distributed processing across multiple systems
- Use multiple AI models for comparison and validation

## Hands-On Exercise

### Execution Instructions
- 1. Navigate to your orchestrator project directory
- 1.5. Ensure you have access to required services: Web search APIs
- 2. Run: python scripts/run_pipeline.py examples/research_minimal.yaml
- 3. Monitor the output for progress and any error messages
- 4. Check the output directory for generated results

### Expected Outputs
- Generated JSON data in the specified output directory
- Generated Markdown documents in the specified output directory
- Execution logs showing step-by-step progress
- Completion message with runtime statistics
- No error messages or warnings (successful execution)

### Troubleshooting
- **API Authentication Errors**: Ensure all required API keys are properly configured in your environment
- **Template Resolution Errors**: Check that all input parameters are provided and template syntax is correct
- **General Execution Errors**: Check the logs for specific error messages and verify your orchestrator installation

### Verification Steps
- Check that the pipeline completed without errors
- Verify all expected output files were created
- Review the output content for quality and accuracy
- Review generated content for relevance and quality

---

*Tutorial generated on 2025-08-27T23:40:24.396567*
