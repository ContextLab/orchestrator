"file","line_start","line_end","type","description","content","test_status","test_file","notes"
"CLAUDE.md","49","59","text","- Redis - For caching and session management","orchestrator/
├── src/orchestrator/          # Core library code
│   ├── compiler/             # YAML parsing and compilation
│   ├── models/               # Model abstractions and registry
│   ├── adapters/             # Control system adapters
│   ├── executor/             # Sandboxed execution
│   └── state/                # State management
├── tests/                    # Unit and integration tests
├── examples/                 # Example pipeline definitions
├── docs/                     # Documentation
└── config/                   # Configuration schemas","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"CLAUDE.md","67","73","yaml","Pipelines use <AUTO> tags for LLM-resolved ambiguities:","steps:
  - id: analyze_data
    action: analyze
    parameters:
      data: ""{{ input_data }}""
      method: <AUTO>Choose best analysis method for this data type</AUTO>
      depth: <AUTO>Determine analysis depth based on data complexity</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","32","32","bash","- 💾 Lazy Model Loading: Models are downloaded only when needed, saving disk space","pip install py-orc","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","37","40","bash","For additional features:","pip install py-orc[ollama]      # Ollama model support
pip install py-orc[cloud]        # Cloud model providers
pip install py-orc[dev]          # Development tools
pip install py-orc[all]          # Everything","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","48","66","yaml","1. Create a simple pipeline (hello_world.yaml):","id: hello_world
name: Hello World Pipeline
description: A simple example pipeline

steps:
  - id: greet
    action: generate_text
    parameters:
      prompt: ""Say hello to the world in a creative way!""

  - id: translate
    action: generate_text
    parameters:
      prompt: ""Translate this greeting to Spanish: {{ greet.result }}""
    dependencies: [greet]

outputs:
  greeting: ""{{ greet.result }}""
  spanish: ""{{ translate.result }}""","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","72","81","python","2. Run the pipeline:","import orchestrator as orc

# Initialize models (auto-detects available models)
orc.init_models()

# Compile and run the pipeline
pipeline = orc.compile(""hello_world.yaml"")
result = pipeline.run()

print(result)","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","89","95","yaml","Orchestrator's <AUTO> tags let AI decide configuration details:","steps:
  - id: analyze_data
    action: analyze
    parameters:
      data: ""{{ input_data }}""
      method: <AUTO>Choose the best analysis method for this data type</AUTO>
      visualization: <AUTO>Decide if we should create a chart</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","103","125","yaml","Configure available models in models.yaml:","models:
  # Local models (via Ollama) - downloaded on first use
  - source: ollama
    name: llama3.1:8b
    expertise: [general, reasoning, multilingual]
    size: 8b

  - source: ollama
    name: qwen2.5-coder:7b
    expertise: [code, programming]
    size: 7b

  # Cloud models
  - source: openai
    name: gpt-4o
    expertise: [general, reasoning, code, analysis, vision]
    size: 1760b  # Estimated

defaults:
  expertise_preferences:
    code: qwen2.5-coder:7b
    reasoning: deepseek-r1:8b
    fast: llama3.2:1b","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","135","192","yaml","Here's a more complex example showing model requirements and parallel execution:","id: research_pipeline
name: AI Research Pipeline
description: Research a topic and create a comprehensive report

inputs:
  - name: topic
    type: string
    description: Research topic

  - name: depth
    type: string
    default: <AUTO>Determine appropriate research depth</AUTO>

steps:
  # Parallel research from multiple sources
  - id: web_search
    action: search_web
    parameters:
      query: ""{{ topic }} latest research 2025""
      count: <AUTO>Decide how many results to fetch</AUTO>
    requires_model:
      expertise: [research, web]

  - id: academic_search
    action: search_academic
    parameters:
      query: ""{{ topic }}""
      filters: <AUTO>Set appropriate academic filters</AUTO>
    requires_model:
      expertise: [research, academic]

  # Analyze findings with specialized model
  - id: analyze_findings
    action: analyze
    parameters:
      web_results: ""{{ web_search.results }}""
      academic_results: ""{{ academic_search.results }}""
      analysis_focus: <AUTO>Determine key aspects to analyze</AUTO>
    dependencies: [web_search, academic_search]
    requires_model:
      expertise: [analysis, reasoning]
      min_size: 20b  # Require large model for complex analysis

  # Generate report
  - id: write_report
    action: generate_document
    parameters:
      topic: ""{{ topic }}""
      analysis: ""{{ analyze_findings.result }}""
      style: <AUTO>Choose appropriate writing style</AUTO>
      length: <AUTO>Determine optimal report length</AUTO>
    dependencies: [analyze_findings]
    requires_model:
      expertise: [writing, general]

outputs:
  report: ""{{ write_report.document }}""
  summary: ""{{ analyze_findings.summary }}""","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","200","274","yaml","Here's a fully functional pipeline that generates research reports:","# research_report.yaml
id: research_report
name: Research Report Generator
description: Generate comprehensive research reports with citations

inputs:
  - name: topic
    type: string
    description: Research topic
  - name: instructions
    type: string
    description: Additional instructions for the report

outputs:
  - pdf: <AUTO>Generate appropriate filename for the research report PDF</AUTO>

steps:
  - id: search
    name: Web Search
    action: search_web
    parameters:
      query: <AUTO>Create effective search query for {topic} with {instructions}</AUTO>
      max_results: 10
    requires_model:
      expertise: fast

  - id: compile_notes
    name: Compile Research Notes
    action: generate_text
    parameters:
      prompt: |
        Compile comprehensive research notes from these search results:
        {{ search.results }}

        Topic: {{ topic }}
        Instructions: {{ instructions }}

        Create detailed notes with:
        - Key findings
        - Important quotes
        - Source citations
        - Relevant statistics
    dependencies: [search]
    requires_model:
      expertise: [analysis, reasoning]
      min_size: 7b

  - id: write_report
    name: Write Report
    action: generate_document
    parameters:
      content: |
        Write a comprehensive research report on ""{{ topic }}""

        Research notes:
        {{ compile_notes.result }}

        Requirements:
        - Professional academic style
        - Include introduction, body sections, and conclusion
        - Cite sources properly
        - {{ instructions }}
      format: markdown
    dependencies: [compile_notes]
    requires_model:
      expertise: [writing, general]
      min_size: 20b

  - id: create_pdf
    name: Create PDF
    action: convert_to_pdf
    parameters:
      markdown: ""{{ write_report.document }}""
      filename: ""{{ outputs.pdf }}""
    dependencies: [write_report]","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","280","294","python","Run it with:","import orchestrator as orc

# Initialize models
orc.init_models()

# Compile pipeline
pipeline = orc.compile(""research_report.yaml"")

# Run with inputs
result = pipeline.run(
    topic=""quantum computing applications in medicine"",
    instructions=""Focus on recent breakthroughs and future potential""
)

print(f""Report saved to: {result}"")","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"README.md","353","359","bibtex","If you use Orchestrator in your research, please cite:","@software{orchestrator2025,
  title = {Orchestrator: AI Pipeline Orchestration Framework},
  author = {Manning, Jeremy R. and {Contextual Dynamics Lab}},
  year = {2025},
  url = {https://github.com/ContextLab/orchestrator},
  organization = {Dartmouth College}
}","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","12","47","text","The Orchestrator is a Python library that provides a unified interface for executing AI pipelines defined in YAML with automatic ambiguity resolution using LLMs. It transparently integrates multiple c","┌─────────────────────────────────────────────────────────────────┐
│                         User Interface                          │
│                    (YAML Pipeline Definition)                   │
└───────────────────────┬─────────────────────────────────────────┘
                        │
┌───────────────────────▼───────────────────────────────────────┐
│                    YAML Parser & Compiler                     │
│  ┌─────────────┐  ┌──────────────┐  ┌────────────────────┐    │
│  │ Schema      │  │ Ambiguity    │  │ Pipeline           │    │
│  │ Validator   │  │ Detector     │  │ Optimizer          │    │
│  └─────────────┘  └──────────────┘  └────────────────────┘    │
└───────────────────────┬───────────────────────────────────────┘
                        │
┌───────────────────────▼───────────────────────────────────────┐
│                  Orchestration Engine                         │
│  ┌─────────────┐  ┌──────────────┐  ┌────────────────────┐    │
│  │ Task        │  │ Dependency   │  │ Resource           │    │
│  │ Scheduler   │  │ Manager      │  │ Allocator          │    │
│  └─────────────┘  └──────────────┘  └────────────────────┘    │
└───────────────────────┬───────────────────────────────────────┘
                        │
┌───────────────────────▼───────────────────────────────────────┐
│                  Control System Adapters                      │
│  ┌─────────────┐  ┌──────────────┐  ┌───────────────────┐     │
│  │ LangGraph   │  │ MCP          │  │ Custom            │     │
│  │ Adapter     │  │ Adapter      │  │ Adapters          │     │
│  └─────────────┘  └──────────────┘  └───────────────────┘     │
└───────────────────────┬───────────────────────────────────────┘
                        │
┌───────────────────────▼───────────────────────────────────────┐
│                    Execution Layer                            │
│  ┌─────────────┐  ┌──────────────┐  ┌────────────────────┐    │
│  │ Sandboxed   │  │ Model        │  │ State              │    │
│  │ Executors   │  │ Registry     │  │ Persistence        │    │
│  └─────────────┘  └──────────────┘  └────────────────────┘    │
└───────────────────────────────────────────────────────────────┘","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","63","89","python","5. Security by Default: Sandboxed execution and input validation","from dataclasses import dataclass
from typing import Dict, Any, Optional, List
from enum import Enum

class TaskStatus(Enum):
    PENDING = ""pending""
    RUNNING = ""running""
    COMPLETED = ""completed""
    FAILED = ""failed""
    SKIPPED = ""skipped""

@dataclass
class Task:
    """"""Core task abstraction for the orchestrator""""""
    id: str
    name: str
    action: str
    parameters: Dict[str, Any]
    dependencies: List[str]
    status: TaskStatus = TaskStatus.PENDING
    result: Optional[Any] = None
    error: Optional[Exception] = None
    metadata: Dict[str, Any] = None

    def is_ready(self, completed_tasks: set) -> bool:
        """"""Check if all dependencies are satisfied""""""
        return all(dep in completed_tasks for dep in self.dependencies)","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","95","135","python","`","@dataclass
class Pipeline:
    """"""Pipeline represents a collection of tasks with dependencies""""""
    id: str
    name: str
    tasks: Dict[str, Task]
    context: Dict[str, Any]
    metadata: Dict[str, Any]

    def get_execution_order(self) -> List[List[str]]:
        """"""Returns tasks grouped by execution level (parallel groups)""""""
        from collections import defaultdict, deque

        # Build dependency graph
        in_degree = {task_id: len(task.dependencies) for task_id, task in self.tasks.items()}
        graph = defaultdict(list)

        for task_id, task in self.tasks.items():
            for dep in task.dependencies:
                graph[dep].append(task_id)

        # Topological sort with level grouping
        levels = []
        queue = deque([task_id for task_id, degree in in_degree.items() if degree == 0])

        while queue:
            current_level = []
            level_size = len(queue)

            for _ in range(level_size):
                task_id = queue.popleft()
                current_level.append(task_id)

                for neighbor in graph[task_id]:
                    in_degree[neighbor] -= 1
                    if in_degree[neighbor] == 0:
                        queue.append(neighbor)

            levels.append(current_level)

        return levels","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","141","171","python","`","@dataclass
class ModelCapabilities:
    """"""Defines what a model can do""""""
    supported_tasks: List[str]
    context_window: int
    supports_function_calling: bool
    supports_structured_output: bool
    supports_streaming: bool
    languages: List[str]

@dataclass
class ModelRequirements:
    """"""Resource requirements for a model""""""
    memory_gb: float
    gpu_memory_gb: Optional[float]
    cpu_cores: int
    supports_quantization: List[str]  # [""int8"", ""int4"", ""gptq"", ""awq""]

class Model:
    """"""Abstract base class for all models""""""
    def __init__(self, name: str, provider: str):
        self.name = name
        self.provider = provider
        self.capabilities = self._load_capabilities()
        self.requirements = self._load_requirements()

    async def generate(self, prompt: str, **kwargs) -> str:
        raise NotImplementedError

    async def generate_structured(self, prompt: str, schema: dict, **kwargs) -> dict:
        raise NotImplementedError","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","177","200","python","`","from abc import ABC, abstractmethod

class ControlSystem(ABC):
    """"""Abstract base class for control system adapters""""""

    @abstractmethod
    async def execute_task(self, task: Task, context: Dict[str, Any]) -> Any:
        """"""Execute a single task""""""
        pass

    @abstractmethod
    async def execute_pipeline(self, pipeline: Pipeline) -> Dict[str, Any]:
        """"""Execute an entire pipeline""""""
        pass

    @abstractmethod
    def get_capabilities(self) -> Dict[str, Any]:
        """"""Return system capabilities""""""
        pass

    @abstractmethod
    async def health_check(self) -> bool:
        """"""Check if the system is healthy""""""
        pass","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","208","271","python","`","import yaml
from typing import Dict, Any, List
import jsonschema
from jinja2 import Environment, StrictUndefined

class YAMLCompiler:
    """"""Compiles YAML definitions into executable pipelines""""""

    def __init__(self):
        self.schema_validator = SchemaValidator()
        self.ambiguity_resolver = AmbiguityResolver()
        self.template_engine = Environment(undefined=StrictUndefined)

    def compile(self, yaml_content: str, context: Dict[str, Any] = None) -> Pipeline:
        """"""Compile YAML to Pipeline object""""""
        # Step 1: Parse YAML safely
        raw_pipeline = yaml.safe_load(yaml_content)

        # Step 2: Validate against schema
        self.schema_validator.validate(raw_pipeline)

        # Step 3: Process templates
        processed = self._process_templates(raw_pipeline, context or {})

        # Step 4: Detect and resolve ambiguities
        resolved = self._resolve_ambiguities(processed)

        # Step 5: Build pipeline object
        return self._build_pipeline(resolved)

    def _process_templates(self, pipeline_def: dict, context: dict) -> dict:
        """"""Process Jinja2 templates in the pipeline definition""""""
        def process_value(value):
            if isinstance(value, str):
                template = self.template_engine.from_string(value)
                return template.render(**context)
            elif isinstance(value, dict):
                return {k: process_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [process_value(item) for item in value]
            return value

        return process_value(pipeline_def)

    def _resolve_ambiguities(self, pipeline_def: dict) -> dict:
        """"""Detect and resolve <AUTO> tags""""""
        def process_auto_tags(obj, path=""""):
            if isinstance(obj, dict):
                result = {}
                for key, value in obj.items():
                    if isinstance(value, str) and value.startswith(""<AUTO>"") and value.endswith(""</AUTO>""):
                        # Extract ambiguous content
                        content = value[6:-7]  # Remove <AUTO> tags
                        # Resolve ambiguity
                        resolved = self.ambiguity_resolver.resolve(content, path + ""."" + key)
                        result[key] = resolved
                    else:
                        result[key] = process_auto_tags(value, path + ""."" + key)
                return result
            elif isinstance(obj, list):
                return [process_auto_tags(item, f""{path}[{i}]"") for i, item in enumerate(obj)]
            return obj

        return process_auto_tags(pipeline_def)","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","277","338","python","`","class AmbiguityResolver:
    """"""Resolves ambiguous specifications using LLMs""""""

    def __init__(self):
        self.model_selector = ModelSelector()
        self.format_cache = FormatCache()
        self.resolution_strategies = {
            ""task_type"": self._resolve_task_type,
            ""model_selection"": self._resolve_model_selection,
            ""parameter_inference"": self._resolve_parameters,
            ""dependency_detection"": self._resolve_dependencies
        }

    async def resolve(self, ambiguous_content: str, context_path: str) -> Any:
        """"""Main resolution method""""""
        # Step 1: Classify ambiguity type
        ambiguity_type = await self._classify_ambiguity(ambiguous_content, context_path)

        # Step 2: Check cache
        cache_key = self._generate_cache_key(ambiguous_content, ambiguity_type)
        if cached := self.format_cache.get(cache_key):
            return cached

        # Step 3: Select appropriate model
        model = await self.model_selector.select_for_task(""ambiguity_resolution"")

        # Step 4: Generate format specification (two-step approach)
        format_spec = await self._generate_format_spec(model, ambiguous_content, ambiguity_type)

        # Step 5: Execute resolution with format spec
        resolution_strategy = self.resolution_strategies[ambiguity_type]
        result = await resolution_strategy(model, ambiguous_content, format_spec)

        # Step 6: Cache result
        self.format_cache.set(cache_key, result)

        return result

    async def _generate_format_spec(self, model, content: str, ambiguity_type: str) -> dict:
        """"""Generate output format specification""""""
        prompt = f""""""
        Analyze this ambiguous specification and generate a JSON schema for the expected output:

        Ambiguity Type: {ambiguity_type}
        Content: {content}

        Return a JSON schema that describes the expected structure of the resolved output.
        """"""

        schema = await model.generate_structured(
            prompt,
            schema={
                ""type"": ""object"",
                ""properties"": {
                    ""schema"": {""type"": ""object""},
                    ""description"": {""type"": ""string""},
                    ""examples"": {""type"": ""array""}
                }
            }
        )

        return schema","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","344","435","python","`","import pickle
import json
from datetime import datetime
from typing import Optional
import asyncio
from contextlib import asynccontextmanager

class StateManager:
    """"""Manages pipeline state and checkpointing""""""

    def __init__(self, backend: str = ""postgres""):
        self.backend = self._init_backend(backend)
        self.checkpoint_strategy = AdaptiveCheckpointStrategy()

    async def save_checkpoint(self, pipeline_id: str, state: dict, metadata: dict = None):
        """"""Save pipeline state checkpoint""""""
        checkpoint = {
            ""pipeline_id"": pipeline_id,
            ""state"": state,
            ""metadata"": metadata or {},
            ""timestamp"": datetime.utcnow().isoformat(),
            ""version"": ""1.0""
        }

        # Compress state if large
        if self._should_compress(state):
            checkpoint[""state""] = self._compress_state(state)
            checkpoint[""compressed""] = True

        await self.backend.save(checkpoint)

    async def restore_checkpoint(self, pipeline_id: str,
                                timestamp: Optional[datetime] = None) -> Optional[dict]:
        """"""Restore pipeline state from checkpoint""""""
        checkpoint = await self.backend.load(pipeline_id, timestamp)

        if not checkpoint:
            return None

        # Decompress if needed
        if checkpoint.get(""compressed""):
            checkpoint[""state""] = self._decompress_state(checkpoint[""state""])

        return checkpoint

    @asynccontextmanager
    async def checkpoint_context(self, pipeline_id: str, task_id: str):
        """"""Context manager for automatic checkpointing""""""
        start_time = datetime.utcnow()

        try:
            yield
            # Save checkpoint on success
            if self.checkpoint_strategy.should_checkpoint(pipeline_id, task_id):
                await self.save_checkpoint(
                    pipeline_id,
                    {""last_completed_task"": task_id},
                    {""execution_time"": (datetime.utcnow() - start_time).total_seconds()}
                )
        except Exception as e:
            # Save error state
            await self.save_checkpoint(
                pipeline_id,
                {""last_failed_task"": task_id, ""error"": str(e)},
                {""failure_time"": datetime.utcnow().isoformat()}
            )
            raise

class AdaptiveCheckpointStrategy:
    """"""Determines when to create checkpoints based on various factors""""""

    def __init__(self):
        self.task_history = {}
        self.checkpoint_interval = 5  # Base interval

    def should_checkpoint(self, pipeline_id: str, task_id: str) -> bool:
        """"""Decide if checkpoint is needed""""""
        # Always checkpoint after critical tasks
        if self._is_critical_task(task_id):
            return True

        # Adaptive checkpointing based on task execution time
        if pipeline_id not in self.task_history:
            self.task_history[pipeline_id] = []

        self.task_history[pipeline_id].append(task_id)

        # Checkpoint every N tasks
        if len(self.task_history[pipeline_id]) % self.checkpoint_interval == 0:
            return True

        return False","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","441","553","python","`","import numpy as np
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class ModelMetrics:
    """"""Performance metrics for a model""""""
    latency_p50: float
    latency_p95: float
    throughput: float
    accuracy: float
    cost_per_token: float

class ModelRegistry:
    """"""Central registry for all available models""""""

    def __init__(self):
        self.models: Dict[str, Model] = {}
        self.metrics: Dict[str, ModelMetrics] = {}
        self.bandit = UCBModelSelector()

    def register_model(self, model: Model, metrics: ModelMetrics):
        """"""Register a new model""""""
        self.models[model.name] = model
        self.metrics[model.name] = metrics

    async def select_model(self, requirements: Dict[str, Any]) -> Model:
        """"""Select best model for given requirements""""""
        # Step 1: Filter by capabilities
        eligible_models = self._filter_by_capabilities(requirements)

        # Step 2: Filter by available resources
        available_models = await self._filter_by_resources(eligible_models)

        # Step 3: Use multi-armed bandit for selection
        selected_model_name = self.bandit.select(
            [m.name for m in available_models],
            requirements
        )

        return self.models[selected_model_name]

    def _filter_by_capabilities(self, requirements: Dict[str, Any]) -> List[Model]:
        """"""Filter models by required capabilities""""""
        eligible = []

        for model in self.models.values():
            if self._meets_requirements(model, requirements):
                eligible.append(model)

        return eligible

    async def _filter_by_resources(self, models: List[Model]) -> List[Model]:
        """"""Filter models by available system resources""""""
        system_resources = await self._get_system_resources()
        available = []

        for model in models:
            if self._can_run_on_system(model, system_resources):
                available.append(model)

        # If no models fit, try quantized versions
        if not available:
            available = await self._find_quantized_alternatives(models, system_resources)

        return available

class UCBModelSelector:
    """"""Upper Confidence Bound algorithm for model selection""""""

    def __init__(self, exploration_factor: float = 2.0):
        self.exploration_factor = exploration_factor
        self.model_stats = {}  # Track performance per model

    def select(self, model_names: List[str], context: Dict[str, Any]) -> str:
        """"""Select model using UCB algorithm""""""
        if not model_names:
            raise ValueError(""No models available"")

        # Initialize stats for new models
        for name in model_names:
            if name not in self.model_stats:
                self.model_stats[name] = {
                    ""successes"": 0,
                    ""attempts"": 0,
                    ""total_reward"": 0.0
                }

        # Calculate UCB scores
        scores = {}
        total_attempts = sum(stats[""attempts""] for stats in self.model_stats.values())

        for name in model_names:
            stats = self.model_stats[name]
            if stats[""attempts""] == 0:
                scores[name] = float('inf')  # Explore untried models
            else:
                avg_reward = stats[""total_reward""] / stats[""attempts""]
                exploration_bonus = self.exploration_factor * np.sqrt(
                    np.log(total_attempts + 1) / stats[""attempts""]
                )
                scores[name] = avg_reward + exploration_bonus

        # Select model with highest score
        return max(scores, key=scores.get)

    def update_reward(self, model_name: str, reward: float):
        """"""Update model statistics after execution""""""
        if model_name in self.model_stats:
            self.model_stats[model_name][""attempts""] += 1
            self.model_stats[model_name][""total_reward""] += reward
            if reward > 0:
                self.model_stats[model_name][""successes""] += 1","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","559","677","python","`","from enum import Enum
from typing import Optional, Callable
import asyncio

class ErrorSeverity(Enum):
    CRITICAL = ""critical""
    HIGH = ""high""
    MEDIUM = ""medium""
    LOW = ""low""

class ErrorCategory(Enum):
    RATE_LIMIT = ""rate_limit""
    TIMEOUT = ""timeout""
    RESOURCE_EXHAUSTION = ""resource_exhaustion""
    VALIDATION_ERROR = ""validation_error""
    SYSTEM_ERROR = ""system_error""
    UNKNOWN = ""unknown""

class ErrorHandler:
    """"""Comprehensive error handling system""""""

    def __init__(self):
        self.error_strategies = {
            ErrorCategory.RATE_LIMIT: self._handle_rate_limit,
            ErrorCategory.TIMEOUT: self._handle_timeout,
            ErrorCategory.RESOURCE_EXHAUSTION: self._handle_resource_exhaustion,
            ErrorCategory.VALIDATION_ERROR: self._handle_validation_error,
            ErrorCategory.SYSTEM_ERROR: self._handle_system_error
        }
        self.circuit_breaker = CircuitBreaker()

    async def handle_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:
        """"""Main error handling method""""""
        # Classify error
        category = self._classify_error(error)
        severity = self._determine_severity(error, context)

        # Log error with full context
        await self._log_error(error, category, severity, context)

        # Apply circuit breaker
        if self.circuit_breaker.is_open(context.get(""system_id"")):
            raise SystemUnavailableError(""System circuit breaker is open"")

        # Execute error handling strategy
        strategy = self.error_strategies.get(category, self._handle_unknown)
        result = await strategy(error, context, severity)

        # Update circuit breaker
        if severity == ErrorSeverity.CRITICAL:
            self.circuit_breaker.record_failure(context.get(""system_id""))

        return result

    async def _handle_rate_limit(self, error: Exception, context: Dict[str, Any],
                                severity: ErrorSeverity) -> Dict[str, Any]:
        """"""Handle rate limit errors""""""
        retry_after = self._extract_retry_after(error) or 60

        if severity == ErrorSeverity.LOW:
            # Wait and retry
            await asyncio.sleep(retry_after)
            return {""action"": ""retry"", ""delay"": retry_after}
        else:
            # Switch to alternative system
            return {
                ""action"": ""switch_system"",
                ""reason"": ""rate_limit_exceeded"",
                ""retry_after"": retry_after
            }

    async def _handle_timeout(self, error: Exception, context: Dict[str, Any],
                             severity: ErrorSeverity) -> Dict[str, Any]:
        """"""Handle timeout errors""""""
        if context.get(""retry_count"", 0) < 3:
            # Retry with increased timeout
            new_timeout = context.get(""timeout"", 30) * 2
            return {
                ""action"": ""retry"",
                ""timeout"": new_timeout,
                ""retry_count"": context.get(""retry_count"", 0) + 1
            }
        else:
            # Mark task as failed
            return {""action"": ""fail"", ""reason"": ""timeout_exceeded""}

class CircuitBreaker:
    """"""Circuit breaker pattern implementation""""""

    def __init__(self, failure_threshold: int = 5, timeout: float = 60.0):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = {}
        self.last_failure_time = {}

    def is_open(self, system_id: str) -> bool:
        """"""Check if circuit breaker is open for a system""""""
        if system_id not in self.failures:
            return False

        # Check if timeout has passed
        if system_id in self.last_failure_time:
            time_since_failure = time.time() - self.last_failure_time[system_id]
            if time_since_failure > self.timeout:
                # Reset circuit breaker
                self.failures[system_id] = 0
                return False

        return self.failures[system_id] >= self.failure_threshold

    def record_failure(self, system_id: str):
        """"""Record a failure for a system""""""
        self.failures[system_id] = self.failures.get(system_id, 0) + 1
        self.last_failure_time[system_id] = time.time()

    def record_success(self, system_id: str):
        """"""Record a success for a system""""""
        if system_id in self.failures:
            self.failures[system_id] = max(0, self.failures[system_id] - 1)","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","685","740","yaml","`","# schema/pipeline-schema.yaml
$schema: ""http://json-schema.org/draft-07/schema#""
type: object
required:
  - name
  - version
  - steps
properties:
  name:
    type: string
    pattern: ""^[a-zA-Z][a-zA-Z0-9_-]*$""
  version:
    type: string
    pattern: ""^\\d+\\.\\d+\\.\\d+$""
  description:
    type: string
  metadata:
    type: object
  context:
    type: object
    properties:
      timeout:
        type: integer
        minimum: 1
      max_retries:
        type: integer
        minimum: 0
      checkpoint_strategy:
        type: string
        enum: [""adaptive"", ""fixed"", ""none""]
  steps:
    type: array
    minItems: 1
    items:
      type: object
      required:
        - id
        - action
      properties:
        id:
          type: string
          pattern: ""^[a-zA-Z][a-zA-Z0-9_-]*$""
        action:
          type: string
        parameters:
          type: object
        dependencies:
          type: array
          items:
            type: string
        on_failure:
          type: string
          enum: [""continue"", ""fail"", ""retry"", ""skip""]
        timeout:
          type: integer
          minimum: 1","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","746","794","yaml","`","# example-pipeline.yaml
name: research_report_pipeline
version: 1.0.0
description: Generate a comprehensive research report on a given topic

context:
  timeout: 3600
  max_retries: 3
  checkpoint_strategy: adaptive

steps:
  - id: topic_analysis
    action: analyze
    parameters:
      input: ""{{ topic }}""
      analysis_type: <AUTO>Determine the best analysis approach for this topic</AUTO>
      output_format: <AUTO>Choose appropriate format: bullet_points, narrative, or structured</AUTO>

  - id: research_planning
    action: plan
    parameters:
      topic_analysis: ""{{ steps.topic_analysis.output }}""
      research_depth: <AUTO>Based on topic complexity, choose: shallow, medium, or deep</AUTO>
      sources: <AUTO>Determine number and types of sources needed</AUTO>
    dependencies: [topic_analysis]

  - id: web_search
    action: search
    parameters:
      queries: <AUTO>Generate search queries based on research plan</AUTO>
      num_results: <AUTO>Determine optimal number of results per query</AUTO>
    dependencies: [research_planning]

  - id: content_synthesis
    action: synthesize
    parameters:
      sources: ""{{ steps.web_search.results }}""
      style: <AUTO>Choose writing style: academic, business, or general</AUTO>
      length: <AUTO>Determine appropriate length based on topic</AUTO>
    dependencies: [web_search]

  - id: report_generation
    action: generate_report
    parameters:
      content: ""{{ steps.content_synthesis.output }}""
      format: markdown
      sections: <AUTO>Organize content into appropriate sections</AUTO>
    dependencies: [content_synthesis]
    on_failure: retry","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","802","836","python","`","# main.py
import asyncio
from orchestrator import Orchestrator, YAMLCompiler
from orchestrator.models import ModelRegistry
from orchestrator.storage import PostgresBackend

async def main():
    # Initialize orchestrator
    orchestrator = Orchestrator(
        storage_backend=PostgresBackend(""postgresql://localhost/orchestrator""),
        model_registry=ModelRegistry.from_config(""models.yaml"")
    )

    # Load and compile pipeline
    with open(""research_pipeline.yaml"") as f:
        yaml_content = f.read()

    compiler = YAMLCompiler()
    pipeline = await compiler.compile(
        yaml_content,
        context={""topic"": ""quantum computing applications in cryptography""}
    )

    # Execute pipeline
    try:
        result = await orchestrator.execute_pipeline(pipeline)
        print(f""Pipeline completed successfully: {result}"")
    except Exception as e:
        print(f""Pipeline failed: {e}"")
        # Attempt recovery
        recovered_result = await orchestrator.recover_pipeline(pipeline.id)
        print(f""Recovery result: {recovered_result}"")

if __name__ == ""__main__"":
    asyncio.run(main())","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","842","903","python","`","# adapters/custom_adapter.py
from orchestrator.adapters import ControlSystem
from orchestrator.models import Task

class CustomMLAdapter(ControlSystem):
    """"""Example adapter for a custom ML framework""""""

    def __init__(self, config: dict):
        self.config = config
        self.ml_engine = self._init_ml_engine()

    async def execute_task(self, task: Task, context: dict) -> Any:
        """"""Execute task using custom ML framework""""""
        # Map task action to ML operation
        operation = self._map_action_to_operation(task.action)

        # Prepare inputs
        inputs = self._prepare_inputs(task.parameters, context)

        # Execute with monitoring
        async with self._monitor_execution(task.id):
            result = await self.ml_engine.run(operation, inputs)

        # Post-process results
        return self._process_results(result, task.metadata)

    async def execute_pipeline(self, pipeline: Pipeline) -> dict:
        """"""Execute entire pipeline with optimizations""""""
        # Build execution graph
        exec_graph = self._build_execution_graph(pipeline)

        # Optimize graph (fusion, parallelization)
        optimized_graph = self._optimize_graph(exec_graph)

        # Execute with checkpointing
        results = {}
        for level in optimized_graph.levels:
            # Execute tasks in parallel
            level_results = await asyncio.gather(*[
                self.execute_task(task, pipeline.context)
                for task in level
            ])

            # Store results
            for task, result in zip(level, level_results):
                results[task.id] = result

        return results

    def get_capabilities(self) -> dict:
        return {
            ""supported_operations"": [""train"", ""predict"", ""evaluate"", ""optimize""],
            ""parallel_execution"": True,
            ""gpu_acceleration"": True,
            ""distributed"": self.config.get(""distributed"", False)
        }

    async def health_check(self) -> bool:
        try:
            return await self.ml_engine.ping()
        except Exception:
            return False","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","909","1012","python","`","# sandbox/executor.py
import docker
import asyncio
from typing import Dict, Any, Optional

class SandboxedExecutor:
    """"""Secure sandboxed code execution""""""

    def __init__(self, docker_client=None):
        self.docker = docker_client or docker.from_env()
        self.containers = {}
        self.resource_limits = {
            ""memory"": ""1g"",
            ""cpu_quota"": 50000,
            ""pids_limit"": 100
        }

    async def execute_code(
        self,
        code: str,
        language: str,
        environment: Dict[str, str],
        timeout: int = 30
    ) -> Dict[str, Any]:
        """"""Execute code in sandboxed environment""""""
        # Create container
        container = await self._create_container(language, environment)

        try:
            # Start container
            await self._start_container(container)

            # Execute code with timeout
            result = await asyncio.wait_for(
                self._run_code(container, code),
                timeout=timeout
            )

            return {
                ""success"": True,
                ""output"": result[""output""],
                ""errors"": result.get(""errors"", """"),
                ""execution_time"": result[""execution_time""]
            }

        except asyncio.TimeoutError:
            return {
                ""success"": False,
                ""error"": ""Execution timeout exceeded"",
                ""timeout"": timeout
            }
        finally:
            # Cleanup
            await self._cleanup_container(container)

    async def _create_container(
        self,
        language: str,
        environment: Dict[str, str]
    ) -> docker.models.containers.Container:
        """"""Create isolated container for code execution""""""
        image = self._get_image_for_language(language)

        container = self.docker.containers.create(
            image=image,
            command=""sleep infinity"",  # Keep container running
            detach=True,
            mem_limit=self.resource_limits[""memory""],
            cpu_quota=self.resource_limits[""cpu_quota""],
            pids_limit=self.resource_limits[""pids_limit""],
            network_mode=""none"",  # No network access
            read_only=True,
            tmpfs={""/tmp"": ""rw,noexec,nosuid,size=100m""},
            environment=environment,
            security_opt=[""no-new-privileges:true""],
            user=""nobody""  # Run as non-root
        )

        self.containers[container.id] = container
        return container

    async def _run_code(
        self,
        container: docker.models.containers.Container,
        code: str
    ) -> Dict[str, Any]:
        """"""Execute code inside container""""""
        # Write code to container
        code_file = ""/tmp/code.py""
        container.exec_run(f""sh -c 'cat > {code_file}'"", stdin=True).input = code.encode()

        # Execute code
        start_time = asyncio.get_event_loop().time()
        result = container.exec_run(f""python {code_file}"", demux=True)
        execution_time = asyncio.get_event_loop().time() - start_time

        stdout, stderr = result.output

        return {
            ""output"": stdout.decode() if stdout else """",
            ""errors"": stderr.decode() if stderr else """",
            ""exit_code"": result.exit_code,
            ""execution_time"": execution_time
        }","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","1020","1062","python","`","# tests/test_ambiguity_resolver.py
import pytest
from orchestrator.compiler import AmbiguityResolver

@pytest.mark.asyncio
async def test_ambiguity_resolution():
    resolver = AmbiguityResolver()

    # Test simple ambiguity
    result = await resolver.resolve(
        ""Choose the best format for displaying data"",
        ""steps.display.format""
    )

    assert result in [""table"", ""chart"", ""list"", ""json""]

    # Test complex ambiguity with context
    result = await resolver.resolve(
        ""Determine optimal batch size based on available memory"",
        ""steps.processing.batch_size""
    )

    assert isinstance(result, int)
    assert 1 <= result <= 1000

@pytest.mark.asyncio
async def test_nested_ambiguity_resolution():
    resolver = AmbiguityResolver()

    nested_content = {
        ""query"": ""<AUTO>Generate search query for topic</AUTO>"",
        ""filters"": {
            ""date_range"": ""<AUTO>Choose appropriate date range</AUTO>"",
            ""sources"": ""<AUTO>Select relevant sources</AUTO>""
        }
    }

    result = await resolver.resolve_nested(nested_content, ""steps.search"")

    assert ""query"" in result
    assert ""filters"" in result
    assert ""date_range"" in result[""filters""]
    assert ""sources"" in result[""filters""]","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","1068","1116","python","`","# tests/test_pipeline_execution.py
import pytest
from orchestrator import Orchestrator

@pytest.mark.integration
@pytest.mark.asyncio
async def test_full_pipeline_execution():
    orchestrator = Orchestrator()

    pipeline_yaml = """"""
    name: test_pipeline
    version: 1.0.0
    steps:
      - id: step1
        action: generate
        parameters:
          prompt: ""Hello, world!""
      - id: step2
        action: transform
        parameters:
          input: ""{{ steps.step1.output }}""
          transformation: uppercase
        dependencies: [step1]
    """"""

    result = await orchestrator.execute_yaml(pipeline_yaml)

    assert result[""status""] == ""completed""
    assert ""HELLO, WORLD!"" in result[""outputs""][""step2""]

@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_recovery():
    orchestrator = Orchestrator()

    # Simulate failure and recovery
    pipeline_id = ""test_recovery_pipeline""

    # Create checkpoint
    await orchestrator.state_manager.save_checkpoint(
        pipeline_id,
        {""completed_steps"": [""step1"", ""step2""], ""failed_step"": ""step3""}
    )

    # Attempt recovery
    result = await orchestrator.recover_pipeline(pipeline_id)

    assert result[""recovered""] == True
    assert result[""resumed_from""] == ""step3""","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","1124","1152","python","`","class NestedAmbiguityHandler:
    """"""Handles complex nested ambiguities""""""

    async def resolve_nested(self, obj: Any, path: str = """") -> Any:
        """"""Recursively resolve nested ambiguities""""""
        if isinstance(obj, dict):
            # Check for circular dependencies
            self._check_circular_deps(obj, path)

            resolved = {}
            for key, value in obj.items():
                new_path = f""{path}.{key}"" if path else key

                if self._is_ambiguous(value):
                    # Resolve with parent context
                    context = self._build_context(obj, key, path)
                    resolved[key] = await self._resolve_with_context(value, context)
                else:
                    resolved[key] = await self.resolve_nested(value, new_path)

            return resolved

        elif isinstance(obj, list):
            return [
                await self.resolve_nested(item, f""{path}[{i}]"")
                for i, item in enumerate(obj)
            ]

        return obj","test_created","tests/snippet_tests/test_snippets_batch_1.py",""
"design.md","1158","1196","python","`","class DynamicModelSwitcher:
    """"""Handles model switching during pipeline execution""""""

    async def switch_model(
        self,
        current_model: str,
        reason: str,
        context: Dict[str, Any]
    ) -> str:
        """"""Switch to alternative model mid-execution""""""
        # Save current state
        checkpoint = await self._create_switching_checkpoint(
            current_model, reason, context
        )

        # Find alternative model
        alternatives = await self._find_alternatives(current_model, reason)

        if not alternatives:
            raise NoAlternativeModelError(
                f""No alternative found for {current_model}""
            )

        # Select best alternative
        new_model = await self._select_alternative(
            alternatives,
            context,
            checkpoint
        )

        # Migrate state if needed
        if self._needs_state_migration(current_model, new_model):
            context = await self._migrate_state(
                context,
                current_model,
                new_model
            )

        return new_model","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1202","1232","python","`","class DependencyValidator:
    """"""Validates and resolves pipeline dependencies""""""

    def detect_cycles(self, tasks: Dict[str, Task]) -> List[List[str]]:
        """"""Detect circular dependencies using DFS""""""
        WHITE, GRAY, BLACK = 0, 1, 2
        color = {task_id: WHITE for task_id in tasks}
        cycles = []

        def dfs(task_id: str, path: List[str]):
            color[task_id] = GRAY
            path.append(task_id)

            for dep in tasks[task_id].dependencies:
                if dep not in tasks:
                    raise InvalidDependencyError(f""Unknown dependency: {dep}"")

                if color[dep] == GRAY:
                    # Found cycle
                    cycle_start = path.index(dep)
                    cycles.append(path[cycle_start:])
                elif color[dep] == WHITE:
                    dfs(dep, path[:])

            color[task_id] = BLACK

        for task_id in tasks:
            if color[task_id] == WHITE:
                dfs(task_id, [])

        return cycles","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1240","1271","python","`","class MultiLevelCache:
    """"""Multi-level caching system for performance""""""

    def __init__(self):
        self.memory_cache = LRUCache(maxsize=1000)
        self.redis_cache = RedisCache()
        self.disk_cache = DiskCache(""/var/cache/orchestrator"")

    async def get(self, key: str) -> Optional[Any]:
        """"""Get from cache with fallback hierarchy""""""
        # L1: Memory cache
        if value := self.memory_cache.get(key):
            return value

        # L2: Redis cache
        if value := await self.redis_cache.get(key):
            self.memory_cache.set(key, value)
            return value

        # L3: Disk cache
        if value := await self.disk_cache.get(key):
            await self.redis_cache.set(key, value, ttl=3600)
            self.memory_cache.set(key, value)
            return value

        return None

    async def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """"""Set in all cache levels""""""
        self.memory_cache.set(key, value)
        await self.redis_cache.set(key, value, ttl=ttl or 3600)
        await self.disk_cache.set(key, value)","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1277","1320","python","`","class ParallelExecutor:
    """"""Optimized parallel task execution""""""

    def __init__(self, max_workers: int = 10):
        self.semaphore = asyncio.Semaphore(max_workers)
        self.task_queue = asyncio.Queue()

    async def execute_level(
        self,
        tasks: List[Task],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """"""Execute tasks in parallel with resource management""""""
        # Group tasks by resource requirements
        task_groups = self._group_by_resources(tasks)

        results = {}
        for group in task_groups:
            # Execute group with resource limits
            group_results = await self._execute_group(group, context)
            results.update(group_results)

        return results

    async def _execute_group(
        self,
        tasks: List[Task],
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """"""Execute a group of similar tasks""""""
        async def execute_with_semaphore(task):
            async with self.semaphore:
                return await self._execute_single(task, context)

        # Execute all tasks in parallel
        results = await asyncio.gather(*[
            execute_with_semaphore(task) for task in tasks
        ], return_exceptions=True)

        # Process results
        return {
            task.id: self._process_result(result)
            for task, result in zip(tasks, results)
        }","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1328","1444","yaml","`","# models.yaml
models:
  # Large models
  gpt-4o:
    provider: openai
    capabilities:
      tasks: [reasoning, code_generation, analysis, creative_writing]
      context_window: 128000
      supports_function_calling: true
      supports_structured_output: true
      languages: [en, es, fr, de, zh, ja, ko]
    requirements:
      memory_gb: 16
      gpu_memory_gb: 24
      cpu_cores: 8
    metrics:
      latency_p50: 2.1
      latency_p95: 4.5
      throughput: 10
      accuracy: 0.95
      cost_per_token: 0.00003

  claude-4-opus:
    provider: anthropic
    capabilities:
      tasks: [reasoning, analysis, creative_writing, code_review]
      context_window: 200000
      supports_function_calling: false
      supports_structured_output: true
      languages: [en, es, fr, de, zh, ja]
    requirements:
      memory_gb: 16
      gpu_memory_gb: 32
      cpu_cores: 8
    metrics:
      latency_p50: 2.5
      latency_p95: 5.0
      throughput: 8
      accuracy: 0.94
      cost_per_token: 0.00003

  # Medium models
  gpt-4o-mini:
    provider: openai
    capabilities:
      tasks: [general, code_generation, summarization]
      context_window: 16384
      supports_function_calling: true
      supports_structured_output: true
      languages: [en, es, fr, de]
    requirements:
      memory_gb: 8
      gpu_memory_gb: 8
      cpu_cores: 4
    metrics:
      latency_p50: 0.8
      latency_p95: 1.5
      throughput: 50
      accuracy: 0.85
      cost_per_token: 0.000002

  # Small models (local)
  llama2-7b:
    provider: local
    capabilities:
      tasks: [general, summarization]
      context_window: 4096
      supports_function_calling: false
      supports_structured_output: false
      languages: [en]
    requirements:
      memory_gb: 16
      gpu_memory_gb: 8
      cpu_cores: 4
      supports_quantization: [int8, int4, gptq]
    metrics:
      latency_p50: 0.5
      latency_p95: 1.0
      throughput: 20
      accuracy: 0.75
      cost_per_token: 0

  # Quantized versions
  llama2-7b-int4:
    provider: local
    base_model: llama2-7b
    quantization: int4
    requirements:
      memory_gb: 8
      gpu_memory_gb: 4
      cpu_cores: 4
    metrics:
      latency_p50: 0.6
      latency_p95: 1.2
      throughput: 15
      accuracy: 0.72
      cost_per_token: 0

# Model selection policies
selection_policies:
  default:
    strategy: ucb  # upper confidence bound
    exploration_factor: 2.0

  cost_optimized:
    strategy: weighted
    weights:
      cost: 0.6
      accuracy: 0.3
      latency: 0.1

  performance_optimized:
    strategy: weighted
    weights:
      latency: 0.5
      accuracy: 0.4
      cost: 0.1","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1450","1527","yaml","`","# config.yaml
orchestrator:
  # Core settings
  version: 1.0.0
  environment: production

  # Storage backend
  storage:
    backend: postgres
    connection_string: ${DATABASE_URL}
    pool_size: 20
    checkpoint_compression: true
    retention_days: 30

  # Execution settings
  execution:
    max_concurrent_pipelines: 10
    max_concurrent_tasks: 50
    default_timeout: 300
    max_retries: 3
    retry_backoff_factor: 2.0

  # Model settings
  models:
    registry_path: models.yaml
    selection_policy: default
    fallback_enabled: true
    local_models_path: /opt/models

  # Sandbox settings
  sandbox:
    enabled: true
    docker_socket: /var/run/docker.sock
    images:
      python: orchestrator/python:3.11-slim
      nodejs: orchestrator/node:18-slim
      custom: ${CUSTOM_SANDBOX_IMAGE}
    resource_limits:
      memory: 1GB
      cpu: 0.5
      disk: 100MB
      network: none

  # Security settings
  security:
    api_key_required: true
    rate_limiting:
      enabled: true
      requests_per_minute: 60
      burst_size: 10
    allowed_actions:
      - generate
      - transform
      - analyze
      - search
      - execute
    forbidden_modules:
      - os
      - subprocess
      - eval
      - exec

  # Monitoring settings
  monitoring:
    metrics_enabled: true
    metrics_port: 9090
    tracing_enabled: true
    tracing_endpoint: ${JAEGER_ENDPOINT}
    log_level: INFO
    structured_logging: true

  # Cache settings
  cache:
    enabled: true
    redis_url: ${REDIS_URL}
    memory_cache_size: 1000
    ttl_seconds: 3600
    compression_enabled: true","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1535","1577","yaml","`","# pipelines/code-review.yaml
name: automated_code_review
version: 1.0.0
description: Comprehensive code review with multiple analysis passes

steps:
  - id: code_parsing
    action: parse_code
    parameters:
      source: ""{{ github_pr_url }}""
      languages: <AUTO>Detect programming languages in PR</AUTO>

  - id: security_scan
    action: security_analysis
    parameters:
      code: ""{{ steps.code_parsing.parsed_code }}""
      severity_threshold: <AUTO>Based on project type, set threshold</AUTO>
      scan_depth: <AUTO>Determine scan depth based on code size</AUTO>
    dependencies: [code_parsing]

  - id: style_check
    action: style_analysis
    parameters:
      code: ""{{ steps.code_parsing.parsed_code }}""
      style_guide: <AUTO>Select appropriate style guide for language</AUTO>
    dependencies: [code_parsing]

  - id: complexity_analysis
    action: analyze_complexity
    parameters:
      code: ""{{ steps.code_parsing.parsed_code }}""
      metrics: <AUTO>Choose relevant complexity metrics</AUTO>
    dependencies: [code_parsing]

  - id: generate_review
    action: synthesize_review
    parameters:
      security_results: ""{{ steps.security_scan.findings }}""
      style_results: ""{{ steps.style_check.violations }}""
      complexity_results: ""{{ steps.complexity_analysis.metrics }}""
      review_tone: <AUTO>Professional, constructive, or educational</AUTO>
      priority_order: <AUTO>Order findings by severity and impact</AUTO>
    dependencies: [security_scan, style_check, complexity_analysis]","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1583","1629","yaml","`","# pipelines/data-analysis.yaml
name: intelligent_data_analysis
version: 1.0.0
description: Automated data analysis with visualization

steps:
  - id: data_ingestion
    action: load_data
    parameters:
      source: ""{{ data_source }}""
      format: <AUTO>Detect data format (csv, json, parquet, etc)</AUTO>
      sampling_strategy: <AUTO>Full load or sampling based on size</AUTO>

  - id: data_profiling
    action: profile_data
    parameters:
      data: ""{{ steps.data_ingestion.data }}""
      profile_depth: <AUTO>Basic, standard, or comprehensive</AUTO>
      anomaly_detection: <AUTO>Enable based on data characteristics</AUTO>
    dependencies: [data_ingestion]

  - id: statistical_analysis
    action: analyze_statistics
    parameters:
      data: ""{{ steps.data_ingestion.data }}""
      profile: ""{{ steps.data_profiling.profile }}""
      tests: <AUTO>Select appropriate statistical tests</AUTO>
      confidence_level: <AUTO>Set based on data quality</AUTO>
    dependencies: [data_profiling]

  - id: visualization_planning
    action: plan_visualizations
    parameters:
      data_profile: ""{{ steps.data_profiling.profile }}""
      insights: ""{{ steps.statistical_analysis.insights }}""
      num_visualizations: <AUTO>Determine optimal number</AUTO>
      chart_types: <AUTO>Select appropriate chart types</AUTO>
    dependencies: [statistical_analysis]

  - id: generate_report
    action: create_analysis_report
    parameters:
      visualizations: ""{{ steps.visualization_planning.charts }}""
      insights: ""{{ steps.statistical_analysis.insights }}""
      executive_summary: <AUTO>Generate executive summary</AUTO>
      technical_depth: <AUTO>Adjust based on audience</AUTO>
    dependencies: [visualization_planning]","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"design.md","1635","1678","yaml","`","# pipelines/ensemble-prediction.yaml
name: ensemble_prediction
version: 1.0.0
description: Ensemble multiple models for robust predictions

steps:
  - id: data_preparation
    action: prepare_data
    parameters:
      input_data: ""{{ raw_data }}""
      preprocessing: <AUTO>Determine required preprocessing steps</AUTO>
      feature_engineering: <AUTO>Identify useful feature transformations</AUTO>

  - id: model_selection
    action: select_models
    parameters:
      task_type: ""{{ prediction_task }}""
      num_models: <AUTO>Optimal ensemble size (3-7 models)</AUTO>
      diversity_strategy: <AUTO>Ensure model diversity</AUTO>
    dependencies: [data_preparation]

  - id: parallel_predictions
    action: batch_predict
    parameters:
      models: ""{{ steps.model_selection.selected_models }}""
      data: ""{{ steps.data_preparation.processed_data }}""
      execution_strategy: parallel
    dependencies: [model_selection]

  - id: ensemble_aggregation
    action: aggregate_predictions
    parameters:
      predictions: ""{{ steps.parallel_predictions.results }}""
      aggregation_method: <AUTO>voting, averaging, or stacking</AUTO>
      confidence_calculation: <AUTO>Method for confidence scores</AUTO>
    dependencies: [parallel_predictions]

  - id: result_validation
    action: validate_results
    parameters:
      ensemble_predictions: ""{{ steps.ensemble_aggregation.final_predictions }}""
      validation_threshold: <AUTO>Set based on task criticality</AUTO>
      fallback_strategy: <AUTO>Define fallback if validation fails</AUTO>
    dependencies: [ensemble_aggregation]","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","54","68","python","Edit config/models.yaml to customize model settings.","import asyncio
from orchestrator import Orchestrator

async def run_example():
    orchestrator = Orchestrator()

    # Run simple pipeline
    results = await orchestrator.execute_yaml_file(
        ""examples/simple_pipeline.yaml"",
        context={""input_topic"": ""machine learning""}
    )

    print(""Pipeline results:"", results)

asyncio.run(run_example())","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","74","80","bash","`","# Simple pipeline
python -m orchestrator run examples/simple_pipeline.yaml \
    --context input_topic=""machine learning""

# Multi-model pipeline
python -m orchestrator run examples/multi_model_pipeline.yaml \
    --context dataset_url=""https://example.com/sales_data.csv""","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","89","89","yaml","The examples demonstrate various AUTO tag patterns:","analysis_type: <AUTO>What type of analysis is most appropriate for this text?</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","94","94","yaml","`","format: <AUTO>Determine the best format for this data source</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","99","99","yaml","`","methods: <AUTO>Choose the most appropriate statistical methods</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","104","104","yaml","`","validation_rules: <AUTO>Generate appropriate validation rules for this dataset</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","112","131","yaml","`","id: my_pipeline
name: My Custom Pipeline
description: Description of what this pipeline does
version: ""1.0""

context:
  # Global variables accessible to all tasks
  variable_name: value

steps:
  - id: task1
    name: First Task
    action: generate  # or analyze, transform, etc.
    parameters:
      # Task-specific parameters
      prompt: ""Your prompt here""
    metadata:
      # Optional metadata
      requires_model: true
      priority: 1.0","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","137","152","yaml","`","steps:
  - id: advanced_task
    name: Advanced Task
    action: analyze
    parameters:
      data: ""{{ results.previous_task }}""  # Reference previous results
      method: <AUTO>Choose best method</AUTO>  # AUTO resolution
    dependencies:
      - previous_task  # Task dependencies
    metadata:
      requires_model: gpt-4  # Specific model requirement
      cpu_cores: 4  # Resource requirements
      memory_mb: 2048
      timeout: 300
      priority: 0.8
      on_failure: continue  # Error handling","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","181","184","python","Enable debug logging to see detailed execution information:","import logging
logging.basicConfig(level=logging.DEBUG)

orchestrator = Orchestrator()","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/README.md","192","193","python","Verify system health before running pipelines:","health = await orchestrator.health_check()
print(""System health:"", health[""overall""])","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","59","59","text","- Execution Engine: Action orchestration and error handling","Thought → Action → Observation → Thought → ...","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","68","68","text","- Observe results and adjust approach","Goal → Plan → Execute Steps → Evaluate → Replan if needed","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","74","76","text","Suitable for complex, multi-step tasks requiring upfront planning.","Agent A ↔ Agent B ↔ Agent C
   ↓         ↓         ↓
Shared State/Communication","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","88","95","python","- Key Features: Chains, agents, tools, memory, callbacks","from langchain.agents import create_react_agent
from langchain.tools import Tool

agent = create_react_agent(
    llm=llm,
    tools=[search_tool, calculator_tool],
    prompt=agent_prompt
)","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","104","107","python","- Key Features: Conversable agents, group chat, code execution","from autogen import AssistantAgent, UserProxyAgent

assistant = AssistantAgent(""assistant"", llm_config=llm_config)
user_proxy = UserProxyAgent(""user"", code_execution_config=Ellipsis)","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","116","120","python","- Key Features: Crews, roles, tasks, processes","from crewai import Agent, Task, Crew

researcher = Agent(role=""Researcher"", goal=""Find information"")
writer = Agent(role=""Writer"", goal=""Create content"")
crew = Crew(agents=[researcher, writer], tasks=[...])","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","128","144","python","`","# Using LangChain
from langchain.agents import initialize_agent, AgentType
from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun

tools = [
    DuckDuckGoSearchRun(),
    WikipediaQueryRun()
]

research_agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

result = research_agent.run(""What are the latest developments in quantum computing?"")","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","150","166","python","`","# Using AutoGen
coding_assistant = AssistantAgent(
    ""coding_assistant"",
    system_message=""You are a helpful AI that writes and explains code."",
    llm_config={""model"": ""gpt-4""}
)

user_proxy = UserProxyAgent(
    ""user_proxy"",
    human_input_mode=""NEVER"",
    code_execution_config={""work_dir"": ""coding"", ""use_docker"": False}
)

user_proxy.initiate_chat(
    coding_assistant,
    message=""Write a Python function to calculate fibonacci numbers efficiently""
)","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/draft_report.md","172","206","python","`","# Using CrewAI
from crewai import Agent, Task, Crew, Process

# Define agents
researcher = Agent(
    role='Senior Research Analyst',
    goal='Uncover cutting-edge developments in AI',
    backstory=""You're an expert researcher with a keen eye for detail.""
)

writer = Agent(
    role='Tech Content Strategist',
    goal='Create compelling content about AI developments',
    backstory=""You're a skilled writer who makes complex topics accessible.""
)

# Define tasks
research_task = Task(
    description='Research the latest AI developments in the past month',
    agent=researcher
)

writing_task = Task(
    description='Write a blog post about the research findings',
    agent=writer
)

# Create crew
crew = Crew(
    agents=[researcher, writer],
    tasks=[research_task, writing_task],
    process=Process.sequential
)

result = crew.kickoff()","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/final_report.md","59","59","text","- Execution Engine: Action orchestration and error handling","Thought → Action → Observation → Thought → ...","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/final_report.md","68","68","text","- Observe results and adjust approach","Goal → Plan → Execute Steps → Evaluate → Replan if needed","test_created","tests/snippet_tests/test_snippets_batch_2.py",""
"examples/output/readme_report/final_report.md","74","76","text","Suitable for complex, multi-step tasks requiring upfront planning.","Agent A ↔ Agent B ↔ Agent C
   ↓         ↓         ↓
Shared State/Communication","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/final_report.md","88","95","python","- Key Features: Chains, agents, tools, memory, callbacks","from langchain.agents import create_react_agent
from langchain.tools import Tool

agent = create_react_agent(
    llm=llm,
    tools=[search_tool, calculator_tool],
    prompt=agent_prompt
)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/final_report.md","104","107","python","- Key Features: Conversable agents, group chat, code execution","from autogen import AssistantAgent, UserProxyAgent

assistant = AssistantAgent(""assistant"", llm_config=llm_config)
user_proxy = UserProxyAgent(""user"", code_execution_config=Ellipsis)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/final_report.md","116","120","python","- Key Features: Crews, roles, tasks, processes","from crewai import Agent, Task, Crew

researcher = Agent(role=""Researcher"", goal=""Find information"")
writer = Agent(role=""Writer"", goal=""Create content"")
crew = Crew(agents=[researcher, writer], tasks=[...])","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/final_report.md","128","144","python","`","# Using LangChain
from langchain.agents import initialize_agent, AgentType
from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun

tools = [
    DuckDuckGoSearchRun(),
    WikipediaQueryRun()
]

research_agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

result = research_agent.run(""What are the latest developments in quantum computing?"")","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/final_report.md","150","166","python","`","# Using AutoGen
coding_assistant = AssistantAgent(
    ""coding_assistant"",
    system_message=""You are a helpful AI that writes and explains code."",
    llm_config={""model"": ""gpt-4""}
)

user_proxy = UserProxyAgent(
    ""user_proxy"",
    human_input_mode=""NEVER"",
    code_execution_config={""work_dir"": ""coding"", ""use_docker"": False}
)

user_proxy.initiate_chat(
    coding_assistant,
    message=""Write a Python function to calculate fibonacci numbers efficiently""
)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/final_report.md","172","206","python","`","# Using CrewAI
from crewai import Agent, Task, Crew, Process

# Define agents
researcher = Agent(
    role='Senior Research Analyst',
    goal='Uncover cutting-edge developments in AI',
    backstory=""You're an expert researcher with a keen eye for detail.""
)

writer = Agent(
    role='Tech Content Strategist',
    goal='Create compelling content about AI developments',
    backstory=""You're a skilled writer who makes complex topics accessible.""
)

# Define tasks
research_task = Task(
    description='Research the latest AI developments in the past month',
    agent=researcher
)

writing_task = Task(
    description='Write a blog post about the research findings',
    agent=writer
)

# Create crew
crew = Crew(
    agents=[researcher, writer],
    tasks=[research_task, writing_task],
    process=Process.sequential
)

result = crew.kickoff()","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","59","59","text","- Execution Engine: Action orchestration and error handling","Thought → Action → Observation → Thought → ...","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","68","68","text","- Observe results and adjust approach","Goal → Plan → Execute Steps → Evaluate → Replan if needed","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","74","76","text","Suitable for complex, multi-step tasks requiring upfront planning.","Agent A ↔ Agent B ↔ Agent C
   ↓         ↓         ↓
Shared State/Communication","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","88","95","python","- Key Features: Chains, agents, tools, memory, callbacks","from langchain.agents import create_react_agent
from langchain.tools import Tool

agent = create_react_agent(
    llm=llm,
    tools=[search_tool, calculator_tool],
    prompt=agent_prompt
)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","104","107","python","- Key Features: Conversable agents, group chat, code execution","from autogen import AssistantAgent, UserProxyAgent

assistant = AssistantAgent(""assistant"", llm_config=llm_config)
user_proxy = UserProxyAgent(""user"", code_execution_config=Ellipsis)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","116","120","python","- Key Features: Crews, roles, tasks, processes","from crewai import Agent, Task, Crew

researcher = Agent(role=""Researcher"", goal=""Find information"")
writer = Agent(role=""Writer"", goal=""Create content"")
crew = Crew(agents=[researcher, writer], tasks=[...])","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","128","144","python","`","# Using LangChain
from langchain.agents import initialize_agent, AgentType
from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun

tools = [
    DuckDuckGoSearchRun(),
    WikipediaQueryRun()
]

research_agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

result = research_agent.run(""What are the latest developments in quantum computing?"")","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","150","166","python","`","# Using AutoGen
coding_assistant = AssistantAgent(
    ""coding_assistant"",
    system_message=""You are a helpful AI that writes and explains code."",
    llm_config={""model"": ""gpt-4""}
)

user_proxy = UserProxyAgent(
    ""user_proxy"",
    human_input_mode=""NEVER"",
    code_execution_config={""work_dir"": ""coding"", ""use_docker"": False}
)

user_proxy.initiate_chat(
    coding_assistant,
    message=""Write a Python function to calculate fibonacci numbers efficiently""
)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"examples/output/readme_report/research_report.md","172","206","python","`","# Using CrewAI
from crewai import Agent, Task, Crew, Process

# Define agents
researcher = Agent(
    role='Senior Research Analyst',
    goal='Uncover cutting-edge developments in AI',
    backstory=""You're an expert researcher with a keen eye for detail.""
)

writer = Agent(
    role='Tech Content Strategist',
    goal='Create compelling content about AI developments',
    backstory=""You're a skilled writer who makes complex topics accessible.""
)

# Define tasks
research_task = Task(
    description='Research the latest AI developments in the past month',
    agent=researcher
)

writing_task = Task(
    description='Write a blog post about the research findings',
    agent=writer
)

# Create crew
crew = Crew(
    agents=[researcher, writer],
    tasks=[research_task, writing_task],
    process=Process.sequential
)

result = crew.kickoff()","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","57","64","bash","2. Dependencies: Install the orchestrator package and dependencies","# Install the package
pip install -e .

# Install Jupyter (if not already installed)
pip install jupyter

# Start Jupyter
jupyter notebook","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","79","81","python","If you're running from the source repository:","# Add the src directory to your Python path (included in notebooks)
import sys
sys.path.insert(0, '../src')","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","109","115","python","The tutorials use mock models for demonstration. To work with real AI models:","from orchestrator.integrations.openai_model import OpenAIModel

model = OpenAIModel(
    name=""gpt-4"",
    api_key=""your-openai-api-key"",
    model=""gpt-4""
)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","120","126","python","`","from orchestrator.integrations.anthropic_model import AnthropicModel

model = AnthropicModel(
    name=""claude-3-sonnet"",
    api_key=""your-anthropic-api-key"",
    model=""claude-3-sonnet-20240229""
)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","131","136","python","`","from orchestrator.integrations.huggingface_model import HuggingFaceModel

model = HuggingFaceModel(
    name=""llama-7b"",
    model_path=""meta-llama/Llama-2-7b-chat-hf""
)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","155","157","python","Import Errors","# Make sure the src path is correctly added
import sys
sys.path.insert(0, '../src')","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","162","163","python","Mock Model Responses","# Mock models require explicit response configuration
model.set_response(""your prompt"", ""expected response"")","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notebooks/README.md","168","169","python","Async/Await Issues","# Use await in Jupyter notebook cells
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notes/design_compliance_achievement.md","53","60","text","- Clarity: High - Includes tutorials and documentation","src/orchestrator/
├── core/                    # Core abstractions (Task, Pipeline, Model, etc.)
├── compiler/               # YAML parsing and compilation
├── executor/              # Execution engines (sandboxed, parallel)
├── adapters/              # Control system adapters (LangGraph, MCP)
├── models/                # Model registry and selection
├── state/                 # State management and checkpointing
└── orchestrator.py        # Main orchestration engine","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notes/phase2_completion_summary.md","73","77","bash","- Real-world Patterns: Demonstrates dependency management, error handling, resource allocation","✅ OpenAI model integration loads successfully
✅ Anthropic model integration loads successfully
✅ Google model integration loads successfully
✅ HuggingFace model integration loads successfully
✅ All model integrations imported successfully","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notes/phase2_completion_summary.md","82","85","bash","`","✅ YAML compilation successful
  - Pipeline ID: test_pipeline
  - Tasks: 2
  - AUTO resolved method: Mock response for: You are an AI pipeline orchestration expert...","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notes/phase2_completion_summary.md","90","92","bash","`","🚀 Starting orchestrator test...
❌ Pipeline execution failed: Task 'hello' failed and policy is 'fail'
Error: NoEligibleModelsError - No models meet the specified requirements","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notes/phase2_completion_summary.md","135","139","yaml","Revolutionary approach to pipeline ambiguity resolution:","# Before: Manual specification required
analysis_method: ""statistical""

# After: AI-resolved automatically
analysis_method: <AUTO>Choose the best analysis method for this data</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notes/phase2_completion_summary.md","145","151","python","Seamless integration across providers:","# Automatically selects best model for each task
pipeline = await orchestrator.execute_yaml(""""""
steps:
  - action: generate    # Uses GPT for generation
  - action: analyze     # Uses Claude for analysis
  - action: transform   # Uses Gemini for transformation
"""""")","test_created","tests/snippet_tests/test_snippets_batch_3.py",""
"notes/session_summary_context_limit.md","45","49","python","Core Framework Structure:","# Main abstractions in src/orchestrator/core/
- task.py:Task, TaskStatus (lines 1-200+)
- pipeline.py:Pipeline (lines 1-300+)
- model.py:Model, ModelCapabilities (lines 1-250+)
- control_system.py:ControlSystem, ControlAction (lines 1-150+)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"notes/session_summary_context_limit.md","54","60","python","Advanced Components:","# Advanced features in src/orchestrator/
- core/error_handler.py:ErrorHandler, CircuitBreaker (lines 1-400+)
- core/cache.py:MultiLevelCache (lines 1-550+)
- core/resource_allocator.py:ResourceAllocator (lines 1-450+)
- executor/parallel_executor.py:ParallelExecutor (lines 1-425+)
- executor/sandboxed_executor.py:SandboxManager (lines 1-345+)
- state/adaptive_checkpoint.py:AdaptiveCheckpointManager (lines 1-400+)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"notes/session_summary_context_limit.md","65","67","python","Control System Adapters:","# Adapters in src/orchestrator/adapters/
- langgraph_adapter.py:LangGraphAdapter (lines 1-350+)
- mcp_adapter.py:MCPAdapter (lines 1-450+)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/api/compiler.rst","22","26","python","**Example Usage:**","from orchestrator.compiler import YAMLCompiler

compiler = YAMLCompiler()
pipeline = compiler.compile_file(""my_pipeline.yaml"")","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/api/core.rst","108","125","python","~~~~~~~~~~~","from orchestrator import Task, Pipeline, Orchestrator

# Create a task
task = Task(
    id=""hello"",
    name=""Hello Task"",
    action=""generate_text"",
    parameters={""prompt"": ""Hello, world!""}
)

# Create a pipeline
pipeline = Pipeline(id=""demo"", name=""Demo Pipeline"")
pipeline.add_task(task)

# Execute with orchestrator
orchestrator = Orchestrator()
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/api/core.rst","131","140","yaml","~~~~~~~~~~~~~~~~~~","id: demo_pipeline
name: Demo Pipeline

tasks:
  - id: hello
    name: Hello Task
    action: generate_text
    parameters:
      prompt: ""Hello, world!""","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/api/core.rst","146","153","python","~~~~~~~~~~~~~~","from orchestrator.core.error_handler import ErrorHandler

error_handler = ErrorHandler()
orchestrator = Orchestrator(error_handler=error_handler)

# Tasks will automatically retry on failure
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/development/github_actions.rst","67","72","bash","If you prefer to update badges manually, you can extract coverage from the test output:","# Run tests with coverage
pytest --cov=src/orchestrator --cov-report=term

# The output will show coverage percentage
# Update the README badge URL with the percentage","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/development/github_actions.rst","93","100","markdown","You can customize badge colors and styles:","# Different styles
![Badge](https://img.shields.io/badge/style-flat-green)
![Badge](https://img.shields.io/badge/style-flat--square-green?style=flat-square)
![Badge](https://img.shields.io/badge/style-for--the--badge-green?style=for-the-badge)

# Custom colors
![Badge](https://img.shields.io/badge/custom-color-ff69b4)
![Badge](https://img.shields.io/badge/custom-color-blueviolet)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","28","40","python","* **Dependencies** - Other tasks that must complete first","from orchestrator import Task

task = Task(
    id=""summarize"",
    name=""Summarize Document"",
    action=""generate_text"",
    parameters={
        ""prompt"": ""Summarize this document: {document}"",
        ""max_tokens"": 150
    },
    dependencies=[""extract_document""]
)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","48","59","python","A **Pipeline** is a collection of tasks with defined dependencies. It represents your complete workflow:","from orchestrator import Pipeline

pipeline = Pipeline(
    id=""document_processing"",
    name=""Document Processing Pipeline""
)

# Add tasks to pipeline
pipeline.add_task(extract_task)
pipeline.add_task(summarize_task)
pipeline.add_task(classify_task)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","71","78","python","* **Custom models** (your own implementations)","from orchestrator.models import OpenAIModel

model = OpenAIModel(
    name=""gpt-4"",
    api_key=""your-api-key"",
    model=""gpt-4""
)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","91","97","python","* Manages state and checkpointing","from orchestrator import Orchestrator

orchestrator = Orchestrator()
orchestrator.register_model(model)

result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","105","115","python","Tasks can depend on other tasks, creating a directed acyclic graph (DAG):","# Task A (no dependencies)
task_a = Task(id=""a"", name=""Task A"", action=""generate_text"")

# Task B depends on A
task_b = Task(id=""b"", name=""Task B"", action=""generate_text"",
              dependencies=[""a""])

# Task C depends on A and B
task_c = Task(id=""c"", name=""Task C"", action=""generate_text"",
              dependencies=[""a"", ""b""])","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","123","126","text","The orchestrator automatically determines execution order based on dependencies:","Level 0: [Task A]           # No dependencies
Level 1: [Task B]           # Depends on A
Level 2: [Task C]           # Depends on A and B","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","136","150","python","Tasks can reference outputs from other tasks using template syntax:","task_a = Task(
    id=""extract"",
    name=""Extract Information"",
    action=""generate_text"",
    parameters={""prompt"": ""Extract key facts from: {document}""}
)

task_b = Task(
    id=""summarize"",
    name=""Summarize Facts"",
    action=""generate_text"",
    parameters={""prompt"": ""Summarize these facts: {extract}""},
    dependencies=[""extract""]
)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","165","171","python","6. **Returns** results from all tasks","# Execute pipeline
result = await orchestrator.execute_pipeline(pipeline)

# Access individual task results
print(result[""extract""])    # Output from extract task
print(result[""summarize""])  # Output from summarize task","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","184","191","python","* **Cost** - Resource usage and API costs","# Register multiple models
orchestrator.register_model(gpt4_model)
orchestrator.register_model(claude_model)
orchestrator.register_model(local_model)

# Orchestrator will select best model for each task
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","202","209","python","~~~~~~~~~~~~~~~~","from orchestrator.core.error_handler import ErrorHandler

error_handler = ErrorHandler()
orchestrator = Orchestrator(error_handler=error_handler)

# Tasks will automatically retry on failure
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","215","220","python","~~~~~~~~~~~~~~~~","# Circuit breaker prevents cascading failures
breaker = error_handler.get_circuit_breaker(""openai_api"")

# Executes with circuit breaker protection
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","226","232","python","~~~~~~~~~~~~~~~","# Register models in order of preference
orchestrator.register_model(primary_model)
orchestrator.register_model(fallback_model)

# Will use fallback if primary fails
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","243","250","python","~~~~~~~~~~~~~","from orchestrator.state import StateManager

state_manager = StateManager(storage_path=""./checkpoints"")
orchestrator = Orchestrator(state_manager=state_manager)

# Automatically saves checkpoints during execution
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","256","258","python","~~~~~~~~","# Resume from last checkpoint
result = await orchestrator.resume_pipeline(""pipeline_id"")","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","266","283","yaml","Define pipelines declaratively in YAML:","id: document_pipeline
name: Document Processing Pipeline

tasks:
  - id: extract
    name: Extract Information
    action: generate_text
    parameters:
      prompt: ""Extract key facts from: {document}""

  - id: summarize
    name: Summarize Facts
    action: generate_text
    parameters:
      prompt: ""Summarize these facts: {extract}""
    dependencies:
      - extract","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","288","294","python","Load and execute:","from orchestrator.compiler import YAMLCompiler

compiler = YAMLCompiler()
pipeline = compiler.compile_file(""document_pipeline.yaml"")

result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","303","310","python","~~~~~~~~~~~~~~~~~~","from orchestrator.core.resource_allocator import ResourceAllocator

allocator = ResourceAllocator()
orchestrator = Orchestrator(resource_allocator=allocator)

# Automatically manages CPU, memory, and API quotas
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","316","323","python","~~~~~~~~~~~~~~~~~~","from orchestrator.executor import ParallelExecutor

executor = ParallelExecutor(max_workers=4)
orchestrator = Orchestrator(executor=executor)

# Independent tasks run in parallel
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/basic_concepts.rst","329","336","python","~~~~~~~","from orchestrator.core.cache import MultiLevelCache

cache = MultiLevelCache()
orchestrator = Orchestrator(cache=cache)

# Results are cached for faster subsequent runs
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/installation.rst","26","27","bash","Install Orchestrator using pip:","pip install py-orc","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/installation.rst","37","40","bash","To install from source for development:","git clone https://github.com/ContextLab/orchestrator.git
cd orchestrator
pip install -e .","test_created","tests/snippet_tests/test_snippets_batch_4.py",""
"docs/getting_started/installation.rst","56","57","bash","For sandboxed execution with Docker:","pip install py-orc[docker]","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","64","65","bash","For persistent state storage:","pip install py-orc[database]","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","72","73","bash","For all optional dependencies:","pip install py-orc[all]","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","81","92","python","Verify your installation by running:","import orchestrator
print(f""Orchestrator version: {orchestrator.__version__}"")

# Test basic functionality
from orchestrator import Task, Pipeline

task = Task(id=""test"", name=""Test Task"", action=""echo"", parameters={""message"": ""Hello!""})
pipeline = Pipeline(id=""test_pipeline"", name=""Test Pipeline"")
pipeline.add_task(task)

print(""✅ Installation successful!"")","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","105","113","bash","Set these environment variables for optimal performance:","# Optional: Set cache directory
export ORCHESTRATOR_CACHE_DIR=/path/to/cache

# Optional: Set checkpoint directory
export ORCHESTRATOR_CHECKPOINT_DIR=/path/to/checkpoints

# Optional: Set log level
export ORCHESTRATOR_LOG_LEVEL=INFO","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","121","129","bash","Configure API keys for external services:","# OpenAI
export OPENAI_API_KEY=your_openai_key

# Anthropic
export ANTHROPIC_API_KEY=your_anthropic_key

# Google
export GOOGLE_API_KEY=your_google_key","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","137","139","bash","If using Docker features, ensure Docker is running:","docker --version
docker run hello-world","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","157","166","bash","Install system dependencies:","# Ubuntu/Debian
sudo apt-get update
sudo apt-get install python3-dev build-essential

# macOS
brew install python

# Windows
# Use Python from python.org","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/installation.rst","172","174","bash","Ensure Docker is installed and running:","docker --version
docker info","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","13","39","python","Let's create a simple text generation pipeline:","from orchestrator import Orchestrator, Task, Pipeline
from orchestrator.models.mock_model import MockModel

# Create a mock model for testing
model = MockModel(""gpt-test"")
model.set_response(""Hello, world!"", ""Hello! How can I help you today?"")

# Create a task
task = Task(
    id=""greeting"",
    name=""Generate Greeting"",
    action=""generate_text"",
    parameters={""prompt"": ""Hello, world!""}
)

# Create a pipeline
pipeline = Pipeline(id=""hello_pipeline"", name=""Hello Pipeline"")
pipeline.add_task(task)

# Create orchestrator and register model
orchestrator = Orchestrator()
orchestrator.register_model(model)

# Execute pipeline
result = await orchestrator.execute_pipeline(pipeline)
print(f""Result: {result['greeting']}"")","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","47","86","python","Let's create a more complex pipeline with multiple tasks:","from orchestrator import Task, Pipeline

# Task 1: Generate story outline
outline_task = Task(
    id=""outline"",
    name=""Generate Story Outline"",
    action=""generate_text"",
    parameters={""prompt"": ""Create a story outline about space exploration""}
)

# Task 2: Write story (depends on outline)
story_task = Task(
    id=""story"",
    name=""Write Story"",
    action=""generate_text"",
    parameters={""prompt"": ""Write a story based on: {outline}""},
    dependencies=[""outline""]
)

# Task 3: Summarize story (depends on story)
summary_task = Task(
    id=""summary"",
    name=""Summarize Story"",
    action=""generate_text"",
    parameters={""prompt"": ""Summarize this story: {story}""},
    dependencies=[""story""]
)

# Create pipeline with all tasks
pipeline = Pipeline(id=""story_pipeline"", name=""Story Creation Pipeline"")
pipeline.add_task(outline_task)
pipeline.add_task(story_task)
pipeline.add_task(summary_task)

# Execute pipeline
result = await orchestrator.execute_pipeline(pipeline)
print(f""Outline: {result['outline']}"")
print(f""Story: {result['story']}"")
print(f""Summary: {result['summary']}"")","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","94","120","yaml","You can also define pipelines in YAML:","# story_pipeline.yaml
id: story_pipeline
name: Story Creation Pipeline

tasks:
  - id: outline
    name: Generate Story Outline
    action: generate_text
    parameters:
      prompt: ""Create a story outline about space exploration""

  - id: story
    name: Write Story
    action: generate_text
    parameters:
      prompt: ""Write a story based on: {outline}""
    dependencies:
      - outline

  - id: summary
    name: Summarize Story
    action: generate_text
    parameters:
      prompt: ""Summarize this story: {story}""
    dependencies:
      - story","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","125","133","python","Load and execute the YAML pipeline:","from orchestrator.compiler import YAMLCompiler

# Load pipeline from YAML
compiler = YAMLCompiler()
pipeline = compiler.compile_file(""story_pipeline.yaml"")

# Execute pipeline
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","141","155","python","Let's use a real AI model instead of the mock:","from orchestrator.models.openai_model import OpenAIModel

# Create OpenAI model
openai_model = OpenAIModel(
    name=""gpt-4"",
    api_key=""your-api-key-here"",
    model=""gpt-4""
)

# Register model
orchestrator.register_model(openai_model)

# Execute pipeline (will use OpenAI)
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","163","176","python","Orchestrator provides built-in error handling:","from orchestrator.core.error_handler import ErrorHandler

# Create error handler with retry strategy
error_handler = ErrorHandler()

# Configure orchestrator with error handling
orchestrator = Orchestrator(error_handler=error_handler)

# Execute pipeline with automatic retry on failures
try:
    result = await orchestrator.execute_pipeline(pipeline)
except Exception as e:
    print(f""Pipeline failed: {e}"")","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","184","194","python","Enable checkpointing for long-running pipelines:","from orchestrator.state import StateManager

# Create state manager
state_manager = StateManager(storage_path=""./checkpoints"")

# Configure orchestrator with state management
orchestrator = Orchestrator(state_manager=state_manager)

# Execute pipeline with automatic checkpointing
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/quickstart.rst","202","214","python","Enable monitoring to track pipeline execution:","import logging

# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# Execute pipeline with logging
result = await orchestrator.execute_pipeline(pipeline)

# Get execution statistics
stats = orchestrator.get_execution_stats()
print(f""Execution time: {stats['total_time']:.2f}s"")
print(f""Tasks completed: {stats['completed_tasks']}"")","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","24","46","python","First, let's set up our environment:","import asyncio
from orchestrator import Orchestrator, Task, Pipeline
from orchestrator.models.mock_model import MockModel

# Create a mock model for testing
model = MockModel(""research_assistant"")

# Set up responses for our mock model
model.set_response(
    ""Generate 3 research questions about: artificial intelligence"",
    ""1. How does AI impact job markets?\n2. What are the ethical implications of AI?\n3. How can AI be made more accessible?""
)

model.set_response(
    ""Analyze these questions and identify key themes: 1. How does AI impact job markets?\n2. What are the ethical implications of AI?\n3. How can AI be made more accessible?"",
    ""Key themes identified: Economic Impact, Ethics and Responsibility, Accessibility and Democratization""
)

model.set_response(
    ""Write a comprehensive report on artificial intelligence covering these themes: Economic Impact, Ethics and Responsibility, Accessibility and Democratization"",
    ""# AI Research Report\n\n## Economic Impact\nAI is reshaping job markets...\n\n## Ethics and Responsibility\nAI systems must be developed responsibly...\n\n## Accessibility and Democratization\nMaking AI tools accessible to all...""
)","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","54","88","python","Now let's create our three tasks:","# Task 1: Generate research questions
research_task = Task(
    id=""research_questions"",
    name=""Generate Research Questions"",
    action=""generate_text"",
    parameters={
        ""prompt"": ""Generate 3 research questions about: {topic}"",
        ""max_tokens"": 200
    }
)

# Task 2: Analyze questions for themes
analysis_task = Task(
    id=""analyze_themes"",
    name=""Analyze Key Themes"",
    action=""generate_text"",
    parameters={
        ""prompt"": ""Analyze these questions and identify key themes: {research_questions}"",
        ""max_tokens"": 150
    },
    dependencies=[""research_questions""]  # Depends on research task
)

# Task 3: Write comprehensive report
report_task = Task(
    id=""write_report"",
    name=""Write Research Report"",
    action=""generate_text"",
    parameters={
        ""prompt"": ""Write a comprehensive report on {topic} covering these themes: {analyze_themes}"",
        ""max_tokens"": 500
    },
    dependencies=[""analyze_themes""]  # Depends on analysis task
)","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","96","114","python","Combine tasks into a pipeline:","# Create pipeline
pipeline = Pipeline(
    id=""research_assistant"",
    name=""Research Assistant Pipeline"",
    description=""Generates research questions, analyzes themes, and writes a report""
)

# Add tasks to pipeline
pipeline.add_task(research_task)
pipeline.add_task(analysis_task)
pipeline.add_task(report_task)

# Set initial context
pipeline.set_context(""topic"", ""artificial intelligence"")

print(""Pipeline created successfully!"")
print(f""Tasks: {list(pipeline.tasks.keys())}"")
print(f""Execution order: {pipeline.get_execution_order()}"")","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","122","143","python","Now let's execute our pipeline:","async def run_pipeline():
    # Create orchestrator
    orchestrator = Orchestrator()

    # Register our model
    orchestrator.register_model(model)

    print(""Starting pipeline execution..."")

    # Execute pipeline
    result = await orchestrator.execute_pipeline(pipeline)

    print(""\n=== Pipeline Results ==="")
    print(f""Research Questions:\n{result['research_questions']}\n"")
    print(f""Key Themes:\n{result['analyze_themes']}\n"")
    print(f""Final Report:\n{result['write_report']}\n"")

    return result

# Run the pipeline
result = await run_pipeline()","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","151","181","python","Let's make our pipeline more robust:","from orchestrator.core.error_handler import ErrorHandler
from orchestrator.core.error_handler import ExponentialBackoffRetry

async def run_robust_pipeline():
    # Create error handler with retry strategy
    error_handler = ErrorHandler()
    error_handler.register_retry_strategy(
        ""research_retry"",
        ExponentialBackoffRetry(max_retries=3, base_delay=1.0)
    )

    # Create orchestrator with error handling
    orchestrator = Orchestrator(error_handler=error_handler)
    orchestrator.register_model(model)

    try:
        print(""Starting robust pipeline execution..."")
        result = await orchestrator.execute_pipeline(pipeline)
        print(""✅ Pipeline completed successfully!"")
        return result

    except Exception as e:
        print(f""❌ Pipeline failed: {e}"")
        # Get execution statistics
        stats = error_handler.get_error_statistics()
        print(f""Errors encountered: {stats['total_errors']}"")
        return None

# Run robust pipeline
result = await run_robust_pipeline()","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","189","214","python","For longer pipelines, add checkpointing:","from orchestrator.state import StateManager

async def run_stateful_pipeline():
    # Create state manager
    state_manager = StateManager(storage_path=""./checkpoints"")

    # Create orchestrator with state management
    orchestrator = Orchestrator(state_manager=state_manager)
    orchestrator.register_model(model)

    print(""Starting stateful pipeline execution..."")

    # Execute with automatic checkpointing
    result = await orchestrator.execute_pipeline(pipeline)

    print(""✅ Pipeline completed with checkpointing!"")

    # List checkpoints created
    checkpoints = await state_manager.list_checkpoints(""research_assistant"")
    print(f""Checkpoints created: {len(checkpoints)}"")

    return result

# Run stateful pipeline
result = await run_stateful_pipeline()","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","222","255","yaml","Let's convert our pipeline to YAML:","# research_pipeline.yaml
id: research_assistant
name: Research Assistant Pipeline
description: Generates research questions, analyzes themes, and writes a report

context:
  topic: artificial intelligence

tasks:
  - id: research_questions
    name: Generate Research Questions
    action: generate_text
    parameters:
      prompt: ""Generate 3 research questions about: {topic}""
      max_tokens: 200

  - id: analyze_themes
    name: Analyze Key Themes
    action: generate_text
    parameters:
      prompt: ""Analyze these questions and identify key themes: {research_questions}""
      max_tokens: 150
    dependencies:
      - research_questions

  - id: write_report
    name: Write Research Report
    action: generate_text
    parameters:
      prompt: ""Write a comprehensive report on {topic} covering these themes: {analyze_themes}""
      max_tokens: 500
    dependencies:
      - analyze_themes","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","260","281","python","Load and execute the YAML pipeline:","from orchestrator.compiler import YAMLCompiler

async def run_yaml_pipeline():
    # Create compiler and load pipeline
    compiler = YAMLCompiler()
    pipeline = compiler.compile_file(""research_pipeline.yaml"")

    # Create orchestrator
    orchestrator = Orchestrator()
    orchestrator.register_model(model)

    print(""Starting YAML pipeline execution..."")

    # Execute pipeline
    result = await orchestrator.execute_pipeline(pipeline)

    print(""✅ YAML pipeline completed!"")
    return result

# Run YAML pipeline
result = await run_yaml_pipeline()","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","289","313","python","Replace mock model with real AI:","from orchestrator.models.openai_model import OpenAIModel

async def run_with_real_ai():
    # Create OpenAI model
    openai_model = OpenAIModel(
        name=""gpt-4"",
        api_key=""your-openai-api-key"",
        model=""gpt-4""
    )

    # Create orchestrator with real AI
    orchestrator = Orchestrator()
    orchestrator.register_model(openai_model)

    print(""Starting pipeline with real AI..."")

    # Execute pipeline with real AI
    result = await orchestrator.execute_pipeline(pipeline)

    print(""✅ Real AI pipeline completed!"")
    return result

# Run with real AI (uncomment when you have API keys)
# result = await run_with_real_ai()","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","321","351","python","Add monitoring to track performance:","import time
from orchestrator.core.resource_allocator import ResourceAllocator

async def run_monitored_pipeline():
    # Create resource allocator for monitoring
    allocator = ResourceAllocator()

    # Create orchestrator with monitoring
    orchestrator = Orchestrator(resource_allocator=allocator)
    orchestrator.register_model(model)

    print(""Starting monitored pipeline execution..."")
    start_time = time.time()

    # Execute pipeline
    result = await orchestrator.execute_pipeline(pipeline)

    end_time = time.time()
    execution_time = end_time - start_time

    print(f""✅ Pipeline completed in {execution_time:.2f} seconds"")

    # Get resource statistics
    stats = allocator.get_overall_statistics()
    print(f""Resource utilization: {stats['overall_utilization']:.2f}"")

    return result

# Run monitored pipeline
result = await run_monitored_pipeline()","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/getting_started/your_first_pipeline.rst","359","474","python","Here's the complete, production-ready pipeline:","import asyncio
import logging
from orchestrator import Orchestrator, Task, Pipeline
from orchestrator.models.mock_model import MockModel
from orchestrator.core.error_handler import ErrorHandler
from orchestrator.state import StateManager
from orchestrator.core.resource_allocator import ResourceAllocator

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def create_research_pipeline():
    """"""Create a production-ready research assistant pipeline.""""""

    # Create mock model with responses
    model = MockModel(""research_assistant"")
    model.set_response(
        ""Generate 3 research questions about: artificial intelligence"",
        ""1. How does AI impact job markets?\n2. What are the ethical implications of AI?\n3. How can AI be made more accessible?""
    )
    model.set_response(
        ""Analyze these questions and identify key themes: 1. How does AI impact job markets?\n2. What are the ethical implications of AI?\n3. How can AI be made more accessible?"",
        ""Key themes: Economic Impact, Ethics and Responsibility, Accessibility""
    )
    model.set_response(
        ""Write a comprehensive report on artificial intelligence covering these themes: Economic Impact, Ethics and Responsibility, Accessibility"",
        ""# AI Research Report\n\n## Economic Impact\nAI is transforming industries...\n\n## Ethics\nResponsible AI development...\n\n## Accessibility\nDemocratizing AI tools...""
    )

    # Create tasks
    tasks = [
        Task(
            id=""research_questions"",
            name=""Generate Research Questions"",
            action=""generate_text"",
            parameters={
                ""prompt"": ""Generate 3 research questions about: {topic}"",
                ""max_tokens"": 200
            }
        ),
        Task(
            id=""analyze_themes"",
            name=""Analyze Key Themes"",
            action=""generate_text"",
            parameters={
                ""prompt"": ""Analyze these questions and identify key themes: {research_questions}"",
                ""max_tokens"": 150
            },
            dependencies=[""research_questions""]
        ),
        Task(
            id=""write_report"",
            name=""Write Research Report"",
            action=""generate_text"",
            parameters={
                ""prompt"": ""Write a comprehensive report on {topic} covering these themes: {analyze_themes}"",
                ""max_tokens"": 500
            },
            dependencies=[""analyze_themes""]
        )
    ]

    # Create pipeline
    pipeline = Pipeline(
        id=""research_assistant"",
        name=""Research Assistant Pipeline""
    )

    for task in tasks:
        pipeline.add_task(task)

    pipeline.set_context(""topic"", ""artificial intelligence"")

    # Create components
    error_handler = ErrorHandler()
    state_manager = StateManager(storage_path=""./checkpoints"")
    resource_allocator = ResourceAllocator()

    # Create orchestrator
    orchestrator = Orchestrator(
        error_handler=error_handler,
        state_manager=state_manager,
        resource_allocator=resource_allocator
    )

    orchestrator.register_model(model)

    return orchestrator, pipeline

async def main():
    """"""Main execution function.""""""
    logger.info(""Creating research assistant pipeline..."")

    orchestrator, pipeline = await create_research_pipeline()

    logger.info(""Executing pipeline..."")

    try:
        result = await orchestrator.execute_pipeline(pipeline)

        logger.info(""Pipeline completed successfully!"")

        print(""\n=== Results ==="")
        for task_id, output in result.items():
            print(f""\n{task_id}:"")
            print(f""{output}"")

    except Exception as e:
        logger.error(f""Pipeline failed: {e}"")
        raise

# Run the complete example
if __name__ == ""__main__"":
    asyncio.run(main())","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/index.rst","27","28","bash","Get started with Orchestrator in just a few minutes:","pip install py-orc","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/index.rst","107","124","text","The Orchestrator Framework is built with a modular architecture that separates concerns and promotes extensibility:","┌─────────────────────────────────────────────────────────────┐
│                    Orchestrator Engine                      │
└─────────────────────────────────────────────────────────────┘

┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐
│  YAML Compiler  │  │ Model Registry  │  │ State Manager   │
│  - Parser       │  │ - Selection     │  │ - Checkpoints   │
│  - Validation   │  │ - Load Balance  │  │ - Recovery      │
│  - Templates    │  │ - Health Check  │  │ - Persistence   │
└─────────────────┘  └─────────────────┘  └─────────────────┘

┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐
│ Execution Layer │  │ Error Handler   │  │ Resource Mgmt   │
│ - Parallel      │  │ - Circuit Break │  │ - Allocation    │
│ - Sandboxed     │  │ - Retry Logic   │  │ - Monitoring    │
│ - Distributed   │  │ - Recovery      │  │ - Optimization  │
└─────────────────┘  └─────────────────┘  └─────────────────┘","test_created","tests/snippet_tests/test_snippets_batch_5.py",""
"docs/tutorials/notebooks.rst","55","64","bash","~~~~~~~~~~~~","# Install Orchestrator Framework
pip install py-orc

# Install Jupyter (if not already installed)
pip install jupyter

# Clone the repository for tutorials
git clone https://github.com/ContextLab/orchestrator.git
cd orchestrator","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","70","75","bash","~~~~~~~~~~~~~~~~~","# Start Jupyter Notebook
jupyter notebook

# Or start JupyterLab
jupyter lab","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","112","130","python","* Add state management for reliability","# Example from Tutorial 01
from orchestrator import Orchestrator, Task, Pipeline
from orchestrator.models.mock_model import MockModel

# Create your first task
task = Task(
    id=""hello_world"",
    name=""Hello World Task"",
    action=""generate_text"",
    parameters={""prompt"": ""Hello, Orchestrator!""}
)

# Build and execute pipeline
pipeline = Pipeline(id=""first_pipeline"", name=""First Pipeline"")
pipeline.add_task(task)

orchestrator = Orchestrator()
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","165","186","yaml","* Create reusable pipeline templates","# Example from Tutorial 02
id: research_pipeline
name: Research Assistant Pipeline

context:
  topic: artificial intelligence

tasks:
  - id: research
    name: Generate Research Questions
    action: generate_text
    parameters:
      prompt: ""Research questions about: {topic}""

  - id: analyze
    name: Analyze Themes
    action: generate_text
    parameters:
      prompt: ""Analyze themes in: {research}""
    dependencies:
      - research","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","221","234","python","* Optimize for cost and latency","# Example from Tutorial 03
from orchestrator.models.openai_model import OpenAIModel
from orchestrator.models.anthropic_model import AnthropicModel

# Register multiple models
gpt4 = OpenAIModel(name=""gpt-4"", api_key=""your-key"")
claude = AnthropicModel(name=""claude-3"", api_key=""your-key"")

orchestrator.register_model(gpt4)
orchestrator.register_model(claude)

# Orchestrator automatically selects best model
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","275","291","text","The tutorials come with supporting files:","notebooks/
├── 01_getting_started.ipynb
├── 02_yaml_configuration.ipynb
├── 03_advanced_model_integration.ipynb
├── README.md                           # Tutorial guide
├── data/                               # Sample data files
│   ├── sample_pipeline.yaml
│   ├── complex_workflow.yaml
│   └── test_data.json
├── images/                             # Tutorial images
│   ├── architecture_diagram.png
│   └── workflow_visualization.png
└── solutions/                          # Exercise solutions
    ├── 01_solutions.ipynb
    ├── 02_solutions.ipynb
    └── 03_solutions.ipynb","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","310","315","bash","**Jupyter Not Starting**","# Try updating Jupyter
pip install --upgrade jupyter

# Or install JupyterLab
pip install jupyterlab","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","319","324","python","**Import Errors**","# Make sure Orchestrator is installed
pip install py-orc

# Or install in development mode
pip install -e .","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","328","330","python","**Mock Model Issues**","# Mock models need explicit responses
model.set_response(""your prompt"", ""expected response"")","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/tutorials/notebooks.rst","334","336","python","**Async/Await Problems**","# Use await in notebook cells
result = await orchestrator.execute_pipeline(pipeline)","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","33","35","bash","When you install Orchestrator via pip, default configuration files are available but not automatically installed to avoid overwriting existing configurations. To install the default configurations:","# Install default configs to ~/.orchestrator/
orchestrator-install-configs","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","53","81","yaml","The models configuration file defines available AI models:","models:
  # Local models (via Ollama)
  - source: ollama
    name: llama3.1:8b
    expertise: [general, reasoning, multilingual]
    size: 8b

  # Cloud models
  - source: openai
    name: gpt-4o
    expertise: [general, reasoning, code, analysis, vision]
    size: 1760b  # Estimated

  # HuggingFace models
  - source: huggingface
    name: microsoft/Phi-3.5-mini-instruct
    expertise: [reasoning, code, compact]
    size: 3.8b

defaults:
  expertise_preferences:
    code: qwen2.5-coder:7b
    reasoning: deepseek-r1:8b
    fast: llama3.2:1b
  fallback_chain:
    - llama3.1:8b
    - mistral:7b
    - llama3.2:1b","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","86","91","yaml","You can add new models by editing this file:","# Add a new Ollama model
- source: ollama
  name: my-custom-model:13b
  expertise: [domain-specific, analysis]
  size: 13b","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","99","129","yaml","The main configuration file controls framework behavior:","# Execution settings
execution:
  parallel_tasks: 10
  timeout_seconds: 300
  retry_attempts: 3
  retry_delay: 1.0

# Resource limits
resources:
  max_memory_mb: 8192
  max_cpu_percent: 80
  gpu_enabled: true

# Caching
cache:
  enabled: true
  ttl_seconds: 3600
  max_size_mb: 1024

# Monitoring
monitoring:
  log_level: INFO
  metrics_enabled: true
  trace_enabled: false

# Error handling
error_handling:
  circuit_breaker_threshold: 5
  circuit_breaker_timeout: 60
  fallback_enabled: true","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","137","144","bash","You can override configuration settings using environment variables:","# Set custom config location
export ORCHESTRATOR_HOME=/path/to/configs

# Override specific settings
export ORCHESTRATOR_LOG_LEVEL=DEBUG
export ORCHESTRATOR_PARALLEL_TASKS=20
export ORCHESTRATOR_CACHE_ENABLED=false","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","161","167","python","Orchestrator validates configuration files on startup:","import orchestrator as orc

# Validate configuration files
config_valid, errors = orc.validate_config()
if not config_valid:
    print(""Configuration errors:"", errors)","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","176","187","yaml","~~~~~~~~~~~~~~~~~~~~~~","# orchestrator.yaml for development
execution:
  parallel_tasks: 2
  timeout_seconds: 60

monitoring:
  log_level: DEBUG
  trace_enabled: true

cache:
  enabled: false  # Disable cache for testing","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","193","206","yaml","~~~~~~~~~~~~~~~~~~~~~~","# orchestrator.yaml for production
execution:
  parallel_tasks: 50
  timeout_seconds: 600
  retry_attempts: 5

monitoring:
  log_level: WARNING
  metrics_enabled: true

error_handling:
  circuit_breaker_threshold: 10
  fallback_enabled: true","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","212","224","yaml","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","# models.yaml for limited resources
models:
  # Only small, efficient models
  - source: ollama
    name: llama3.2:1b
    expertise: [general, fast]
    size: 1b

  - source: ollama
    name: phi-3-mini:3.8b
    expertise: [reasoning, compact]
    size: 3.8b","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/configuration.rst","230","242","yaml","~~~~~~~~~~~~~~~~~~~~~~~~~~~~","# orchestrator.yaml for high performance
execution:
  parallel_tasks: 100
  use_gpu: true

resources:
  max_memory_mb: 65536
  gpu_memory_fraction: 0.9

cache:
  backend: redis
  redis_url: redis://localhost:6379","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","16","61","yaml","~~~~~~~~~~~~~~~~~~~~~~~","models:
  # Ollama models (automatically installed if not present)
  - source: ollama
    name: gemma2:27b
    expertise:
      - general
      - reasoning
      - analysis
    size: 27b

  - source: ollama
    name: codellama:7b
    expertise:
      - code
      - programming
    size: 7b

  # HuggingFace models (automatically downloaded)
  - source: huggingface
    name: microsoft/phi-2
    expertise:
      - reasoning
      - code
    size: 2.7b

  # Cloud models (require API keys)
  - source: openai
    name: gpt-4o
    expertise:
      - general
      - reasoning
      - code
      - analysis
      - vision
    size: 1760b

defaults:
  expertise_preferences:
    code: codellama:7b
    reasoning: gemma2:27b
    fast: llama3.2:1b
  fallback_chain:
    - gemma2:27b
    - llama3.2:1b
    - TinyLlama/TinyLlama-1.1B-Chat-v1.0","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","104","112","python","The framework uses lazy loading for both Ollama and HuggingFace models to avoid downloading large models until they're actually needed:","import orchestrator as orc

# This registers models but doesn't download them yet
registry = orc.init_models()

# Models are downloaded only when first used by a pipeline
pipeline = orc.compile(""my_pipeline.yaml"")
result = pipeline.run()  # Model downloads happen here if needed","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","130","133","yaml","HuggingFace models are also downloaded on first use:","- source: huggingface
  name: microsoft/Phi-3.5-mini-instruct
  expertise: [reasoning, code]","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","156","162","yaml","Specify a model by name:","steps:
  - id: summarize
    action: generate_text
    parameters:
      prompt: ""Summarize this text...""
    requires_model: gemma2:27b  # Use specific model","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","170","178","yaml","Specify requirements and let the framework choose:","steps:
  - id: generate_code
    action: generate_text
    parameters:
      prompt: ""Write a Python function...""
    requires_model:
      expertise: code
      min_size: 7b  # At least 7B parameters","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","186","196","yaml","Specify multiple expertise areas (any match will qualify):","steps:
  - id: analyze
    action: analyze
    parameters:
      content: ""{input_data}""
    requires_model:
      expertise:
        - reasoning
        - analysis
      min_size: 20b","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","204","240","yaml","Here's a complete pipeline demonstrating model requirements:","id: multi_model_pipeline
name: Multi-Model Processing Pipeline

inputs:
  - name: topic
    type: string

steps:
  # Fast task with small model
  - id: quick_check
    action: generate_text
    parameters:
      prompt: ""Is this topic related to programming: {topic}?""
    requires_model:
      expertise: fast
      min_size: 0  # Any size

  # Code generation with specialized model
  - id: code_example
    action: generate_text
    parameters:
      prompt: ""Generate example code for: {topic}""
    requires_model:
      expertise: code
      min_size: 7b
    dependencies: [quick_check]

  # Complex reasoning with large model
  - id: deep_analysis
    action: analyze
    parameters:
      content: ""{topic} with code: {code_example.result}""
    requires_model:
      expertise: [reasoning, analysis]
      min_size: 27b
    dependencies: [code_example]","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","280","291","python","Check which models are being used:","import orchestrator as orc

# Initialize and list available models
registry = orc.init_models()
print(""Available models:"")
for model_key in registry.list_models():
    print(f""  - {model_key}"")

# Run pipeline and check model selection
pipeline = orc.compile(""pipeline.yaml"")
result = pipeline.run(topic=""AI agents"")","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","296","299","text","The framework logs model selection decisions:",">> Using model for task 'quick_check': ollama:llama3.2:1b (fast, 1B params)
>> Using model for task 'code_example': ollama:codellama:7b (code, 7B params)
>> Using model for task 'deep_analysis': ollama:gemma2:27b (reasoning, 27B params)","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","337","338","text","**Model Installation Fails**:",">> ❌ Failed to install gemma2:27b: connection timeout","test_created","tests/snippet_tests/test_snippets_batch_6.py",""
"docs/user_guide/model_configuration.rst","348","349","text","**No Models Match Requirements**:","NoEligibleModelsError: No models meet the specified requirements","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/model_configuration.rst","359","360","text","**API Key Missing**:",">> ⚠️  OpenAI models configured but OPENAI_API_KEY not set","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","34","50","python","~~~~~~~~~~~~~~~~~~~","import orchestrator as orc

# Initialize and discover available models
registry = orc.init_models()

# List all detected models
available_models = registry.list_models()
print(""Available models:"", available_models)

# Check specific model availability
if any(""gemma2:27b"" in model for model in available_models):
    print(""Large Ollama model available"")
elif any(""llama3.2:1b"" in model for model in available_models):
    print(""Lightweight Ollama model available"")
else:
    print(""Using fallback models"")","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","60","67","bash","Install Ollama and pull recommended models:","# Install Ollama
brew install ollama  # macOS
# or visit https://ollama.ai for other platforms

# Pull recommended models
ollama pull gemma2:27b    # Large model for complex tasks
ollama pull llama3.2:1b   # Lightweight fallback","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","74","75","bash","Install the transformers library:","pip install transformers torch","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","82","85","bash","Set up API keys as environment variables:","export OPENAI_API_KEY=""sk-...""
export ANTHROPIC_API_KEY=""sk-ant-...""
export GOOGLE_API_KEY=""...""","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","96","103","python","~~~~~~~~~~~~~","from orchestrator.models.openai_model import OpenAIModel

model = OpenAIModel(
    name=""gpt-4o"",
    api_key=""your-api-key"",
    model=""gpt-4o""
)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","109","116","python","~~~~~~~~~~~~~~~~","from orchestrator.models.anthropic_model import AnthropicModel

model = AnthropicModel(
    name=""claude-3.5-sonnet"",
    api_key=""your-api-key"",
    model=""claude-3.5-sonnet""
)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","122","128","python","~~~~~~~~~~~~","from orchestrator.models.huggingface_model import HuggingFaceModel

model = HuggingFaceModel(
    name=""llama-3.2-3b"",
    model_path=""meta-llama/Llama-3.2-3B-Instruct""
)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/models_and_adapters.rst","136","144","python","The model registry manages model selection and load balancing:","from orchestrator.models.model_registry import ModelRegistry

registry = ModelRegistry()
registry.register_model(gpt4_model)
registry.register_model(claude_model)

# Automatic selection based on task requirements
selected_model = registry.select_model(task)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/yaml_configuration.rst","26","44","yaml","A basic pipeline YAML file contains:","id: my_pipeline
name: My Pipeline
description: A sample pipeline

tasks:
  - id: task1
    name: First Task
    action: generate_text
    parameters:
      prompt: ""Hello, world!""

  - id: task2
    name: Second Task
    action: generate_text
    parameters:
      prompt: ""Process this: {task1}""
    dependencies:
      - task1","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/yaml_configuration.rst","52","65","yaml","Use template variables for dynamic content:","id: research_pipeline
name: Research Pipeline

context:
  topic: artificial intelligence
  depth: detailed

tasks:
  - id: research
    name: Research Task
    action: generate_text
    parameters:
      prompt: ""Research {topic} with {depth} analysis""","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs/user_guide/yaml_configuration.rst","73","80","yaml","The AUTO tag automatically resolves ambiguous parameters:","tasks:
  - id: analysis
    name: Analysis Task
    action: <AUTO>
    parameters:
      data: {previous_task}
      model: <AUTO>","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","118","128","python","-----------","import orchestrator as orc

# Initialize models
registry = orc.init_models()

# Compile pipeline
pipeline = orc.compile(""my_pipeline.yaml"")

# Execute
result = pipeline.run(input_param=""value"")","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","134","151","python","--------------","from orchestrator import Orchestrator
from orchestrator.core.control_system import MockControlSystem
from orchestrator.models.model_registry import ModelRegistry

# Create custom orchestrator
control_system = MockControlSystem()
orchestrator = Orchestrator(control_system=control_system)

# Use custom model registry
registry = ModelRegistry()
# ... configure models

# Compile with custom settings
pipeline = orchestrator.compile(
    yaml_content,
    config={""timeout"": 3600}
)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","159","168","python","The Orchestrator framework uses comprehensive type annotations for better IDE support and type checking:","from typing import Dict, Any, List, Optional
from orchestrator import Pipeline, Task

def process_pipeline(
    pipeline: Pipeline,
    inputs: Dict[str, Any],
    timeout: Optional[int] = None
) -> Dict[str, Any]:
    return pipeline.run(**inputs)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","198","215","yaml","Default configuration can be overridden using a config file at ``~/.orchestrator/config.yaml``:","models:
  default: ""ollama:gemma2:27b""
  fallback: ""ollama:llama3.2:1b""
  timeout: 300

tools:
  mcp_port: 8000
  auto_start: true

execution:
  parallel: true
  checkpoint: true
  timeout: 3600

logging:
  level: ""INFO""
  format: ""%(asctime)s - %(name)s - %(levelname)s - %(message)s""","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","226","235","python","Models are loaded lazily and cached. For better performance:","# Initialize models once at startup
orc.init_models()

# Reuse compiled pipelines
pipeline = orc.compile(""pipeline.yaml"")

# Multiple executions reuse the same pipeline
for inputs in input_batches:
    result = pipeline.run(**inputs)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","243","253","python","Large pipelines and datasets can consume significant memory:","# Enable checkpointing for long pipelines
pipeline = orc.compile(""pipeline.yaml"", config={
    ""checkpoint"": True,
    ""memory_limit"": ""8GB""
})

# Process data in batches
for batch in data_batches:
    result = pipeline.run(data=batch)
    # Results are automatically checkpointed","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","261","273","python","The framework provides structured error handling:","from orchestrator import CompilationError, ExecutionError

try:
    pipeline = orc.compile(""pipeline.yaml"")
    result = pipeline.run(input=""value"")
except CompilationError as e:
    print(f""Pipeline compilation failed: {e}"")
    print(f""Error details: {e.details}"")
except ExecutionError as e:
    print(f""Pipeline execution failed: {e}"")
    print(f""Failed step: {e.step_id}"")
    print(f""Error context: {e.context}"")","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","281","291","python","Enable detailed logging for debugging:","import logging

# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# Compile with debug information
pipeline = orc.compile(""pipeline.yaml"", debug=True)

# Execute with verbose output
result = pipeline.run(input=""value"", _verbose=True)","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","302","308","python","----------------------","from orchestrator.core.control_system import ControlSystem

class MyControlSystem(ControlSystem):
    async def execute_task(self, task: Task, context: dict) -> dict:
        # Custom execution logic
        pass","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","314","323","python","------------","from orchestrator.tools.base import Tool

class MyTool(Tool):
    def __init__(self):
        super().__init__(""my-tool"", ""Description"")

    async def execute(self, **kwargs) -> dict:
        # Tool implementation
        pass","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","329","335","python","-------------","from orchestrator.core.model import Model

class MyModel(Model):
    async def generate(self, prompt: str, **kwargs) -> str:
        # Model implementation
        pass","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","343","356","python","The framework is designed to be thread-safe:","import concurrent.futures

# Safe to use across threads
pipeline = orc.compile(""pipeline.yaml"")

def process_input(input_data):
    return pipeline.run(**input_data)

# Parallel execution
with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
    futures = [executor.submit(process_input, data)
              for data in input_datasets]
    results = [f.result() for f in futures]","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/api/index.rst","364","381","python","Testing utilities and patterns:","from orchestrator.testing import MockModel, TestRunner

def test_my_pipeline():
    # Use mock model for testing
    with MockModel() as mock:
        mock.set_response(""test response"")

        pipeline = orc.compile(""test_pipeline.yaml"")
        result = pipeline.run(input=""test"")

        assert result == ""expected""

# Test runner for pipeline validation
runner = TestRunner(""pipelines/"")
runner.validate_all()  # Validates all YAML files
runner.test_compilation()  # Tests compilation
runner.run_smoke_tests()  # Basic execution tests","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/concepts.rst","19","31","yaml","One of Orchestrator's core innovations is **input-agnostic pipelines**. This means a single pipeline definition can work with different inputs to produce different outputs:","# One pipeline definition
name: research-pipeline

inputs:
  topic: { type: string, required: true }
  depth: { type: string, default: ""medium"" }

steps:
  - id: research
    action: search_web
    parameters:
      query: ""{{ inputs.topic }}""","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/concepts.rst","55","62","yaml","Every task has these key components:","- id: unique_identifier        # Required: Unique name
  action: what_to_do           # Required: Action to perform
  description: ""What it does""  # Optional: Human description
  parameters:                  # Optional: Input parameters
    key: value
  depends_on: [other_task]     # Optional: Dependencies
  condition: ""when_to_run""     # Optional: Conditional execution","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/concepts.rst","70","87","yaml","Tasks can depend on other tasks, creating execution ordering:","steps:
  - id: fetch_data
    action: download_file
    parameters:
      url: ""{{ inputs.data_url }}""

  - id: process_data
    depends_on: [fetch_data]   # Runs after fetch_data
    action: transform_data
    parameters:
      data: ""$results.fetch_data""

  - id: save_results
    depends_on: [process_data] # Runs after process_data
    action: write_file
    parameters:
      content: ""$results.process_data""","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/concepts.rst","98","109","yaml","---------------","# Access input values
query: ""{{ inputs.search_term }}""

# Reference results from other tasks
data: ""$results.previous_task""

# Use filters and functions
filename: ""{{ inputs.name | slugify }}.pdf""

# Conditional expressions
mode: ""{{ 'advanced' if inputs.premium else 'basic' }}""","test_created","tests/snippet_tests/test_snippets_batch_7.py",""
"docs_sphinx/concepts.rst","120","129","yaml","- **Runtime**: Dynamic values resolved during execution","steps:
  - id: example
    parameters:
      # Compile-time: resolved once during compilation
      timestamp: ""{{ compile_time.now }}""

      # Runtime: resolved during each execution
      user_input: ""{{ inputs.query }}""
      previous_result: ""$results.other_task""","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","137","153","yaml","**AUTO tags** are Orchestrator's solution to ambiguous or uncertain values. When you're not sure what value to use, let an AI model decide:","parameters:
  # Simple AUTO tag
  style: <AUTO>Choose appropriate writing style</AUTO>

  # Context-aware AUTO tag
  method: <AUTO>Based on data type {{ results.data.type }}, choose best analysis method</AUTO>

  # Complex AUTO tag with instructions
  sections: |
    <AUTO>
    For a report about {{ inputs.topic }}, determine which sections to include:
    - Executive Summary: yes/no
    - Technical Details: yes/no
    - Future Outlook: yes/no
    Return as JSON object
    </AUTO>","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","197","215","yaml","Actions are how you invoke tools in pipelines:","# Web search
- action: search_web
  parameters:
    query: ""machine learning""

# File operations
- action: write_file
  parameters:
    path: ""output.txt""
    content: ""Hello world""

# Shell commands (prefix with !)
- action: ""!ls -la""

# AI generation
- action: generate_content
  parameters:
    prompt: ""Write a summary about {{ topic }}""","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","223","227","yaml","Orchestrator automatically detects required tools from your pipeline:","steps:
  - action: search_web        # → Requires web tool
  - action: ""!python script.py""  # → Requires terminal tool
  - action: write_file        # → Requires filesystem tool","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","265","271","python","- **Cost considerations** (API costs, efficiency)","# Models are selected automatically
registry = orc.init_models()

# Available models are ranked by capability
print(registry.list_models())
# ['ollama:gemma2:27b', 'ollama:llama3.2:1b', 'huggingface:gpt2']","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","284","291","yaml","Orchestrator can save pipeline state at task boundaries:","config:
  checkpoint: true  # Enable automatic checkpointing

steps:
  - id: expensive_task
    action: long_running_process
    checkpoint: true  # Force checkpoint after this step","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","299","304","python","If a pipeline fails, it can resume from the last checkpoint:","# Pipeline fails at step 5
pipeline.run(inputs)  # Fails

# Resume from last checkpoint
pipeline.resume()  # Continues from step 4","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","335","341","python","You can create custom control systems for specific needs:","from orchestrator.core.control_system import ControlSystem

class MyControlSystem(ControlSystem):
    async def execute_task(self, task, context):
        # Custom execution logic
        pass","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","352","366","yaml","---------------","imports:
  - common/validation.yaml as validator
  - workflows/analysis.yaml as analyzer

steps:
  - id: validate
    pipeline: validator
    inputs:
      data: ""{{ inputs.raw_data }}""

  - id: analyze
    pipeline: analyzer
    inputs:
      validated_data: ""$results.validate""","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","386","401","yaml","---------------","steps:
  - id: risky_task
    action: external_api_call
    error_handling:
      # Retry with backoff
      retry:
        max_attempts: 3
        backoff: exponential

      # Fallback action
      fallback:
        action: use_cached_data

      # Continue pipeline on error
      continue_on_error: true","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","422","434","yaml","Tasks without dependencies can run in parallel:","steps:
  # These run in parallel
  - id: source1
    action: fetch_data_a

  - id: source2
    action: fetch_data_b

  # This waits for both
  - id: combine
    depends_on: [source1, source2]
    action: merge_data","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","442","449","yaml","Expensive operations can be cached:","steps:
  - id: expensive_computation
    action: complex_analysis
    cache:
      enabled: true
      key: ""{{ inputs.data_hash }}""
      ttl: 3600  # 1 hour","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","457","462","yaml","Control resource usage:","config:
  resources:
    max_memory: ""8GB""
    max_threads: 4
    gpu_enabled: false","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","483","494","yaml","All inputs are validated:","inputs:
  email:
    type: string
    validation:
      pattern: ""^[\\w.-]+@[\\w.-]+\\.\\w+$""

  amount:
    type: number
    validation:
      min: 0
      max: 10000","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","502","505","yaml","Sensitive data is handled securely:","parameters:
  api_key: ""{{ env.SECRET_API_KEY }}""  # From environment
  password: ""{{ vault.db_password }}""   # From secret vault","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/concepts.rst","523","533","","--------------------","pipelines/
├── common/           # Shared components
│   ├── validation.yaml
│   └── formatting.yaml
├── workflows/        # Complete workflows
│   ├── research.yaml
│   └── analysis.yaml
└── specialized/      # Domain-specific
    ├── finance.yaml
    └── healthcare.yaml","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","29","43","yaml","A pipeline is a collection of tasks that work together to achieve a goal. Pipelines are defined in YAML and can include:","name: research-report
description: Generate comprehensive research reports

inputs:
  topic:
    type: string
    description: Research topic
    required: true

steps:
  - id: search
    action: search_web
    parameters:
      query: ""{{ inputs.topic }} latest research""","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","61","64","yaml","When you're unsure about a value, use ``<AUTO>`` tags to let AI models decide:","parameters:
  method: <AUTO>Choose best analysis method for this data</AUTO>
  depth: <AUTO>Determine appropriate depth level</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","83","108","yaml","1. **Create a pipeline definition** (``research.yaml``):","name: quick-research
description: Quick research on any topic

inputs:
  topic:
    type: string
    required: true

outputs:
  report:
    type: string
    value: ""{{ inputs.topic }}_report.md""

steps:
  - id: search
    action: search_web
    parameters:
      query: ""{{ inputs.topic }}""
      max_results: 5

  - id: summarize
    action: generate_summary
    parameters:
      content: ""$results.search""
      style: <AUTO>Choose appropriate summary style</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","113","126","python","2. **Run the pipeline**:","import orchestrator as orc

# Initialize models
orc.init_models()

# Compile the pipeline
pipeline = orc.compile(""research.yaml"")

# Execute with different topics
result1 = pipeline.run(topic=""artificial intelligence"")
result2 = pipeline.run(topic=""climate change"")

print(f""Reports generated: {result1}, {result2}"")","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","141","148","python","The same pipeline works with different inputs:","# One pipeline, many uses
pipeline = orc.compile(""report-template.yaml"")

# Generate different reports
ai_report = pipeline.run(topic=""AI"", style=""technical"")
bio_report = pipeline.run(topic=""Biology"", style=""educational"")
eco_report = pipeline.run(topic=""Economics"", style=""executive"")","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","153","160","yaml","eco_report = pipeline.run(topic=""Economics"", style=""executive"")","inputs:
  topic:
    type: string
    required: true
  style:
    type: string
    default: ""technical""","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","168","177","yaml","Tools are automatically detected and made available:","steps:
  - id: fetch_data
    action: search_web        # Auto-detects web tool

  - id: save_results
    action: write_file        # Auto-detects filesystem tool

  - id: run_analysis
    action: ""!python analyze.py""  # Auto-detects terminal tool","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/getting_started.rst","185","193","python","The framework intelligently selects the best model for each task:","# Models are selected based on:
# - Task requirements (reasoning, coding, etc.)
# - Available resources
# - Performance history

registry = orc.init_models()
print(registry.list_models())
# Output: ['ollama:gemma2:27b', 'ollama:llama3.2:1b', 'huggingface:gpt2']","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/index.rst","43","56","python","-------------","import orchestrator as orc

# Initialize models
orc.init_models()

# Compile a pipeline
pipeline = orc.compile(""pipelines/research-report.yaml"")

# Execute with different inputs
result = pipeline.run(
    topic=""quantum_computing"",
    instructions=""Focus on error correction""
)","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/installation.rst","34","41","bash","-----------------------","# Install from PyPI (when available)
pip install py-orc

# Or install from source
git clone https://github.com/ContextLab/orchestrator.git
cd orchestrator
pip install -e .","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/installation.rst","47","53","bash","-----------","# Create conda environment
conda create -n py-orc python=3.11
conda activate py-orc

# Install orchestrator
pip install py-orc","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/installation.rst","59","64","bash","------------","# Pull the official image
docker pull contextlab/py-orc:latest

# Run with volume mount
docker run -v $(pwd):/workspace contextlab/py-orc","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/installation.rst","72","85","bash","For contributors and developers:","# Clone the repository
git clone https://github.com/ContextLab/orchestrator.git
cd orchestrator

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode with extras
pip install -e "".[dev,test,docs]""

# Install pre-commit hooks
pre-commit install","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/installation.rst","96","101","bash","1. **Install Ollama**:","# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh","test_created","tests/snippet_tests/test_snippets_batch_8.py",""
"docs_sphinx/installation.rst","106","114","bash","2. **Pull recommended models**:","# Large model for complex tasks
ollama pull gemma2:27b

# Small model for simple tasks
ollama pull llama3.2:1b

# Code-focused model
ollama pull codellama:7b","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","119","124","python","3. **Verify installation**:","import orchestrator as orc

# Initialize and check models
registry = orc.init_models()
print(registry.list_models())","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","132","137","bash","For HuggingFace models, set up your token:","# Set environment variable
export HUGGINGFACE_TOKEN=""your-token-here""

# Or create .env file
echo ""HUGGINGFACE_TOKEN=your-token-here"" > .env","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","145","150","bash","For cloud models, configure API keys:","# OpenAI
export OPENAI_API_KEY=""sk-...""

# Anthropic
export ANTHROPIC_API_KEY=""sk-ant-...""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","161","168","bash","For headless browser functionality:","# Install Playwright
pip install playwright
playwright install chromium

# Or use Selenium
pip install selenium
# Download appropriate driver","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","181","186","bash","Install optional data processing libraries:","# For advanced data processing
pip install pandas numpy scipy

# For data validation
pip install pydantic jsonschema","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","194","214","yaml","Create a configuration file at ``~/.orchestrator/config.yaml``:","# Model preferences
models:
  default: ""ollama:gemma2:27b""
  fallback: ""ollama:llama3.2:1b""

# Resource limits
resources:
  max_memory: ""16GB""
  max_threads: 8
  gpu_enabled: true

# Tool settings
tools:
  mcp_port: 8000
  sandbox_enabled: true

# State management
state:
  backend: ""postgresql""
  connection: ""postgresql://user:pass@localhost/orchestrator""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","222","233","bash","Set these environment variables for additional configuration:","# Core settings
export ORCHESTRATOR_HOME=""$HOME/.orchestrator""
export ORCHESTRATOR_LOG_LEVEL=""INFO""

# Model settings
export ORCHESTRATOR_MODEL_TIMEOUT=""300""
export ORCHESTRATOR_MODEL_RETRIES=""3""

# Tool settings
export ORCHESTRATOR_TOOL_TIMEOUT=""60""
export ORCHESTRATOR_MCP_AUTO_START=""true""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","241","266","python","Run the verification script:","import orchestrator as orc

# Check version
print(f""Orchestrator version: {orc.__version__}"")

# Check models
try:
    registry = orc.init_models()
    models = registry.list_models()
    print(f""Available models: {models}"")
except Exception as e:
    print(f""Model initialization failed: {e}"")

# Check tools
from orchestrator.tools.base import default_registry
tools = default_registry.list_tools()
print(f""Available tools: {tools}"")

# Run test pipeline
try:
    pipeline = orc.compile(""examples/hello-world.yaml"")
    result = pipeline.run(message=""Hello, Orchestrator!"")
    print(f""Test pipeline result: {result}"")
except Exception as e:
    print(f""Pipeline test failed: {e}"")","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","277","278","text","**Import Error**:","ModuleNotFoundError: No module named 'orchestrator'","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","285","286","text","**Model Connection Error**:","Failed to connect to Ollama at http://localhost:11434","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","293","294","text","**Permission Error**:","Permission denied: '/home/user/.orchestrator'","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/installation.rst","299","301","bash","Solution: Create directory with proper permissions:","mkdir -p ~/.orchestrator
chmod 755 ~/.orchestrator","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","19","59","yaml","Create a file called ``summarize.yaml``:","name: topic-summarizer
description: Generate a concise summary of any topic

inputs:
  topic:
    type: string
    description: The topic to summarize
    required: true

  length:
    type: integer
    description: Approximate word count for the summary
    default: 200

outputs:
  summary:
    type: string
    value: ""{{ inputs.topic }}_summary.txt""

steps:
  - id: research
    action: generate_content
    parameters:
      prompt: |
        Research and provide key information about: {{ inputs.topic }}
        Focus on the most important and interesting aspects.
      max_length: 500

  - id: summarize
    action: generate_summary
    parameters:
      content: ""$results.research""
      target_length: ""{{ inputs.length }}""
      style: <AUTO>Choose appropriate style for the topic</AUTO>

  - id: save_summary
    action: write_file
    parameters:
      path: ""{{ outputs.summary }}""
      content: ""$results.summarize""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","67","87","python","Create a Python script to run your pipeline:","import orchestrator as orc

# Initialize the model pool
orc.init_models()

# Compile the pipeline
pipeline = orc.compile(""summarize.yaml"")

# Run with different topics
result1 = pipeline.run(
    topic=""quantum computing"",
    length=150
)

result2 = pipeline.run(
    topic=""sustainable energy"",
    length=250
)

print(f""Created summaries: {result1}, {result2}"")","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","107","179","yaml","Let's create a more sophisticated pipeline that generates research reports:","name: research-report-generator
description: Generate comprehensive research reports with citations

inputs:
  topic:
    type: string
    required: true
  focus_areas:
    type: array
    description: Specific areas to focus on
    default: []

outputs:
  report_pdf:
    type: string
    value: ""reports/{{ inputs.topic }}_report.pdf""

steps:
  # Web search for recent information
  - id: search_recent
    action: search_web
    parameters:
      query: ""{{ inputs.topic }} 2024 latest developments""
      max_results: 10

  # Search academic sources
  - id: search_academic
    action: search_web
    parameters:
      query: ""{{ inputs.topic }} research papers scholarly""
      max_results: 5

  # Compile all sources
  - id: compile_sources
    action: compile_markdown
    parameters:
      sources:
        - ""$results.search_recent""
        - ""$results.search_academic""
      include_citations: true

  # Generate the report
  - id: write_report
    action: generate_report
    parameters:
      research: ""$results.compile_sources""
      topic: ""{{ inputs.topic }}""
      focus_areas: ""{{ inputs.focus_areas }}""
      style: ""academic""
      sections:
        - ""Executive Summary""
        - ""Introduction""
        - ""Current State""
        - ""Recent Developments""
        - ""Future Outlook""
        - ""Conclusions""

  # Quality check
  - id: validate
    action: validate_report
    parameters:
      report: ""$results.write_report""
      checks:
        - completeness
        - citation_accuracy
        - readability

  # Generate PDF
  - id: create_pdf
    action: ""!pandoc -o {{ outputs.report_pdf }} --pdf-engine=xelatex""
    parameters:
      input: ""$results.write_report""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","192","201","yaml","**Web Tools**:","# Web search
- action: search_web
  parameters:
    query: ""your search query""

# Scrape webpage
- action: scrape_page
  parameters:
    url: ""https://example.com""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","206","218","yaml","**System Tools**:","# Run shell commands (prefix with !)
- action: ""!ls -la""

# File operations
- action: read_file
  parameters:
    path: ""data.txt""

- action: write_file
  parameters:
    path: ""output.txt""
    content: ""Your content""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","223","238","yaml","**Data Tools**:","# Process data
- action: transform_data
  parameters:
    input: ""$results.previous_step""
    operations:
      - type: filter
        condition: ""value > 100""

# Validate data
- action: validate_data
  parameters:
    data: ""$results.data""
    schema:
      type: object
      required: [""name"", ""value""]","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","246","254","yaml","AUTO tags let AI models make intelligent decisions:","steps:
  - id: analyze
    action: analyze_data
    parameters:
      data: ""$results.fetch""
      method: <AUTO>Choose best analysis method based on data type</AUTO>
      visualization: <AUTO>Determine if visualization would be helpful</AUTO>
      depth: <AUTO>Set analysis depth (shallow/medium/deep)</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","264","288","yaml","You can compose pipelines from smaller, reusable components:","name: composite-pipeline

imports:
  - common/data_fetcher.yaml as fetcher
  - common/validator.yaml as validator

steps:
  # Use imported pipeline
  - id: fetch_data
    pipeline: fetcher
    parameters:
      source: ""api""

  # Local step
  - id: process
    action: process_data
    parameters:
      data: ""$results.fetch_data""

  # Use another import
  - id: validate
    pipeline: validator
    parameters:
      data: ""$results.process""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","296","309","yaml","Add error handling to make pipelines robust:","steps:
  - id: risky_operation
    action: fetch_external_data
    parameters:
      url: ""{{ inputs.data_source }}""
    error_handling:
      retry:
        max_attempts: 3
        backoff: exponential
      fallback:
        action: use_cached_data
        parameters:
          cache_key: ""{{ inputs.topic }}""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/quickstart.rst","317","332","python","Enable debug mode for detailed execution logs:","import logging
import orchestrator as orc

# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# Compile with debug flag
pipeline = orc.compile(""pipeline.yaml"", debug=True)

# Run with verbose output
result = pipeline.run(
    topic=""test"",
    _verbose=True,
    _step_callback=lambda step: print(f""Executing: {step.id}"")
)","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","48","100","yaml","**Parameters**:","# Web search
- id: search
  action: search_web
  parameters:
    query: ""orchestrator framework tutorial""    # Required: Search query
    max_results: 10                            # Optional: Number of results (default: 10)
    search_engine: ""google""                    # Optional: google|bing|duckduckgo (default: google)
    include_snippets: true                     # Optional: Include text snippets (default: true)
    region: ""us""                              # Optional: Region code (default: us)
    language: ""en""                            # Optional: Language code (default: en)
    safe_search: ""moderate""                   # Optional: off|moderate|strict (default: moderate)

# Scrape webpage
- id: scrape
  action: scrape_page
  parameters:
    url: ""https://example.com/article""        # Required: URL to scrape
    selectors:                                # Optional: CSS selectors to extract
      title: ""h1.main-title""
      content: ""div.article-body""
      author: ""span.author-name""
    wait_for: ""div.content-loaded""            # Optional: Wait for element
    timeout: 30                               # Optional: Timeout in seconds (default: 30)
    javascript: true                          # Optional: Execute JavaScript (default: true)
    clean_html: true                          # Optional: Clean extracted HTML (default: true)

# Take screenshot
- id: screenshot
  action: screenshot_page
  parameters:
    url: ""https://example.com""                # Required: URL to screenshot
    full_page: true                           # Optional: Capture full page (default: false)
    width: 1920                               # Optional: Viewport width (default: 1920)
    height: 1080                              # Optional: Viewport height (default: 1080)
    wait_for: ""img""                           # Optional: Wait for element
    output_path: ""screenshots/page.png""       # Optional: Save path

# Interact with page
- id: interact
  action: interact_with_page
  parameters:
    url: ""https://example.com/form""           # Required: URL to interact with
    actions:                                  # Required: List of interactions
      - type: ""fill""
        selector: ""#username""
        value: ""testuser""
      - type: ""click""
        selector: ""#submit-button""
      - type: ""wait""
        duration: 2000
      - type: ""extract""
        selector: "".result""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","105","138","yaml","**Example Pipeline**:","name: web-research-pipeline
description: Comprehensive web research with validation

steps:
  # Search for information
  - id: search_topic
    action: search_web
    parameters:
      query: ""{{ inputs.topic }} latest news 2024""
      max_results: 20
      search_engine: ""google""

  # Scrape top results
  - id: scrape_articles
    for_each: ""{{ results.search_topic.results[:5] }}""
    as: result
    action: scrape_page
    parameters:
      url: ""{{ result.url }}""
      selectors:
        title: ""h1, h2.article-title""
        content: ""main, article, div.content""
        date: ""time, .date, .published""
      clean_html: true

  # Take screenshots for reference
  - id: capture_pages
    for_each: ""{{ results.search_topic.results[:3] }}""
    as: result
    action: screenshot_page
    parameters:
      url: ""{{ result.url }}""
      output_path: ""research/{{ inputs.topic }}/{{ loop.index }}.png""","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","154","189","yaml","**Parameters**:","# Quick search
- id: search
  action: quick_search
  parameters:
    query: ""machine learning basics""          # Required: Search query
    max_results: 5                           # Optional: Result count (default: 10)
    format: ""json""                           # Optional: json|text (default: json)

# News search
- id: news
  action: search_news
  parameters:
    query: ""AI breakthroughs""                # Required: Search query
    date_range: ""last_week""                  # Optional: last_day|last_week|last_month|last_year
    sources: [""reuters"", ""techcrunch""]       # Optional: Preferred sources
    sort_by: ""relevance""                     # Optional: relevance|date (default: relevance)

# Academic search
- id: academic
  action: search_academic
  parameters:
    query: ""quantum computing""               # Required: Search query
    databases: [""arxiv"", ""pubmed""]          # Optional: Databases to search
    year_range: ""2020-2024""                 # Optional: Year range
    peer_reviewed: true                      # Optional: Only peer-reviewed (default: false)

# Image search
- id: images
  action: search_images
  parameters:
    query: ""data visualization examples""     # Required: Search query
    max_results: 10                         # Optional: Number of images
    size: ""large""                           # Optional: small|medium|large|any
    type: ""photo""                           # Optional: photo|clipart|lineart|any
    license: ""creative_commons""             # Optional: License filter","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","207","233","yaml","**Parameters**:","# Direct command execution
- id: list_files
  action: ""!ls -la /data""

# Command with parameters
- id: run_command
  action: execute_command
  parameters:
    command: ""python analyze.py""              # Required: Command to execute
    arguments: [""--input"", ""data.csv""]       # Optional: Command arguments
    working_dir: ""/project""                  # Optional: Working directory
    environment:                             # Optional: Environment variables
      PYTHONPATH: ""/project/lib""
      DEBUG: ""true""
    timeout: 300                             # Optional: Timeout in seconds (default: 60)
    capture_output: true                     # Optional: Capture stdout/stderr (default: true)
    shell: true                              # Optional: Use shell execution (default: true)

# Run script file
- id: run_analysis
  action: run_script
  parameters:
    script_path: ""scripts/analyze.sh""        # Required: Path to script
    arguments: [""{{ inputs.data_file }}""]    # Optional: Script arguments
    interpreter: ""bash""                      # Optional: bash|python|node (default: auto-detect)
    working_dir: ""{{ execution.temp_dir }}""  # Optional: Working directory","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","238","282","yaml","**Example Pipeline**:","name: data-processing-automation
description: Automated data processing with shell commands

steps:
  # Setup environment
  - id: setup
    action: ""!mkdir -p output/{{ inputs.project_name }}""

  # Download data
  - id: download
    action: execute_command
    parameters:
      command: ""wget""
      arguments:
        - ""-O""
        - ""data/raw_data.csv""
        - ""{{ inputs.data_url }}""
      timeout: 600

  # Process with Python
  - id: process
    action: execute_command
    parameters:
      command: ""python""
      arguments:
        - ""scripts/process_data.py""
        - ""--input""
        - ""data/raw_data.csv""
        - ""--output""
        - ""output/{{ inputs.project_name }}/processed.csv""
      environment:
        DATA_QUALITY: ""high""
        PROCESSING_MODE: ""{{ inputs.mode }}""

  # Generate report with R
  - id: report
    action: ""!Rscript reports/generate_report.R output/{{ inputs.project_name }}/processed.csv""

  # Package results
  - id: package
    action: execute_command
    parameters:
      command: ""tar""
      arguments: [""-czf"", ""{{ outputs.package }}"", ""output/{{ inputs.project_name }}""]","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","302","367","yaml","**Parameters**:","# Read file
- id: read_config
  action: read_file
  parameters:
    path: ""config/settings.json""             # Required: File path
    encoding: ""utf-8""                        # Optional: File encoding (default: utf-8)
    parse: true                              # Optional: Parse JSON/YAML (default: false)

# Write file
- id: save_results
  action: write_file
  parameters:
    path: ""output/results.json""              # Required: File path
    content: ""{{ results.analysis | json }}"" # Required: Content to write
    encoding: ""utf-8""                        # Optional: File encoding (default: utf-8)
    create_dirs: true                        # Optional: Create parent dirs (default: true)
    overwrite: true                          # Optional: Overwrite existing (default: false)

# Copy file
- id: backup
  action: copy_file
  parameters:
    source: ""data/important.db""              # Required: Source path
    destination: ""backup/important_{{ execution.timestamp }}.db""  # Required: Destination
    overwrite: false                         # Optional: Overwrite existing (default: false)

# Move file
- id: archive
  action: move_file
  parameters:
    source: ""temp/processed.csv""             # Required: Source path
    destination: ""archive/2024/processed.csv"" # Required: Destination
    create_dirs: true                        # Optional: Create parent dirs (default: true)

# Delete file
- id: cleanup
  action: delete_file
  parameters:
    path: ""temp/*""                           # Required: Path or pattern
    recursive: true                          # Optional: Delete recursively (default: false)
    force: false                             # Optional: Force deletion (default: false)

# List directory
- id: scan_files
  action: list_directory
  parameters:
    path: ""data/""                            # Required: Directory path
    pattern: ""*.csv""                         # Optional: File pattern
    recursive: true                          # Optional: Search subdirs (default: false)
    include_hidden: false                    # Optional: Include hidden files (default: false)
    details: true                            # Optional: Include file details (default: false)

# Create directory
- id: setup_dirs
  action: create_directory
  parameters:
    path: ""output/{{ inputs.project }}/data"" # Required: Directory path
    parents: true                            # Optional: Create parents (default: true)
    exist_ok: true                           # Optional: Ok if exists (default: true)

# Check existence
- id: check_file
  action: file_exists
  parameters:
    path: ""config/custom.yaml""               # Required: Path to check","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","372","416","yaml","**Example Pipeline**:","name: file-organization-pipeline
description: Organize and process files automatically

steps:
  # Check for existing data
  - id: check_existing
    action: file_exists
    parameters:
      path: ""data/current_dataset.csv""

  # Backup if exists
  - id: backup
    condition: ""{{ results.check_existing }}""
    action: copy_file
    parameters:
      source: ""data/current_dataset.csv""
      destination: ""backups/dataset_{{ execution.timestamp }}.csv""

  # Read configuration
  - id: read_config
    action: read_file
    parameters:
      path: ""config/processing.yaml""
      parse: true

  # Process files based on config
  - id: process_files
    for_each: ""{{ results.read_config.file_patterns }}""
    as: pattern
    action: list_directory
    parameters:
      path: ""{{ pattern.directory }}""
      pattern: ""{{ pattern.glob }}""
      recursive: true

  # Organize by type
  - id: organize
    for_each: ""{{ results.process_files | flatten }}""
    as: file
    action: move_file
    parameters:
      source: ""{{ file.path }}""
      destination: ""organized/{{ file.extension }}/{{ file.name }}""
      create_dirs: true","test_created","tests/snippet_tests/test_snippets_batch_9.py",""
"docs_sphinx/tool_reference.rst","436","507","yaml","**Parameters**:","# Transform data
- id: transform
  action: transform_data
  parameters:
    data: ""$results.load_data""               # Required: Input data or path
    operations:                              # Required: List of operations
      - type: ""rename_columns""
        mapping:
          old_name: ""new_name""
          price: ""cost""
      - type: ""add_column""
        name: ""total""
        expression: ""quantity * cost""
      - type: ""drop_columns""
        columns: [""unnecessary_field""]
      - type: ""convert_types""
        conversions:
          date: ""datetime""
          amount: ""float""

# Filter data
- id: filter
  action: filter_data
  parameters:
    data: ""$results.transform""               # Required: Input data
    conditions:                              # Required: Filter conditions
      - field: ""status""
        operator: ""equals""                   # equals|not_equals|contains|gt|lt|gte|lte
        value: ""active""
      - field: ""amount""
        operator: ""gt""
        value: 1000
    mode: ""and""                              # Optional: and|or (default: and)

# Aggregate data
- id: aggregate
  action: aggregate_data
  parameters:
    data: ""$results.filter""                  # Required: Input data
    group_by: [""category"", ""region""]        # Optional: Grouping columns
    aggregations:                            # Required: Aggregation rules
      total_amount:
        column: ""amount""
        function: ""sum""                      # sum|mean|median|min|max|count|std
      average_price:
        column: ""price""
        function: ""mean""
      item_count:
        column: ""*""
        function: ""count""

# Merge data
- id: merge
  action: merge_data
  parameters:
    left: ""$results.main_data""               # Required: Left dataset
    right: ""$results.lookup_data""            # Required: Right dataset
    on: ""customer_id""                        # Required: Join column(s)
    how: ""left""                              # Optional: left|right|inner|outer (default: left)
    suffixes: [""_main"", ""_lookup""]          # Optional: Column suffixes

# Convert format
- id: convert
  action: convert_format
  parameters:
    data: ""$results.final_data""              # Required: Input data
    from_format: ""json""                      # Optional: Auto-detect if not specified
    to_format: ""parquet""                     # Required: Target format
    options:                                 # Optional: Format-specific options
      compression: ""snappy""
      index: false","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","512","582","yaml","**Example Pipeline**:","name: sales-data-analysis
description: Process and analyze sales data

steps:
  # Load raw data
  - id: load_sales
    action: read_file
    parameters:
      path: ""data/sales_2024.csv""
      parse: true

  # Clean and transform
  - id: clean_data
    action: transform_data
    parameters:
      data: ""$results.load_sales""
      operations:
        - type: ""rename_columns""
          mapping:
            ""Sale Date"": ""sale_date""
            ""Customer Name"": ""customer_name""
            ""Product ID"": ""product_id""
            ""Sale Amount"": ""amount""
        - type: ""convert_types""
          conversions:
            sale_date: ""datetime""
            amount: ""float""
        - type: ""add_column""
          name: ""quarter""
          expression: ""sale_date.quarter""

  # Filter valid sales
  - id: filter_valid
    action: filter_data
    parameters:
      data: ""$results.clean_data""
      conditions:
        - field: ""amount""
          operator: ""gt""
          value: 0
        - field: ""product_id""
          operator: ""not_equals""
          value: null

  # Aggregate by quarter
  - id: quarterly_summary
    action: aggregate_data
    parameters:
      data: ""$results.filter_valid""
      group_by: [""quarter"", ""product_id""]
      aggregations:
        total_sales:
          column: ""amount""
          function: ""sum""
        avg_sale:
          column: ""amount""
          function: ""mean""
        num_transactions:
          column: ""*""
          function: ""count""

  # Save results
  - id: save_summary
    action: convert_format
    parameters:
      data: ""$results.quarterly_summary""
      to_format: ""excel""
      options:
        sheet_name: ""Quarterly Sales""
        index: false","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","598","692","yaml","**Parameters**:","# Validate against schema
- id: validate_structure
  action: validate_schema
  parameters:
    data: ""$results.processed_data""          # Required: Data to validate
    schema:                                  # Required: Validation schema
      type: ""object""
      required: [""id"", ""name"", ""email""]
      properties:
        id:
          type: ""integer""
          minimum: 1
        name:
          type: ""string""
          minLength: 2
          maxLength: 100
        email:
          type: ""string""
          format: ""email""
        age:
          type: ""integer""
          minimum: 0
          maximum: 150
    strict: false                            # Optional: Strict mode (default: false)

# Business rule validation
- id: validate_rules
  action: validate_data
  parameters:
    data: ""$results.transactions""            # Required: Data to validate
    rules:                                   # Required: Validation rules
      - name: ""positive_amounts""
        field: ""amount""
        condition: ""value > 0""
        severity: ""error""                    # error|warning|info
        message: ""Transaction amounts must be positive""

      - name: ""valid_date_range""
        field: ""transaction_date""
        condition: ""value >= '2024-01-01' and value <= today()""
        severity: ""error""

      - name: ""customer_exists""
        field: ""customer_id""
        condition: ""value in valid_customers""
        severity: ""warning""
        context:
          valid_customers: ""$results.customer_list""

    stop_on_error: false                     # Optional: Stop on first error (default: false)

# Data quality checks
- id: quality_check
  action: check_quality
  parameters:
    data: ""$results.dataset""                 # Required: Data to check
    checks:                                  # Required: Quality checks
      - type: ""completeness""
        threshold: 0.95                      # 95% non-null required
        columns: [""id"", ""name"", ""email""]

      - type: ""uniqueness""
        columns: [""id"", ""email""]

      - type: ""consistency""
        rules:
          - ""start_date <= end_date""
          - ""total == sum(line_items)""

      - type: ""accuracy""
        validations:
          email: ""regex:^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$""
          phone: ""regex:^\\+?1?\\d{9,15}$""

      - type: ""timeliness""
        field: ""last_updated""
        max_age_days: 30

# Report validation
- id: validate_report
  action: validate_report
  parameters:
    report: ""$results.generated_report""      # Required: Report to validate
    checks:                                  # Required: Report checks
      - ""completeness""                       # All sections present
      - ""accuracy""                           # Facts are accurate
      - ""consistency""                        # No contradictions
      - ""readability""                        # Appropriate reading level
      - ""citations""                          # Sources properly cited
    requirements:                            # Optional: Specific requirements
      min_words: 1000
      max_words: 5000
      required_sections: [""intro"", ""analysis"", ""conclusion""]
      citation_style: ""APA""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","697","784","yaml","**Example Pipeline**:","name: data-quality-pipeline
description: Comprehensive data validation and quality assurance

steps:
  # Load data
  - id: load
    action: read_file
    parameters:
      path: ""{{ inputs.data_file }}""
      parse: true

  # Schema validation
  - id: validate_schema
    action: validate_schema
    parameters:
      data: ""$results.load""
      schema:
        type: ""array""
        items:
          type: ""object""
          required: [""order_id"", ""customer_id"", ""amount"", ""date""]
          properties:
            order_id:
              type: ""string""
              pattern: ""^ORD-[0-9]{6}$""
            customer_id:
              type: ""integer""
              minimum: 1
            amount:
              type: ""number""
              minimum: 0
            date:
              type: ""string""
              format: ""date""

  # Business rules
  - id: validate_business
    action: validate_data
    parameters:
      data: ""$results.load""
      rules:
        - name: ""valid_amounts""
          field: ""amount""
          condition: ""value > 0 and value < 10000""
          severity: ""error""

        - name: ""recent_orders""
          field: ""date""
          condition: ""days_between(value, today()) <= 365""
          severity: ""warning""
          message: ""Order is older than 1 year""

  # Quality assessment
  - id: quality_report
    action: check_quality
    parameters:
      data: ""$results.load""
      checks:
        - type: ""completeness""
          threshold: 0.98
        - type: ""uniqueness""
          columns: [""order_id""]
        - type: ""consistency""
          rules:
            - ""item_total == quantity * unit_price""
        - type: ""accuracy""
          validations:
            email: ""regex:^[\\w.-]+@[\\w.-]+\\.\\w+$""

  # Generate validation report
  - id: create_report
    action: generate_content
    parameters:
      template: |
        # Data Validation Report

        ## Schema Validation
        {{ results.validate_schema.summary }}

        ## Business Rules
        {{ results.validate_business.summary }}

        ## Quality Metrics
        {{ results.quality_report | format_quality_metrics }}

        ## Recommendations
        <AUTO>Based on the validation results, provide recommendations</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","804","870","yaml","**Parameters**:","# Generate content
- id: generate
  action: generate_content
  parameters:
    prompt: ""{{ inputs.prompt }}""            # Required: Generation prompt
    model: <AUTO>Select best model</AUTO>    # Optional: Model selection
    max_tokens: 1000                         # Optional: Maximum tokens
    temperature: 0.7                         # Optional: Creativity (0-2)
    system_prompt: ""You are a helpful AI""    # Optional: System message
    format: ""markdown""                       # Optional: Output format
    style: ""professional""                    # Optional: Writing style

# Analyze text
- id: analyze
  action: analyze_text
  parameters:
    text: ""$results.document""                # Required: Text to analyze
    analysis_types:                          # Required: Types of analysis
      - sentiment                            # Positive/negative/neutral
      - entities                             # Named entities
      - topics                               # Main topics
      - summary                              # Brief summary
      - key_points                           # Bullet points
      - language                             # Detect language
    output_format: ""structured""              # Optional: structured|narrative

# Extract information
- id: extract
  action: extract_information
  parameters:
    content: ""$results.raw_text""             # Required: Source content
    extract:                                 # Required: What to extract
      dates:
        description: ""All mentioned dates""
        format: ""YYYY-MM-DD""
      people:
        description: ""Person names with roles""
        include_context: true
      organizations:
        description: ""Company and organization names""
      numbers:
        description: ""Numerical values with units""
        categories: [""financial"", ""metrics""]
    output_format: ""json""                    # Optional: json|table|text

# Generate code
- id: code_gen
  action: generate_code
  parameters:
    description: ""{{ inputs.feature_request }}"" # Required: What to build
    language: ""python""                       # Required: Programming language
    framework: ""fastapi""                     # Optional: Framework/library
    include_tests: true                      # Optional: Generate tests
    include_docs: true                       # Optional: Generate docs
    style_guide: ""PEP8""                     # Optional: Code style
    example_usage: true                      # Optional: Include examples

# Reasoning task
- id: reason
  action: reason_about
  parameters:
    question: ""{{ inputs.problem }}""         # Required: Problem/question
    context: ""$results.research""             # Optional: Additional context
    approach: ""step_by_step""                 # Optional: Reasoning approach
    show_work: true                          # Optional: Show reasoning
    confidence_level: true                   # Optional: Include confidence","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","889","898","yaml","**Parameters**:","# Query database
- id: fetch_data
  action: query_database
  parameters:
    connection: ""postgresql://localhost/mydb"" # Required: Connection string
    query: ""SELECT * FROM users WHERE active = true"" # Required: SQL query
    parameters: []                           # Optional: Query parameters
    fetch_size: 1000                         # Optional: Batch size
    timeout: 30                              # Optional: Query timeout","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","913","927","yaml","**Parameters**:","# REST API call
- id: api_call
  action: call_api
  parameters:
    url: ""https://api.example.com/data""     # Required: API endpoint
    method: ""POST""                           # Required: HTTP method
    headers:                                 # Optional: Headers
      Authorization: ""Bearer {{ env.API_TOKEN }}""
      Content-Type: ""application/json""
    body:                                    # Optional: Request body
      query: ""{{ inputs.search_term }}""
      limit: 100
    timeout: 60                              # Optional: Request timeout
    retry: 3                                 # Optional: Retry attempts","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","936","1002","yaml","----------------------------","name: comprehensive-research-tool-chain
description: Chain multiple tools for research and reporting

steps:
  # 1. Search multiple sources
  - id: web_search
    action: search_web
    parameters:
      query: ""{{ inputs.topic }} latest research 2024""
      max_results: 20

  # 2. Scrape promising articles
  - id: scrape_articles
    for_each: ""{{ results.web_search.results[:5] }}""
    as: article
    action: scrape_page
    parameters:
      url: ""{{ article.url }}""
      selectors:
        content: ""article, main, .content""

  # 3. Extract key information
  - id: extract_facts
    action: extract_information
    parameters:
      content: ""$results.scrape_articles""
      extract:
        facts:
          description: ""Key facts and findings""
        statistics:
          description: ""Numerical data with context""
        quotes:
          description: ""Notable quotes with attribution""

  # 4. Validate information
  - id: cross_validate
    action: validate_data
    parameters:
      data: ""$results.extract_facts""
      rules:
        - name: ""source_diversity""
          condition: ""count(unique(sources)) >= 3""
          severity: ""warning""

  # 5. Generate report
  - id: create_report
    action: generate_content
    parameters:
      prompt: |
        Create a comprehensive report about {{ inputs.topic }}
        using the following validated information:
        {{ results.extract_facts | json }}
      style: ""academic""
      format: ""markdown""
      max_tokens: 2000

  # 6. Save report
  - id: save_report
    action: write_file
    parameters:
      path: ""reports/{{ inputs.topic }}_{{ execution.date }}.md""
      content: ""$results.create_report""

  # 7. Generate PDF
  - id: create_pdf
    action: ""!pandoc -f markdown -t pdf -o reports/{{ inputs.topic }}.pdf reports/{{ inputs.topic }}_{{ execution.date }}.md""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","1008","1099","yaml","------------------------","name: etl-tool-chain
description: Extract, transform, and load data using tool chain

steps:
  # Extract from multiple sources
  - id: extract_database
    action: query_database
    parameters:
      connection: ""{{ env.DB_CONNECTION }}""
      query: ""SELECT * FROM sales WHERE date >= '2024-01-01'""

  - id: extract_api
    action: call_api
    parameters:
      url: ""https://api.company.com/v2/transactions""
      method: ""GET""
      headers:
        Authorization: ""Bearer {{ env.API_KEY }}""
      params:
        start_date: ""2024-01-01""
        page_size: 1000

  - id: extract_files
    action: list_directory
    parameters:
      path: ""data/uploads/""
      pattern: ""sales_*.csv""
      recursive: true

  # Load file data
  - id: load_files
    for_each: ""{{ results.extract_files }}""
    as: file
    action: read_file
    parameters:
      path: ""{{ file.path }}""
      parse: true

  # Transform all data
  - id: merge_all
    action: merge_data
    parameters:
      datasets:
        - ""$results.extract_database""
        - ""$results.extract_api.data""
        - ""$results.load_files""
      key: ""transaction_id""

  - id: clean_data
    action: transform_data
    parameters:
      data: ""$results.merge_all""
      operations:
        - type: ""remove_duplicates""
          columns: [""transaction_id""]
        - type: ""fill_missing""
          strategy: ""forward""
        - type: ""standardize_formats""
          columns:
            date: ""YYYY-MM-DD""
            amount: ""decimal(10,2)""

  # Validate
  - id: validate_quality
    action: check_quality
    parameters:
      data: ""$results.clean_data""
      checks:
        - type: ""completeness""
          threshold: 0.99
        - type: ""accuracy""
          validations:
            amount: ""range:0,1000000""
            date: ""date_range:2024-01-01,today""

  # Load to destination
  - id: save_processed
    action: write_file
    parameters:
      path: ""processed/sales_cleaned_{{ execution.date }}.parquet""
      content: ""$results.clean_data""
      format: ""parquet""

  - id: update_database
    condition: ""{{ results.validate_quality.passed }}""
    action: insert_data
    parameters:
      connection: ""{{ env.DW_CONNECTION }}""
      table: ""sales_fact""
      data: ""$results.clean_data""
      mode: ""append""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","1110","1152","python","To create your own tools:","from orchestrator.tools.base import Tool

class MyCustomTool(Tool):
    def __init__(self):
        super().__init__(
            name=""my-custom-tool"",
            description=""Does something special""
        )

        # Define parameters
        self.add_parameter(
            name=""input_data"",
            type=""string"",
            description=""Data to process"",
            required=True
        )

        self.add_parameter(
            name=""mode"",
            type=""string"",
            description=""Processing mode"",
            required=False,
            default=""standard"",
            enum=[""standard"", ""advanced"", ""expert""]
        )

    async def execute(self, **kwargs):
        """"""Execute the tool action.""""""
        input_data = kwargs[""input_data""]
        mode = kwargs.get(""mode"", ""standard"")

        # Your tool logic here
        result = process_data(input_data, mode)

        return {
            ""status"": ""success"",
            ""result"": result,
            ""metadata"": {
                ""mode"": mode,
                ""timestamp"": datetime.now()
            }
        }","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tool_reference.rst","1160","1175","python","Register your tool to make it available:","from orchestrator.tools.base import default_registry

# Register tool
tool = MyCustomTool()
default_registry.register(tool)

# Use in pipeline
pipeline_yaml = """"""
steps:
  - id: custom_step
    action: my-custom-tool
    parameters:
      input_data: ""{{ inputs.data }}""
      mode: ""advanced""
""""""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","35","239","yaml","Create ``sales_etl.yaml``:","name: sales-etl-pipeline
description: Extract, transform, and load sales data

inputs:
  data_source:
    type: string
    description: ""Path to source data file""
    required: true

  output_format:
    type: string
    description: ""Output format""
    default: ""parquet""
    validation:
      enum: [""csv"", ""json"", ""parquet"", ""excel""]

  date_range:
    type: object
    description: ""Date range for filtering""
    default:
      start: ""2024-01-01""
      end: ""2024-12-31""

outputs:
  processed_data:
    type: string
    value: ""processed/sales_{{ execution.date }}.{{ inputs.output_format }}""

  quality_report:
    type: string
    value: ""reports/quality_{{ execution.date }}.json""

  summary_stats:
    type: string
    value: ""reports/summary_{{ execution.date }}.md""

steps:
  # Extract: Load raw data
  - id: extract_data
    action: read_file
    parameters:
      path: ""{{ inputs.data_source }}""
      parse: true
    error_handling:
      retry:
        max_attempts: 3
      fallback:
        action: generate_content
        parameters:
          prompt: ""Generate sample sales data for testing""

  # Transform: Clean and process data
  - id: clean_data
    action: transform_data
    parameters:
      data: ""$results.extract_data""
      operations:
        # Standardize column names
        - type: ""rename_columns""
          mapping:
            ""Sale Date"": ""sale_date""
            ""Customer Name"": ""customer_name""
            ""Product ID"": ""product_id""
            ""Sale Amount"": ""amount""
            ""Quantity"": ""quantity""
            ""Sales Rep"": ""sales_rep""

        # Convert data types
        - type: ""convert_types""
          conversions:
            sale_date: ""datetime""
            amount: ""float""
            quantity: ""integer""
            product_id: ""string""

        # Remove duplicates
        - type: ""remove_duplicates""
          columns: [""product_id"", ""sale_date"", ""customer_name""]

        # Handle missing values
        - type: ""fill_missing""
          strategy: ""forward""
          columns: [""sales_rep""]

        # Add calculated fields
        - type: ""add_column""
          name: ""total_value""
          expression: ""amount * quantity""

        - type: ""add_column""
          name: ""quarter""
          expression: ""quarter(sale_date)""

        - type: ""add_column""
          name: ""year""
          expression: ""year(sale_date)""

  # Filter data by date range
  - id: filter_data
    action: filter_data
    parameters:
      data: ""$results.clean_data""
      conditions:
        - field: ""sale_date""
          operator: ""gte""
          value: ""{{ inputs.date_range.start }}""
        - field: ""sale_date""
          operator: ""lte""
          value: ""{{ inputs.date_range.end }}""
        - field: ""amount""
          operator: ""gt""
          value: 0

  # Data quality validation
  - id: validate_quality
    action: check_quality
    parameters:
      data: ""$results.filter_data""
      checks:
        - type: ""completeness""
          threshold: 0.95
          columns: [""product_id"", ""amount"", ""sale_date""]

        - type: ""uniqueness""
          columns: [""product_id"", ""sale_date"", ""customer_name""]

        - type: ""consistency""
          rules:
            - ""total_value == amount * quantity""
            - ""amount > 0""
            - ""quantity > 0""

        - type: ""accuracy""
          validations:
            product_id: ""regex:^PROD-[0-9]{6}$""
            amount: ""range:1,50000""
            quantity: ""range:1,1000""

  # Generate summary statistics
  - id: calculate_summary
    action: aggregate_data
    parameters:
      data: ""$results.filter_data""
      group_by: [""year"", ""quarter""]
      aggregations:
        total_sales:
          column: ""total_value""
          function: ""sum""
        avg_sale:
          column: ""amount""
          function: ""mean""
        num_transactions:
          column: ""*""
          function: ""count""
        unique_customers:
          column: ""customer_name""
          function: ""nunique""
        top_product:
          column: ""product_id""
          function: ""mode""

  # Load: Save processed data
  - id: save_processed_data
    action: convert_format
    parameters:
      data: ""$results.filter_data""
      to_format: ""{{ inputs.output_format }}""
      output_path: ""{{ outputs.processed_data }}""
      options:
        compression: ""snappy""
        index: false

  # Save quality report
  - id: save_quality_report
    action: write_file
    parameters:
      path: ""{{ outputs.quality_report }}""
      content: ""{{ results.validate_quality | json }}""

  # Generate readable summary
  - id: create_summary_report
    action: generate_content
    parameters:
      prompt: |
        Create a summary report for sales data processing:

        Quality Results: {{ results.validate_quality | json }}
        Summary Statistics: {{ results.calculate_summary | json }}

        Include:
        - Data quality assessment
        - Key metrics and trends
        - Any issues or recommendations
        - Processing summary

      style: ""professional""
      format: ""markdown""

  # Save summary report
  - id: save_summary
    action: write_file
    parameters:
      path: ""{{ outputs.summary_stats }}""
      content: ""$results.create_summary_report""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","245","264","python","----------------------------","import orchestrator as orc

# Initialize
orc.init_models()

# Compile pipeline
etl_pipeline = orc.compile(""sales_etl.yaml"")

# Process sales data
result = etl_pipeline.run(
    data_source=""data/raw/sales_2024.csv"",
    output_format=""parquet"",
    date_range={
        ""start"": ""2024-01-01"",
        ""end"": ""2024-06-30""
    }
)

print(f""ETL completed: {result}"")","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","277","521","yaml","Create ``data_integration.yaml``:","name: multi-source-integration
description: Integrate data from multiple sources with validation

inputs:
  sources:
    type: object
    description: ""Data source configurations""
    required: true
    # Example:
    # database:
    #   type: ""postgresql""
    #   connection: ""postgresql://...""
    #   query: ""SELECT * FROM sales""
    # api:
    #   type: ""rest""
    #   url: ""https://api.company.com/data""
    #   headers: {...}
    # files:
    #   type: ""file""
    #   paths: [""data1.csv"", ""data2.json""]

  merge_strategy:
    type: string
    description: ""How to merge data sources""
    default: ""outer""
    validation:
      enum: [""inner"", ""outer"", ""left"", ""right""]

  deduplication_fields:
    type: array
    description: ""Fields to use for deduplication""
    default: [""id"", ""timestamp""]

outputs:
  integrated_data:
    type: string
    value: ""integrated/master_data_{{ execution.timestamp }}.parquet""

  integration_report:
    type: string
    value: ""reports/integration_{{ execution.timestamp }}.md""

steps:
  # Extract from database sources
  - id: extract_database
    condition: ""'database' in inputs.sources""
    action: query_database
    parameters:
      connection: ""{{ inputs.sources.database.connection }}""
      query: ""{{ inputs.sources.database.query }}""
      fetch_size: 10000
    error_handling:
      continue_on_error: true

  # Extract from API sources
  - id: extract_api
    condition: ""'api' in inputs.sources""
    action: call_api
    parameters:
      url: ""{{ inputs.sources.api.url }}""
      method: ""GET""
      headers: ""{{ inputs.sources.api.headers | default({}) }}""
      params: ""{{ inputs.sources.api.params | default({}) }}""
      timeout: 300
    error_handling:
      retry:
        max_attempts: 3
        backoff: ""exponential""

  # Extract from file sources
  - id: extract_files
    condition: ""'files' in inputs.sources""
    for_each: ""{{ inputs.sources.files.paths }}""
    as: file_path
    action: read_file
    parameters:
      path: ""{{ file_path }}""
      parse: true

  # Standardize data schemas
  - id: standardize_database
    condition: ""results.extract_database is defined""
    action: transform_data
    parameters:
      data: ""$results.extract_database""
      operations:
        - type: ""add_column""
          name: ""source""
          value: ""database""
        - type: ""standardize_schema""
          target_schema:
            id: ""string""
            timestamp: ""datetime""
            value: ""float""
            category: ""string""

  - id: standardize_api
    condition: ""results.extract_api is defined""
    action: transform_data
    parameters:
      data: ""$results.extract_api.data""
      operations:
        - type: ""add_column""
          name: ""source""
          value: ""api""
        - type: ""flatten_nested""
          columns: [""metadata"", ""attributes""]
        - type: ""standardize_schema""
          target_schema:
            id: ""string""
            timestamp: ""datetime""
            value: ""float""
            category: ""string""

  - id: standardize_files
    condition: ""results.extract_files is defined""
    action: transform_data
    parameters:
      data: ""$results.extract_files""
      operations:
        - type: ""add_column""
          name: ""source""
          value: ""files""
        - type: ""combine_files""
          strategy: ""union""
        - type: ""standardize_schema""
          target_schema:
            id: ""string""
            timestamp: ""datetime""
            value: ""float""
            category: ""string""

  # Merge all data sources
  - id: merge_sources
    action: merge_data
    parameters:
      datasets:
        - ""$results.standardize_database""
        - ""$results.standardize_api""
        - ""$results.standardize_files""
      how: ""{{ inputs.merge_strategy }}""
      on: [""id""]
      suffixes: [""_db"", ""_api"", ""_file""]

  # Remove duplicates
  - id: deduplicate
    action: transform_data
    parameters:
      data: ""$results.merge_sources""
      operations:
        - type: ""remove_duplicates""
          columns: ""{{ inputs.deduplication_fields }}""
          keep: ""last""  # Keep most recent

  # Data quality assessment
  - id: assess_integration_quality
    action: check_quality
    parameters:
      data: ""$results.deduplicate""
      checks:
        - type: ""completeness""
          threshold: 0.90
          critical_columns: [""id"", ""timestamp""]

        - type: ""consistency""
          rules:
            - ""value_db == value_api OR value_db IS NULL OR value_api IS NULL""
            - ""timestamp >= '2020-01-01'""

        - type: ""accuracy""
          validations:
            id: ""not_null""
            timestamp: ""datetime_format""
            value: ""numeric_range:-1000000,1000000""

  # Resolve conflicts between sources
  - id: resolve_conflicts
    action: transform_data
    parameters:
      data: ""$results.deduplicate""
      operations:
        - type: ""resolve_conflicts""
          strategy: ""priority""
          priority_order: [""database"", ""api"", ""files""]
          conflict_columns: [""value"", ""category""]

        - type: ""add_column""
          name: ""confidence_score""
          expression: ""calculate_confidence(source_count, data_age, validation_status)""

  # Create final integrated dataset
  - id: finalize_integration
    action: transform_data
    parameters:
      data: ""$results.resolve_conflicts""
      operations:
        - type: ""select_columns""
          columns: [""id"", ""timestamp"", ""value"", ""category"", ""source"", ""confidence_score""]

        - type: ""sort""
          columns: [""timestamp""]
          ascending: [false]

  # Save integrated data
  - id: save_integrated
    action: convert_format
    parameters:
      data: ""$results.finalize_integration""
      to_format: ""parquet""
      output_path: ""{{ outputs.integrated_data }}""
      options:
        compression: ""snappy""
        partition_cols: [""category""]

  # Generate integration report
  - id: create_integration_report
    action: generate_content
    parameters:
      prompt: |
        Create an integration report for multi-source data merge:

        Sources processed:
        {% for source in inputs.sources.keys() %}
        - {{ source }}
        {% endfor %}

        Quality assessment: {{ results.assess_integration_quality | json }}
        Final record count: {{ results.finalize_integration | length }}

        Include:
        - Source summary and statistics
        - Data quality metrics
        - Conflict resolution summary
        - Recommendations for data improvement

      style: ""technical""
      format: ""markdown""

  # Save integration report
  - id: save_report
    action: write_file
    parameters:
      path: ""{{ outputs.integration_report }}""
      content: ""$results.create_integration_report""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","527","558","python","-----------------------------------","import orchestrator as orc

# Initialize
orc.init_models()

# Compile integration pipeline
integration = orc.compile(""data_integration.yaml"")

# Integrate data from multiple sources
result = integration.run(
    sources={
        ""database"": {
            ""type"": ""postgresql"",
            ""connection"": ""postgresql://user:pass@localhost/mydb"",
            ""query"": ""SELECT * FROM transactions WHERE date >= '2024-01-01'""
        },
        ""api"": {
            ""type"": ""rest"",
            ""url"": ""https://api.external.com/v1/data"",
            ""headers"": {""Authorization"": ""Bearer token123""}
        },
        ""files"": {
            ""type"": ""file"",
            ""paths"": [""data/file1.csv"", ""data/file2.json""]
        }
    },
    merge_strategy=""outer"",
    deduplication_fields=[""transaction_id"", ""timestamp""]
)

print(f""Integration completed: {result}"")","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","571","815","yaml","Create ``data_quality.yaml``:","name: data-quality-assessment
description: Comprehensive data quality evaluation and reporting

inputs:
  dataset_path:
    type: string
    required: true

  quality_rules:
    type: object
    description: ""Custom quality rules""
    default:
      completeness_threshold: 0.95
      uniqueness_fields: [""id""]
      date_range_field: ""created_date""
      numeric_fields: [""amount"", ""quantity""]

  remediation_mode:
    type: string
    description: ""How to handle quality issues""
    default: ""report""
    validation:
      enum: [""report"", ""fix"", ""quarantine""]

outputs:
  quality_report:
    type: string
    value: ""quality/report_{{ execution.timestamp }}.html""

  cleaned_data:
    type: string
    value: ""quality/cleaned_{{ execution.timestamp }}.parquet""

  issues_log:
    type: string
    value: ""quality/issues_{{ execution.timestamp }}.json""

steps:
  # Load the dataset
  - id: load_dataset
    action: read_file
    parameters:
      path: ""{{ inputs.dataset_path }}""
      parse: true

  # Basic data profiling
  - id: profile_data
    action: analyze_data
    parameters:
      data: ""$results.load_dataset""
      analysis_types:
        - schema
        - statistics
        - distributions
        - patterns
        - outliers

  # Completeness assessment
  - id: check_completeness
    action: check_quality
    parameters:
      data: ""$results.load_dataset""
      checks:
        - type: ""completeness""
          threshold: ""{{ inputs.quality_rules.completeness_threshold }}""
          report_by_column: true

        - type: ""null_patterns""
          identify_patterns: true

  # Uniqueness validation
  - id: check_uniqueness
    action: validate_data
    parameters:
      data: ""$results.load_dataset""
      rules:
        - name: ""primary_key_uniqueness""
          type: ""uniqueness""
          columns: ""{{ inputs.quality_rules.uniqueness_fields }}""
          severity: ""error""

        - name: ""near_duplicates""
          type: ""similarity""
          threshold: 0.9
          columns: [""name"", ""email""]
          severity: ""warning""

  # Consistency validation
  - id: check_consistency
    action: validate_data
    parameters:
      data: ""$results.load_dataset""
      rules:
        - name: ""date_logic""
          condition: ""start_date <= end_date""
          severity: ""error""

        - name: ""numeric_consistency""
          condition: ""total == sum(line_items)""
          severity: ""error""

        - name: ""referential_integrity""
          type: ""foreign_key""
          reference_table: ""lookup_table""
          foreign_key: ""category_id""
          severity: ""warning""

  # Accuracy validation
  - id: check_accuracy
    action: validate_data
    parameters:
      data: ""$results.load_dataset""
      rules:
        - name: ""email_format""
          field: ""email""
          validation: ""regex:^[\\w.-]+@[\\w.-]+\\.\\w+$""
          severity: ""warning""

        - name: ""phone_format""
          field: ""phone""
          validation: ""regex:^\\+?1?\\d{9,15}$""
          severity: ""info""

        - name: ""numeric_ranges""
          field: ""{{ inputs.quality_rules.numeric_fields }}""
          validation: ""range:0,999999""
          severity: ""error""

  # Timeliness assessment
  - id: check_timeliness
    action: validate_data
    parameters:
      data: ""$results.load_dataset""
      rules:
        - name: ""data_freshness""
          field: ""{{ inputs.quality_rules.date_range_field }}""
          condition: ""date_diff(value, today()) <= 30""
          severity: ""warning""
          message: ""Data is older than 30 days""

  # Outlier detection
  - id: detect_outliers
    action: analyze_data
    parameters:
      data: ""$results.load_dataset""
      analysis_types:
        - outliers
      methods:
        - statistical  # Z-score, IQR
        - isolation_forest
        - local_outlier_factor
      numeric_columns: ""{{ inputs.quality_rules.numeric_fields }}""

  # Compile quality issues
  - id: compile_issues
    action: transform_data
    parameters:
      data:
        completeness: ""$results.check_completeness""
        uniqueness: ""$results.check_uniqueness""
        consistency: ""$results.check_consistency""
        accuracy: ""$results.check_accuracy""
        timeliness: ""$results.check_timeliness""
        outliers: ""$results.detect_outliers""
      operations:
        - type: ""consolidate_issues""
          prioritize: true
        - type: ""categorize_severity""
          levels: [""critical"", ""major"", ""minor"", ""info""]

  # Data remediation (if requested)
  - id: remediate_data
    condition: ""inputs.remediation_mode in ['fix', 'quarantine']""
    action: transform_data
    parameters:
      data: ""$results.load_dataset""
      operations:
        # Fix common issues
        - type: ""standardize_formats""
          columns:
            email: ""lowercase""
            phone: ""normalize_phone""
            name: ""title_case""

        - type: ""fill_missing""
          strategy: ""smart""  # Use ML-based imputation
          columns: ""{{ inputs.quality_rules.numeric_fields }}""

        - type: ""remove_outliers""
          method: ""iqr""
          columns: ""{{ inputs.quality_rules.numeric_fields }}""
          action: ""{{ 'quarantine' if inputs.remediation_mode == 'quarantine' else 'remove' }}""

        - type: ""deduplicate""
          strategy: ""keep_best""  # Keep record with highest completeness

  # Generate comprehensive quality report
  - id: create_quality_report
    action: generate_content
    parameters:
      prompt: |
        Create a comprehensive data quality report:

        Dataset: {{ inputs.dataset_path }}
        Profile: {{ results.profile_data | json }}
        Issues: {{ results.compile_issues | json }}

        Include:
        1. Executive Summary
        2. Data Profile Overview
        3. Quality Metrics Dashboard
        4. Issue Analysis by Category
        5. Impact Assessment
        6. Remediation Recommendations
        7. Quality Score Calculation

        Format as HTML with charts and tables.

      style: ""technical""
      format: ""html""
      max_tokens: 3000

  # Save quality report
  - id: save_quality_report
    action: write_file
    parameters:
      path: ""{{ outputs.quality_report }}""
      content: ""$results.create_quality_report""

  # Save cleaned data (if remediation performed)
  - id: save_cleaned_data
    condition: ""inputs.remediation_mode in ['fix', 'quarantine']""
    action: write_file
    parameters:
      path: ""{{ outputs.cleaned_data }}""
      content: ""$results.remediate_data""
      format: ""parquet""

  # Save issues log
  - id: save_issues_log
    action: write_file
    parameters:
      path: ""{{ outputs.issues_log }}""
      content: ""{{ results.compile_issues | json }}""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","828","923","yaml","Create ``realtime_processing.yaml``:","name: realtime-data-processing
description: Process streaming data with real-time analytics

inputs:
  stream_source:
    type: object
    description: ""Stream configuration""
    required: true
    # Example:
    # type: ""kafka""
    # topic: ""events""
    # batch_size: 1000
    # window_size: ""5m""

  processing_rules:
    type: array
    description: ""Processing rules to apply""
    default:
      - type: ""filter""
        condition: ""event_type in ['purchase', 'click']""
      - type: ""enrich""
        lookup_table: ""user_profiles""
      - type: ""aggregate""
        window: ""5m""
        metrics: [""count"", ""sum"", ""avg""]

outputs:
  processed_stream:
    type: string
    value: ""stream/processed_{{ execution.date }}""

  alerts:
    type: string
    value: ""alerts/stream_alerts_{{ execution.timestamp }}.json""

steps:
  # Connect to stream source
  - id: connect_stream
    action: connect_stream
    parameters:
      source: ""{{ inputs.stream_source }}""
      batch_size: ""{{ inputs.stream_source.batch_size | default(1000) }}""
      timeout: 30

  # Process incoming batches
  - id: process_batches
    action: process_stream_batch
    parameters:
      stream: ""$results.connect_stream""
      processing_rules: ""{{ inputs.processing_rules }}""
      window_config:
        size: ""{{ inputs.stream_source.window_size | default('5m') }}""
        type: ""tumbling""  # or ""sliding"", ""session""

  # Real-time anomaly detection
  - id: detect_anomalies
    action: detect_anomalies
    parameters:
      data: ""$results.process_batches""
      methods:
        - statistical_control
        - machine_learning
      thresholds:
        statistical: 3.0  # standard deviations
        ml_confidence: 0.95

  # Generate alerts
  - id: generate_alerts
    condition: ""results.detect_anomalies.anomalies | length > 0""
    action: generate_content
    parameters:
      prompt: |
        Generate alerts for detected anomalies:
        {{ results.detect_anomalies.anomalies | json }}

        Include severity, description, and recommended actions.

      format: ""json""

  # Save processed data
  - id: save_processed
    action: write_stream
    parameters:
      data: ""$results.process_batches""
      destination: ""{{ outputs.processed_stream }}""
      format: ""parquet""
      partition_by: [""date"", ""hour""]

  # Save alerts
  - id: save_alerts
    condition: ""results.generate_alerts is defined""
    action: write_file
    parameters:
      path: ""{{ outputs.alerts }}""
      content: ""$results.generate_alerts""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","932","990","yaml","---------------------------------","name: customer-data-platform
description: Unified customer data processing and analytics

inputs:
  customer_sources:
    type: object
    required: true
    # CRM, support tickets, web analytics, purchase history

steps:
  # Extract from all customer touchpoints
  - id: extract_crm
    action: query_database
    parameters:
      connection: ""{{ inputs.customer_sources.crm.connection }}""
      query: ""SELECT * FROM customers WHERE updated_at >= CURRENT_DATE - INTERVAL '1 day'""

  - id: extract_support
    action: call_api
    parameters:
      url: ""{{ inputs.customer_sources.support.api_url }}""
      headers:
        Authorization: ""Bearer {{ env.SUPPORT_API_KEY }}""

  - id: extract_analytics
    action: read_file
    parameters:
      path: ""{{ inputs.customer_sources.analytics.export_path }}""
      parse: true

  # Create unified customer profiles
  - id: merge_customer_data
    action: merge_data
    parameters:
      datasets:
        - ""$results.extract_crm""
        - ""$results.extract_support""
        - ""$results.extract_analytics""
      on: ""customer_id""
      how: ""outer""

  # Calculate customer metrics
  - id: calculate_metrics
    action: transform_data
    parameters:
      data: ""$results.merge_customer_data""
      operations:
        - type: ""add_column""
          name: ""customer_lifetime_value""
          expression: ""sum(purchase_amounts) * retention_probability""

        - type: ""add_column""
          name: ""churn_risk_score""
          expression: ""calculate_churn_risk(days_since_last_activity, support_tickets, engagement_score)""

        - type: ""add_column""
          name: ""segment""
          expression: ""classify_customer_segment(clv, engagement, recency)""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","996","1062","yaml","----------------------------------","name: financial-data-pipeline
description: Process financial transactions with compliance checks

inputs:
  transaction_sources:
    type: array
    required: true

  compliance_rules:
    type: object
    required: true

steps:
  # Extract transactions from multiple sources
  - id: extract_transactions
    for_each: ""{{ inputs.transaction_sources }}""
    as: source
    action: extract_financial_data
    parameters:
      source_config: ""{{ source }}""
      date_range: ""{{ execution.date | date_range('-1d') }}""

  # Compliance screening
  - id: screen_transactions
    action: validate_data
    parameters:
      data: ""$results.extract_transactions""
      rules:
        - name: ""aml_screening""
          type: ""anti_money_laundering""
          threshold: ""{{ inputs.compliance_rules.aml_threshold }}""

        - name: ""sanctions_check""
          type: ""sanctions_screening""
          watchlists: ""{{ inputs.compliance_rules.watchlists }}""

        - name: ""pep_screening""
          type: ""politically_exposed_person""
          databases: ""{{ inputs.compliance_rules.pep_databases }}""

  # Risk scoring
  - id: calculate_risk_scores
    action: transform_data
    parameters:
      data: ""$results.extract_transactions""
      operations:
        - type: ""add_column""
          name: ""risk_score""
          expression: ""calculate_transaction_risk(amount, counterparty, geography, transaction_type)""

        - type: ""add_column""
          name: ""risk_category""
          expression: ""categorize_risk(risk_score)""

  # Generate compliance report
  - id: create_compliance_report
    action: generate_content
    parameters:
      prompt: |
        Generate daily compliance report:

        Transactions processed: {{ results.extract_transactions | length }}
        Screening results: {{ results.screen_transactions | json }}
        Risk distribution: {{ results.calculate_risk_scores | group_by('risk_category') }}

        Include regulatory compliance status and any required actions.","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","1073","1078","yaml","Build a pipeline that processes e-commerce data:","# Your challenge:
# - Extract: Orders, customers, products, reviews
# - Transform: Calculate metrics, segment customers
# - Load: Create analytics-ready datasets
# - Quality: Validate business rules","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","1086","1091","yaml","Create a pipeline for IoT sensor data:","# Requirements:
# - Handle high-volume time series data
# - Detect sensor anomalies
# - Aggregate by time windows
# - Generate maintenance alerts","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_data_processing.rst","1099","1104","yaml","Build a social media data processing pipeline:","# Features:
# - Extract from multiple platforms
# - Text analysis and sentiment
# - Trend detection
# - Influence measurement","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","36","91","yaml","Create a file called ``web_search.yaml``:","name: basic-web-search
description: Search the web and compile results into a report

inputs:
  query:
    type: string
    description: ""Search query""
    required: true

  max_results:
    type: integer
    description: ""Maximum number of results to return""
    default: 10
    validation:
      min: 1
      max: 50

outputs:
  report:
    type: string
    value: ""search_results_{{ inputs.query | slugify }}.md""

steps:
  # Search the web
  - id: search
    action: search_web
    parameters:
      query: ""{{ inputs.query }}""
      max_results: ""{{ inputs.max_results }}""
      include_snippets: true

  # Compile into markdown report
  - id: compile_report
    action: generate_content
    parameters:
      prompt: |
        Create a well-organized markdown report from these search results:

        {{ results.search | json }}

        Include:
        - Executive summary
        - Key findings
        - Source links
        - Relevant details from each result

      style: ""professional""
      format: ""markdown""

  # Save the report
  - id: save_report
    action: write_file
    parameters:
      path: ""{{ outputs.report }}""
      content: ""$results.compile_report""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","97","117","python","------------------------","import orchestrator as orc

# Initialize
orc.init_models()

# Compile and run
pipeline = orc.compile(""web_search.yaml"")

# Search for different topics
result1 = pipeline.run(
    query=""artificial intelligence trends 2024"",
    max_results=15
)

result2 = pipeline.run(
    query=""sustainable energy solutions"",
    max_results=10
)

print(f""Generated reports: {result1}, {result2}"")","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","125","142","markdown","Your pipeline will create markdown files like:","# Search Results: Artificial Intelligence Trends 2024

## Executive Summary

Recent searches reveal significant developments in AI across multiple domains...

## Key Findings

1. **Large Language Models** - Continued advancement in reasoning capabilities
2. **AI Safety** - Increased focus on alignment and control
3. **Enterprise Adoption** - Growing integration in business processes

## Detailed Results

### 1. AI Breakthrough: New Model Achieves Human-Level Performance
**Source**: [TechCrunch](https://techcrunch.com/...)
**Summary**: Details about the latest AI advancement...","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","155","346","yaml","Create ``multi_source_research.yaml``:","name: multi-source-research
description: Comprehensive research using web, news, and academic sources

inputs:
  topic:
    type: string
    required: true

  depth:
    type: string
    description: ""Research depth""
    default: ""medium""
    validation:
      enum: [""light"", ""medium"", ""deep""]

  include_sources:
    type: array
    description: ""Sources to include""
    default: [""web"", ""news"", ""academic""]
    validation:
      enum_items: [""web"", ""news"", ""academic"", ""patents""]

outputs:
  comprehensive_report:
    type: string
    value: ""research/{{ inputs.topic | slugify }}_comprehensive.md""

  data_file:
    type: string
    value: ""research/{{ inputs.topic | slugify }}_data.json""

# Research depth configuration
config:
  research_params:
    light:
      web_results: 10
      news_results: 5
      academic_results: 3
    medium:
      web_results: 20
      news_results: 10
      academic_results: 8
    deep:
      web_results: 40
      news_results: 20
      academic_results: 15

steps:
  # Parallel search across sources
  - id: search_sources
    parallel:
      # Web search
      - id: web_search
        condition: ""'web' in inputs.include_sources""
        action: search_web
        parameters:
          query: ""{{ inputs.topic }} comprehensive overview""
          max_results: ""{{ config.research_params[inputs.depth].web_results }}""
          include_snippets: true

      # News search
      - id: news_search
        condition: ""'news' in inputs.include_sources""
        action: search_news
        parameters:
          query: ""{{ inputs.topic }}""
          max_results: ""{{ config.research_params[inputs.depth].news_results }}""
          date_range: ""last_month""

      # Academic search
      - id: academic_search
        condition: ""'academic' in inputs.include_sources""
        action: search_academic
        parameters:
          query: ""{{ inputs.topic }}""
          max_results: ""{{ config.research_params[inputs.depth].academic_results }}""
          year_range: ""2020-2024""
          peer_reviewed: true

  # Extract key information from each source
  - id: extract_information
    action: extract_information
    parameters:
      content: ""$results.search_sources""
      extract:
        key_facts:
          description: ""Important facts and findings""
        statistics:
          description: ""Numerical data and metrics""
        expert_opinions:
          description: ""Quotes and opinions from experts""
        trends:
          description: ""Emerging trends and developments""
        challenges:
          description: ""Problems and challenges mentioned""
        opportunities:
          description: ""Opportunities and potential solutions""

  # Cross-validate information
  - id: validate_facts
    action: validate_data
    parameters:
      data: ""$results.extract_information""
      rules:
        - name: ""source_diversity""
          condition: ""count(unique(sources)) >= 2""
          severity: ""warning""
          message: ""Information should be confirmed by multiple sources""

        - name: ""recent_information""
          field: ""date""
          condition: ""date_diff(value, today()) <= 365""
          severity: ""info""
          message: ""Information is from the last year""

  # Generate comprehensive analysis
  - id: analyze_findings
    action: generate_content
    parameters:
      prompt: |
        Analyze the following research data about {{ inputs.topic }}:

        {{ results.extract_information | json }}

        Provide:
        1. Current state analysis
        2. Key trends identification
        3. Challenge assessment
        4. Future outlook
        5. Recommendations

        Base your analysis on the evidence provided and note any limitations.

      style: ""analytical""
      max_tokens: 2000

  # Create structured data export
  - id: export_data
    action: transform_data
    parameters:
      data:
        topic: ""{{ inputs.topic }}""
        research_date: ""{{ execution.timestamp }}""
        depth: ""{{ inputs.depth }}""
        sources_used: ""{{ inputs.include_sources }}""
        extracted_info: ""$results.extract_information""
        validation_results: ""$results.validate_facts""
        analysis: ""$results.analyze_findings""
      operations:
        - type: ""convert_format""
          to_format: ""json""

  # Save structured data
  - id: save_data
    action: write_file
    parameters:
      path: ""{{ outputs.data_file }}""
      content: ""$results.export_data""

  # Generate final report
  - id: create_report
    action: generate_content
    parameters:
      prompt: |
        Create a comprehensive research report about {{ inputs.topic }} using:

        Analysis: {{ results.analyze_findings }}

        Structure the report with:
        1. Executive Summary
        2. Methodology
        3. Current State Analysis
        4. Key Findings
        5. Trends and Developments
        6. Challenges and Limitations
        7. Future Outlook
        8. Recommendations
        9. Sources and References

        Include confidence levels for major claims.

      style: ""professional""
      format: ""markdown""
      max_tokens: 3000

  # Save final report
  - id: save_report
    action: write_file
    parameters:
      path: ""{{ outputs.comprehensive_report }}""
      content: ""$results.create_report""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","352","375","python","---------------------------------","import orchestrator as orc

# Initialize
orc.init_models()

# Compile pipeline
pipeline = orc.compile(""multi_source_research.yaml"")

# Run deep research on quantum computing
result = pipeline.run(
    topic=""quantum computing applications"",
    depth=""deep"",
    include_sources=[""web"", ""academic"", ""news""]
)

print(f""Research complete: {result}"")

# Run lighter research on emerging tech
result2 = pipeline.run(
    topic=""edge computing trends"",
    depth=""medium"",
    include_sources=[""web"", ""news""]
)","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","388","488","yaml","Create ``fact_checker.yaml``:","name: fact-checker
description: Verify claims against multiple reliable sources

inputs:
  claims:
    type: array
    description: ""Claims to verify""
    required: true

  confidence_threshold:
    type: float
    description: ""Minimum confidence level to accept claims""
    default: 0.7
    validation:
      min: 0.0
      max: 1.0

outputs:
  fact_check_report:
    type: string
    value: ""fact_check_{{ execution.timestamp | strftime('%Y%m%d_%H%M') }}.md""

steps:
  # Research each claim
  - id: research_claims
    for_each: ""{{ inputs.claims }}""
    as: claim
    action: search_web
    parameters:
      query: ""{{ claim }} verification facts evidence""
      max_results: 15
      include_snippets: true

  # Extract supporting/contradicting evidence
  - id: analyze_evidence
    for_each: ""{{ inputs.claims }}""
    as: claim
    action: extract_information
    parameters:
      content: ""$results.research_claims[loop.index0]""
      extract:
        supporting_evidence:
          description: ""Evidence that supports the claim""
        contradicting_evidence:
          description: ""Evidence that contradicts the claim""
        source_credibility:
          description: ""Assessment of source reliability""
        expert_opinions:
          description: ""Expert statements about the claim""

  # Assess credibility of each claim
  - id: assess_claims
    for_each: ""{{ inputs.claims }}""
    as: claim
    action: generate_content
    parameters:
      prompt: |
        Assess the veracity of this claim: ""{{ claim }}""

        Based on the evidence:
        {{ results.analyze_evidence[loop.index0] | json }}

        Provide:
        1. Verdict: True/False/Partially True/Insufficient Evidence
        2. Confidence level (0-1)
        3. Supporting evidence summary
        4. Contradicting evidence summary
        5. Overall assessment

        Be objective and cite specific sources.

      style: ""analytical""
      format: ""structured""

  # Compile fact-check report
  - id: create_fact_check_report
    action: generate_content
    parameters:
      prompt: |
        Create a comprehensive fact-check report based on:

        Claims assessed: {{ inputs.claims | json }}
        Assessment results: {{ results.assess_claims | json }}

        Format as a professional fact-checking article with:
        1. Summary of findings
        2. Individual claim assessments
        3. Methodology used
        4. Sources consulted
        5. Limitations and caveats

      style: ""journalistic""
      format: ""markdown""

  # Save report
  - id: save_fact_check
    action: write_file
    parameters:
      path: ""{{ outputs.fact_check_report }}""
      content: ""$results.create_fact_check_report""","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","494","514","python","----------------------------","import orchestrator as orc

# Initialize
orc.init_models()

# Compile fact-checker
fact_checker = orc.compile(""fact_checker.yaml"")

# Check various claims
result = fact_checker.run(
    claims=[
        ""Electric vehicles produce zero emissions"",
        ""AI will replace 50% of jobs by 2030"",
        ""Quantum computers can break all current encryption"",
        ""Renewable energy is now cheaper than fossil fuels""
    ],
    confidence_threshold=0.8
)

print(f""Fact-check report: {result}"")","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","527","805","yaml","Create ``report_generator.yaml``:","name: automated-report-generator
description: Generate professional reports from research data

inputs:
  topic:
    type: string
    required: true

  report_type:
    type: string
    description: ""Type of report to generate""
    default: ""standard""
    validation:
      enum: [""executive"", ""technical"", ""standard"", ""briefing""]

  target_audience:
    type: string
    description: ""Primary audience for the report""
    default: ""general""
    validation:
      enum: [""executives"", ""technical"", ""general"", ""academic""]

  sections:
    type: array
    description: ""Sections to include in report""
    default: [""summary"", ""introduction"", ""analysis"", ""conclusion""]

outputs:
  report_markdown:
    type: string
    value: ""reports/{{ inputs.topic | slugify }}_{{ inputs.report_type }}.md""

  report_pdf:
    type: string
    value: ""reports/{{ inputs.topic | slugify }}_{{ inputs.report_type }}.pdf""

  report_html:
    type: string
    value: ""reports/{{ inputs.topic | slugify }}_{{ inputs.report_type }}.html""

# Report templates by type
config:
  report_templates:
    executive:
      style: ""executive""
      length: ""concise""
      focus: ""strategic""
      sections: [""executive_summary"", ""key_findings"", ""recommendations"", ""appendix""]

    technical:
      style: ""technical""
      length: ""detailed""
      focus: ""implementation""
      sections: [""introduction"", ""technical_analysis"", ""methodology"", ""results"", ""conclusion""]

    standard:
      style: ""professional""
      length: ""medium""
      focus: ""comprehensive""
      sections: [""summary"", ""background"", ""analysis"", ""findings"", ""recommendations""]

    briefing:
      style: ""concise""
      length: ""short""
      focus: ""actionable""
      sections: [""situation"", ""assessment"", ""recommendations""]

steps:
  # Gather comprehensive research data
  - id: research_topic
    action: search_web
    parameters:
      query: ""{{ inputs.topic }} comprehensive analysis research""
      max_results: 25
      include_snippets: true

  # Get recent news for current context
  - id: current_context
    action: search_news
    parameters:
      query: ""{{ inputs.topic }}""
      max_results: 10
      date_range: ""last_week""

  # Extract structured information
  - id: extract_report_data
    action: extract_information
    parameters:
      content:
        research: ""$results.research_topic""
        news: ""$results.current_context""
      extract:
        key_points:
          description: ""Main points and findings""
        statistics:
          description: ""Important numbers and data""
        trends:
          description: ""Current and emerging trends""
        implications:
          description: ""Implications and consequences""
        expert_views:
          description: ""Expert opinions and quotes""
        future_outlook:
          description: ""Predictions and future scenarios""

  # Generate executive summary
  - id: create_executive_summary
    condition: ""'summary' in inputs.sections or 'executive_summary' in inputs.sections""
    action: generate_content
    parameters:
      prompt: |
        Create an executive summary for {{ inputs.target_audience }} audience about {{ inputs.topic }}.

        Based on: {{ results.extract_report_data.key_points | json }}

        Style: {{ config.report_templates[inputs.report_type].style }}
        Focus: {{ config.report_templates[inputs.report_type].focus }}

        Include the most critical points in 200-400 words.

      style: ""{{ config.report_templates[inputs.report_type].style }}""
      max_tokens: 500

  # Generate introduction/background
  - id: create_introduction
    condition: ""'introduction' in inputs.sections or 'background' in inputs.sections""
    action: generate_content
    parameters:
      prompt: |
        Write an introduction/background section about {{ inputs.topic }} for {{ inputs.target_audience }}.

        Context: {{ results.extract_report_data | json }}

        Provide necessary background and context for understanding the topic.

      style: ""{{ config.report_templates[inputs.report_type].style }}""
      max_tokens: 800

  # Generate main analysis
  - id: create_analysis
    condition: ""'analysis' in inputs.sections or 'technical_analysis' in inputs.sections""
    action: generate_content
    parameters:
      prompt: |
        Create a comprehensive analysis section about {{ inputs.topic }}.

        Data: {{ results.extract_report_data | json }}

        Style: {{ config.report_templates[inputs.report_type].style }}
        Audience: {{ inputs.target_audience }}

        Include:
        - Current state analysis
        - Trend analysis
        - Key factors and drivers
        - Challenges and opportunities

        Support points with specific data and examples.

      style: ""{{ config.report_templates[inputs.report_type].style }}""
      max_tokens: 1500

  # Generate findings and implications
  - id: create_findings
    condition: ""'findings' in inputs.sections or 'key_findings' in inputs.sections""
    action: generate_content
    parameters:
      prompt: |
        Summarize key findings and implications regarding {{ inputs.topic }}.

        Analysis: {{ results.create_analysis }}
        Supporting data: {{ results.extract_report_data.implications | json }}

        Present clear, actionable findings with implications.

      style: ""{{ config.report_templates[inputs.report_type].style }}""
      max_tokens: 1000

  # Generate recommendations
  - id: create_recommendations
    condition: ""'recommendations' in inputs.sections""
    action: generate_content
    parameters:
      prompt: |
        Develop actionable recommendations based on the analysis of {{ inputs.topic }}.

        Findings: {{ results.create_findings }}
        Target audience: {{ inputs.target_audience }}

        Provide specific, actionable recommendations with priorities and considerations.

      style: ""{{ config.report_templates[inputs.report_type].style }}""
      max_tokens: 800

  # Generate conclusion
  - id: create_conclusion
    condition: ""'conclusion' in inputs.sections""
    action: generate_content
    parameters:
      prompt: |
        Write a strong conclusion for the {{ inputs.topic }} report.

        Key findings: {{ results.create_findings }}
        Recommendations: {{ results.create_recommendations }}

        Synthesize the main points and end with a clear call to action.

      style: ""{{ config.report_templates[inputs.report_type].style }}""
      max_tokens: 400

  # Assemble complete report
  - id: assemble_report
    action: generate_content
    parameters:
      prompt: |
        Compile a complete, professional report about {{ inputs.topic }}.

        Report type: {{ inputs.report_type }}
        Target audience: {{ inputs.target_audience }}

        Sections to include:
        {% if results.create_executive_summary %}
        Executive Summary: {{ results.create_executive_summary }}
        {% endif %}

        {% if results.create_introduction %}
        Introduction: {{ results.create_introduction }}
        {% endif %}

        {% if results.create_analysis %}
        Analysis: {{ results.create_analysis }}
        {% endif %}

        {% if results.create_findings %}
        Findings: {{ results.create_findings }}
        {% endif %}

        {% if results.create_recommendations %}
        Recommendations: {{ results.create_recommendations }}
        {% endif %}

        {% if results.create_conclusion %}
        Conclusion: {{ results.create_conclusion }}
        {% endif %}

        Format as a professional markdown document with:
        - Proper headings and structure
        - Table of contents
        - Professional formatting
        - Source citations where appropriate

      style: ""professional""
      format: ""markdown""
      max_tokens: 4000

  # Save markdown version
  - id: save_markdown
    action: write_file
    parameters:
      path: ""{{ outputs.report_markdown }}""
      content: ""$results.assemble_report""

  # Convert to PDF
  - id: create_pdf
    action: ""!pandoc {{ outputs.report_markdown }} -o {{ outputs.report_pdf }} --pdf-engine=xelatex""
    error_handling:
      continue_on_error: true
      fallback:
        action: write_file
        parameters:
          path: ""{{ outputs.report_pdf }}.txt""
          content: ""PDF generation requires pandoc with xelatex""

  # Convert to HTML
  - id: create_html
    action: ""!pandoc {{ outputs.report_markdown }} -o {{ outputs.report_html }} --standalone --css=style.css""
    error_handling:
      continue_on_error: true","test_created","tests/snippet_tests/test_snippets_batch_10.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","811","843","python","------------------------------------","import orchestrator as orc

# Initialize
orc.init_models()

# Compile report generator
generator = orc.compile(""report_generator.yaml"")

# Generate executive report
exec_report = generator.run(
    topic=""artificial intelligence in healthcare"",
    report_type=""executive"",
    target_audience=""executives"",
    sections=[""executive_summary"", ""key_findings"", ""recommendations""]
)

# Generate technical report
tech_report = generator.run(
    topic=""blockchain scalability solutions"",
    report_type=""technical"",
    target_audience=""technical"",
    sections=[""introduction"", ""technical_analysis"", ""methodology"", ""results""]
)

# Generate standard briefing
briefing = generator.run(
    topic=""cybersecurity threats 2024"",
    report_type=""briefing"",
    target_audience=""general""
)

print(f""Generated reports: {exec_report}, {tech_report}, {briefing}"")","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","854","865","yaml","Create a pipeline that monitors a specific industry for news, updates, and trends:","# Hints for your solution:
inputs:
  industry: # e.g., ""fintech"", ""biotech"", ""cleantech""
  monitoring_period: # ""daily"", ""weekly"", ""monthly""
  alert_keywords: # Important terms to watch for

steps:
  # Multiple search strategies
  # Trend analysis
  # Alert generation
  # Automated summaries","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","873","878","yaml","Build a system that researches competitors and market positioning:","# Structure your pipeline to:
# 1. Research multiple companies
# 2. Compare features and positioning
# 3. Analyze market trends
# 4. Generate competitive analysis","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/tutorials/tutorial_web_research.rst","886","893","python","Create a pipeline that combines multiple research pipelines for comprehensive analysis:","# Combine:
# - Basic web search
# - Multi-source research
# - Fact-checking
# - Report generation

# Into a single meta-pipeline","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","14","67","yaml","A complete pipeline definition consists of several sections, each serving a specific purpose:","# Pipeline metadata
name: pipeline-name           # Required: Unique identifier
description: Pipeline purpose # Required: Human-readable description
version: ""1.0.0""             # Optional: Version tracking

# Input definitions
inputs:
  parameter_name:
    type: string             # Required: string, integer, float, boolean, array, object
    description: Purpose     # Required: What this input does
    required: true          # Optional: Default is false
    default: ""value""        # Optional: Default value if not provided
    validation:             # Optional: Input validation rules
      pattern: ""^[a-z]+$""   # Regex for strings
      min: 0                # Minimum for numbers
      max: 100              # Maximum for numbers
      enum: [""a"", ""b""]      # Allowed values

# Output definitions
outputs:
  result_name:
    type: string            # Required: Output data type
    value: ""expression""     # Required: How to generate the output
    description: Purpose    # Optional: What this output represents

# Configuration
config:
  timeout: 3600             # Optional: Global timeout in seconds
  parallel: true            # Optional: Enable parallel execution
  checkpoint: true          # Optional: Enable checkpointing
  error_mode: ""continue""    # Optional: stop|continue|retry

# Resource requirements
resources:
  gpu: false                # Optional: Require GPU
  memory: ""8GB""             # Optional: Memory requirement
  model_size: ""large""       # Optional: Preferred model size

# Pipeline steps
steps:
  - id: step_identifier     # Required: Unique step ID
    action: action_name     # Required: What to do
    description: Purpose    # Optional: Step description
    parameters:             # Optional: Step parameters
      key: value
    depends_on: [step_id]   # Optional: Dependencies
    condition: expression   # Optional: Conditional execution
    error_handling:         # Optional: Error handling
      retry:
        max_attempts: 3
        backoff: exponential
      fallback:
        action: alternate_action","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","78","87","yaml","The metadata section identifies and describes your pipeline:","name: advanced-research-pipeline
description: |
  Multi-stage research pipeline that:
  - Searches multiple sources
  - Validates information
  - Generates comprehensive reports
version: ""2.1.0""
author: ""Your Name""
tags: [""research"", ""automation"", ""reporting""]","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","97","148","yaml","**Basic Types**:","inputs:
  # String input with validation
  topic:
    type: string
    description: ""Research topic to investigate""
    required: true
    validation:
      pattern: ""^[A-Za-z0-9 ]+$""
      min_length: 3
      max_length: 100

  # Integer with range
  depth:
    type: integer
    description: ""Research depth (1-5)""
    default: 3
    validation:
      min: 1
      max: 5

  # Boolean flag
  include_images:
    type: boolean
    description: ""Include images in report""
    default: false

  # Array of strings
  sources:
    type: array
    description: ""Preferred information sources""
    default: [""web"", ""academic""]
    validation:
      min_items: 1
      max_items: 10
      item_type: string

  # Complex object
  config:
    type: object
    description: ""Advanced configuration""
    default:
      language: ""en""
      format: ""pdf""
    validation:
      properties:
        language:
          type: string
          enum: [""en"", ""es"", ""fr"", ""de""]
        format:
          type: string
          enum: [""pdf"", ""html"", ""markdown""]","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","156","184","yaml","Outputs define what the pipeline produces:","outputs:
  # Simple file output
  report:
    type: string
    value: ""reports/{{ inputs.topic | slugify }}_report.pdf""
    description: ""Generated PDF report""

  # Dynamic output using AUTO
  summary:
    type: string
    value: <AUTO>Generate filename based on content</AUTO>
    description: ""Executive summary document""

  # Computed output
  metrics:
    type: object
    value:
      word_count: ""{{ results.final_report.word_count }}""
      sources_used: ""{{ results.compile_sources.count }}""
      generation_time: ""{{ execution.duration }}""

  # Multiple file outputs
  artifacts:
    type: array
    value:
      - ""{{ outputs.report }}""
      - ""data/{{ inputs.topic }}_data.json""
      - ""images/{{ inputs.topic }}_charts.png""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","194","226","yaml","**Basic Actions**:","steps:
  # Simple action
  - id: fetch_data
    action: fetch_url
    parameters:
      url: ""https://api.example.com/data""

  # Using input values
  - id: search
    action: search_web
    parameters:
      query: ""{{ inputs.topic }} {{ inputs.year }}""
      max_results: ""{{ inputs.depth * 5 }}""

  # Using previous results
  - id: analyze
    action: analyze_data
    parameters:
      data: ""$results.fetch_data""
      method: ""statistical""

  # Shell command (prefix with !)
  - id: convert
    action: ""!pandoc -f markdown -t pdf -o output.pdf input.md""

  # Using AUTO tags
  - id: summarize
    action: generate_summary
    parameters:
      content: ""$results.analyze""
      style: <AUTO>Choose style based on audience</AUTO>
      length: <AUTO>Determine optimal length</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","231","257","yaml","**Dependencies and Flow Control**:","steps:
  # Parallel execution (no dependencies)
  - id: source1
    action: fetch_source_a

  - id: source2
    action: fetch_source_b

  # Sequential execution
  - id: combine
    action: merge_data
    depends_on: [source1, source2]
    parameters:
      data1: ""$results.source1""
      data2: ""$results.source2""

  # Conditional execution
  - id: premium_analysis
    action: advanced_analysis
    condition: ""{{ inputs.tier == 'premium' }}""
    parameters:
      data: ""$results.combine""

  # Dynamic dependencies
  - id: final_step
    depends_on: ""{{ ['combine', 'premium_analysis'] if inputs.tier == 'premium' else ['combine'] }}""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","262","284","yaml","**Error Handling**:","steps:
  - id: risky_operation
    action: external_api_call
    error_handling:
      # Retry configuration
      retry:
        max_attempts: 3
        backoff: exponential  # or: constant, linear
        initial_delay: 1000   # milliseconds
        max_delay: 30000

      # Fallback action
      fallback:
        action: use_cached_data
        parameters:
          cache_key: ""{{ inputs.topic }}""

      # Continue on error
      continue_on_error: true

      # Custom error message
      error_message: ""Failed to fetch external data, using cache""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","294","308","yaml","**Variable Access**:","# Input variables
""{{ inputs.parameter_name }}""

# Results from previous steps
""$results.step_id""
""$results.step_id.specific_field""

# Output references
""{{ outputs.output_name }}""

# Execution context
""{{ execution.timestamp }}""
""{{ execution.pipeline_id }}""
""{{ execution.run_id }}""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","313","332","yaml","**Filters and Functions**:","# String manipulation
""{{ inputs.topic | lower }}""
""{{ inputs.topic | upper }}""
""{{ inputs.topic | slugify }}""
""{{ inputs.topic | replace(' ', '_') }}""

# Date formatting
""{{ execution.timestamp | strftime('%Y-%m-%d') }}""

# Math operations
""{{ inputs.count * 2 }}""
""{{ inputs.value | round(2) }}""

# Conditionals
""{{ 'premium' if inputs.tier == 'gold' else 'standard' }}""

# Lists and loops
""{{ inputs.items | join(', ') }}""
""{{ inputs.sources | length }}""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","342","353","yaml","**Basic AUTO Tags**:","parameters:
  # Simple decision
  style: <AUTO>Choose appropriate writing style</AUTO>

  # Context-aware decision
  method: <AUTO>Based on the data type {{ results.fetch.type }}, choose the best analysis method</AUTO>

  # Multiple choices
  options:
    visualization: <AUTO>Should we create visualizations for this data?</AUTO>
    format: <AUTO>What's the best output format: json, csv, or parquet?</AUTO>","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","358","379","yaml","**Advanced AUTO Patterns**:","# Conditional AUTO
analysis_depth: |
  <AUTO>
  Given:
  - Data size: {{ results.fetch.size }}
  - Time constraint: {{ inputs.deadline }}
  - Importance: {{ inputs.priority }}

  Determine the appropriate analysis depth (1-10)
  </AUTO>

# Structured AUTO
report_sections: |
  <AUTO>
  For a report about {{ inputs.topic }}, determine which sections to include:
  - Executive Summary: yes/no
  - Technical Details: yes/no
  - Future Outlook: yes/no
  - Recommendations: yes/no
  Return as JSON object
  </AUTO>","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","398","419","python","**User Control Points**:","import orchestrator as orc

# Control compilation options
pipeline = orc.compile(
    ""pipeline.yaml"",
    # Override config values
    config={
        ""timeout"": 7200,
        ""checkpoint"": True
    },
    # Set compilation flags
    strict=True,           # Strict validation
    optimize=True,         # Enable optimizations
    dry_run=False,         # Actually compile (not just validate)
    debug=True            # Include debug information
)

# Inspect compilation result
print(pipeline.get_required_tools())
print(pipeline.get_task_graph())
print(pipeline.get_estimated_cost())","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","424","434","yaml","**Runtime vs Compile-Time Resolution**:","# Compile-time (resolved during compilation)
config:
  timestamp: ""{{ compile_time.timestamp }}""

# Runtime (resolved during execution)
steps:
  - id: dynamic
    parameters:
      query: ""{{ inputs.topic }}""  # Runtime
      results: ""$results.previous""  # Runtime","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","445","464","yaml","Reuse common patterns:","imports:
  # Import specific steps
  - common/data_validation.yaml#validate_step as validate

  # Import entire pipeline
  - workflows/standard_analysis.yaml as analysis

steps:
  # Use imported step
  - id: validation
    extends: validate
    parameters:
      data: ""$results.fetch""

  # Use imported pipeline
  - id: analyze
    pipeline: analysis
    inputs:
      data: ""$results.validation""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","470","498","yaml","-------------------------","steps:
  # Define parallel group
  - id: parallel_fetch
    parallel:
      - id: fetch_api
        action: fetch_url
        parameters:
          url: ""{{ inputs.api_url }}""

      - id: fetch_db
        action: query_database
        parameters:
          query: ""{{ inputs.db_query }}""

      - id: fetch_file
        action: read_file
        parameters:
          path: ""{{ inputs.file_path }}""

  # Use results from parallel group
  - id: merge
    action: combine_data
    depends_on: [parallel_fetch]
    parameters:
      sources:
        - ""$results.parallel_fetch.fetch_api""
        - ""$results.parallel_fetch.fetch_db""
        - ""$results.parallel_fetch.fetch_file""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","504","521","yaml","-------------------","steps:
  # For-each loop
  - id: process_items
    for_each: ""{{ inputs.items }}""
    as: item
    action: process_single_item
    parameters:
      data: ""{{ item }}""
      index: ""{{ loop.index }}""

  # While loop
  - id: iterative_refinement
    while: ""{{ results.quality_check.score < 0.95 }}""
    max_iterations: 10
    action: refine_result
    parameters:
      current: ""$results.previous_iteration""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","527","541","yaml","----------------","# Enable checkpointing
config:
  checkpoint:
    enabled: true
    frequency: ""after_each_step""  # or: ""every_n_steps: 5""
    storage: ""postgresql""         # or: ""redis"", ""filesystem""

steps:
  - id: long_running
    action: expensive_computation
    checkpoint: true  # Force checkpoint after this step
    recovery:
      strategy: ""retry""  # or: ""skip"", ""use_cached""
      max_attempts: 3","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","578","638","yaml","------------------------","name: data-processing-pipeline
description: ETL pipeline with validation

inputs:
  source_url:
    type: string
    required: true

  output_format:
    type: string
    default: ""parquet""
    validation:
      enum: [""csv"", ""json"", ""parquet""]

steps:
  # Extract
  - id: extract
    action: fetch_data
    parameters:
      url: ""{{ inputs.source_url }}""
      format: <AUTO>Detect format from URL</AUTO>

  # Transform
  - id: clean
    action: clean_data
    parameters:
      data: ""$results.extract""
      rules:
        - remove_duplicates: true
        - handle_missing: ""interpolate""
        - standardize_dates: true

  - id: transform
    action: transform_data
    parameters:
      data: ""$results.clean""
      operations:
        - type: ""aggregate""
          group_by: [""category""]
          metrics: [""sum"", ""avg""]

  # Load
  - id: validate
    action: validate_data
    parameters:
      data: ""$results.transform""
      schema:
        type: ""dataframe""
        columns:
          - name: ""category""
            type: ""string""
          - name: ""total""
            type: ""float""

  - id: save
    action: save_data
    parameters:
      data: ""$results.validate""
      path: ""output/processed_data.{{ inputs.output_format }}""
      format: ""{{ inputs.output_format }}""","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
"docs_sphinx/yaml_pipelines.rst","644","703","yaml","------------------------------","name: comprehensive-research
description: Research from multiple sources with cross-validation

inputs:
  topic:
    type: string
    required: true

  sources:
    type: array
    default: [""web"", ""academic"", ""news""]

steps:
  # Parallel source fetching
  - id: fetch_sources
    parallel:
      - id: web_search
        condition: ""'web' in inputs.sources""
        action: search_web
        parameters:
          query: ""{{ inputs.topic }}""
          max_results: 20

      - id: academic_search
        condition: ""'academic' in inputs.sources""
        action: search_academic
        parameters:
          query: ""{{ inputs.topic }}""
          databases: [""arxiv"", ""pubmed"", ""scholar""]

      - id: news_search
        condition: ""'news' in inputs.sources""
        action: search_news
        parameters:
          query: ""{{ inputs.topic }}""
          date_range: ""last_30_days""

  # Process and validate
  - id: extract_facts
    action: extract_information
    parameters:
      sources: ""$results.fetch_sources""
      extract:
        - facts
        - claims
        - statistics

  - id: cross_validate
    action: validate_claims
    parameters:
      claims: ""$results.extract_facts.claims""
      require_sources: 2  # Need 2+ sources to confirm

  # Generate report
  - id: synthesize
    action: generate_synthesis
    parameters:
      validated_facts: ""$results.cross_validate""
      style: ""analytical""
      include_confidence: true","test_created","tests/snippet_tests/test_snippets_batch_11.py",""
