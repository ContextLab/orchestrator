{
  "example": "interactive_chat_bot.yaml",
  "status": "error",
  "error": "Failed to compile YAML: Failed to render template '<AUTO>generate helpful conversational response:\n      User message: {{safety_check.result.filtered_content}}\n      Conversation context: {{retrieve_context.result}}\n      Intent: {{classify_intent.result}}\n      Tool results: {{execute_tools.result || []}}\n      Persona: {{persona}}\n      Max length: {{max_response_length}} tokens\n      \n      Create response that:\n      1. Directly addresses the user's query\n      2. Incorporates tool results naturally\n      3. Maintains conversation flow\n      4. Matches the persona style\n      5. Shows appropriate empathy/emotion\n      6. Includes relevant follow-up suggestions\n      \n      Tone: Match user's formality level\n      Language: {{process_input.result.language}}\n      \n      Return response with metadata</AUTO>': expected token 'name', got '|'",
  "error_type": "YAMLCompilerError",
  "traceback": "Traceback (most recent call last):\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 148, in process_value\n    template = self.template_engine.from_string(value)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 1108, in from_string\n    return cls.from_code(self, self.compile(source), gs, None)\n                               ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 768, in compile\n    self.handle_exception(source=source_hint)\n  File \"/Users/jmanning/miniconda3/lib/python3.12/site-packages/jinja2/environment.py\", line 939, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"<unknown>\", line 5, in template\njinja2.exceptions.TemplateSyntaxError: expected token 'name', got '|'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 94, in compile\n    processed = self._process_templates(raw_pipeline, context or {})\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 179, in _process_templates\n    return process_value(pipeline_def)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 174, in process_value\n    return {k: process_value(v) for k, v in value.items()}\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 176, in process_value\n    return [process_value(item) for item in value]\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 174, in process_value\n    return {k: process_value(v) for k, v in value.items()}\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 170, in process_value\n    raise TemplateRenderError(\norchestrator.compiler.yaml_compiler.TemplateRenderError: Failed to render template '<AUTO>generate helpful conversational response:\n      User message: {{safety_check.result.filtered_content}}\n      Conversation context: {{retrieve_context.result}}\n      Intent: {{classify_intent.result}}\n      Tool results: {{execute_tools.result || []}}\n      Persona: {{persona}}\n      Max length: {{max_response_length}} tokens\n      \n      Create response that:\n      1. Directly addresses the user's query\n      2. Incorporates tool results naturally\n      3. Maintains conversation flow\n      4. Matches the persona style\n      5. Shows appropriate empathy/emotion\n      6. Includes relevant follow-up suggestions\n      \n      Tone: Match user's formality level\n      Language: {{process_input.result.language}}\n      \n      Return response with metadata</AUTO>': expected token 'name', got '|'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/jmanning/orchestrator/test_all_examples_detailed.py\", line 123, in test_example\n    result = await self.orchestrator.execute_yaml(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/orchestrator/src/orchestrator/orchestrator.py\", line 406, in execute_yaml\n    pipeline = await self.yaml_compiler.compile(yaml_content, context)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jmanning/orchestrator/src/orchestrator/compiler/yaml_compiler.py\", line 106, in compile\n    raise YAMLCompilerError(f\"Failed to compile YAML: {e}\") from e\norchestrator.compiler.yaml_compiler.YAMLCompilerError: Failed to compile YAML: Failed to render template '<AUTO>generate helpful conversational response:\n      User message: {{safety_check.result.filtered_content}}\n      Conversation context: {{retrieve_context.result}}\n      Intent: {{classify_intent.result}}\n      Tool results: {{execute_tools.result || []}}\n      Persona: {{persona}}\n      Max length: {{max_response_length}} tokens\n      \n      Create response that:\n      1. Directly addresses the user's query\n      2. Incorporates tool results naturally\n      3. Maintains conversation flow\n      4. Matches the persona style\n      5. Shows appropriate empathy/emotion\n      6. Includes relevant follow-up suggestions\n      \n      Tone: Match user's formality level\n      Language: {{process_input.result.language}}\n      \n      Return response with metadata</AUTO>': expected token 'name', got '|'\n",
  "inputs": {
    "message": "Can you help me understand how neural networks learn?",
    "conversation_id": "conv_12345",
    "persona": "knowledgeable-teacher",
    "enable_streaming": false,
    "safety_level": "moderate",
    "available_tools": [
      "web_search",
      "calculator"
    ],
    "max_response_length": 500
  },
  "timestamp": "2025-07-17T08:20:20.308485"
}