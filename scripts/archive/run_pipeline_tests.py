#!/usr/bin/env python3
"""Run all pipeline examples with real models and verify quality."""

import asyncio
import os
import sys
from pathlib import Path
from datetime import datetime
import yaml

# Add the src directory to Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from orchestrator import Orchestrator
from orchestrator.integrations.openai_model import OpenAIModel
from orchestrator.integrations.anthropic_model import AnthropicModel
from orchestrator.core.control_system import MockControlSystem
from orchestrator.core.task import Task
from orchestrator.core.model import Model, ModelCapabilities


class PipelineTestControlSystem(MockControlSystem):
    """Control system for testing pipelines with real functionality."""
    
    def __init__(self, output_dir: str = "./reports"):
        super().__init__(name="pipeline-test-control")
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self._results = {}
        
        # Import tools
        from orchestrator.tools.web_tools import WebSearchTool
        from orchestrator.tools.data_tools import DataProcessingTool
        from orchestrator.tools.report_tools import ReportGeneratorTool
        
        # Initialize tools
        self.tools = {
            "web_search": WebSearchTool(),
            "data_processor": DataProcessingTool(),
            "report_generator": ReportGeneratorTool()
        }
        
    async def execute_task(self, task: Task, context: dict = None) -> dict:
        """Execute task with real functionality."""
        print(f"\nâš™ï¸  Executing task: {task.id} ({task.action})")
        
        # Resolve references
        self._resolve_references(task, context)
        
        # Route to appropriate handler
        try:
            if task.action in ["search", "search_web"]:
                result = await self._search_web(task, context)
            elif task.action in ["analyze", "analyze_data"]:
                result = await self._analyze_data(task, context)
            elif task.action in ["summarize", "generate_summary"]:
                result = await self._summarize(task, context)
            elif task.action in ["generate_report", "create_report"]:
                result = await self._generate_report(task, context)
            else:
                result = {"status": "completed", "message": f"Executed {task.action}"}
                
            # Store result
            self._results[task.id] = result
            return result
            
        except Exception as e:
            print(f"âŒ Task execution failed: {e}")
            import traceback
            traceback.print_exc()
            error_result = {"status": "failed", "error": str(e)}
            self._results[task.id] = error_result
            return error_result
    
    def _resolve_references(self, task: Task, context: dict):
        """Resolve $results and template references."""
        if not task.parameters:
            return
            
        for key, value in task.parameters.items():
            if isinstance(value, str):
                # Handle $results references
                if value.startswith("$results."):
                    parts = value.split(".")
                    if len(parts) >= 2:
                        task_id = parts[1]
                        if task_id in self._results:
                            result = self._results[task_id]
                            for part in parts[2:]:
                                if isinstance(result, dict) and part in result:
                                    result = result[part]
                                else:
                                    result = None
                                    break
                            task.parameters[key] = result
                
                # Handle template variables
                elif "{{" in value and "}}" in value:
                    # Simple template replacement
                    if context and 'inputs' in context:
                        for input_key, input_value in context['inputs'].items():
                            value = value.replace(f"{{{{ {input_key} }}}}", str(input_value))
                            value = value.replace(f"{{{{ inputs.{input_key} }}}}", str(input_value))
                    task.parameters[key] = value
    
    async def _search_web(self, task: Task, context: dict) -> dict:
        """Perform web search using real WebSearchTool."""
        query = task.parameters.get("query", "")
        max_results = task.parameters.get("max_results", 5)
        
        print(f"   ğŸ” Searching: '{query}' (max_results: {max_results})")
        
        try:
            result = await self.tools["web_search"].execute(
                query=query,
                max_results=max_results
            )
            
            if result.get("success"):
                print(f"   âœ… Found {len(result.get('results', []))} results")
                
                # Save results to file
                search_file = self.output_dir / f"search_results_{task.id}.json"
                import json
                with open(search_file, "w") as f:
                    json.dump(result, f, indent=2)
                
                return {
                    "success": True,
                    "query": query,
                    "results": result.get("results", []),
                    "count": len(result.get("results", [])),
                    "file": str(search_file)
                }
            else:
                print(f"   âŒ Search failed: {result.get('error', 'Unknown error')}")
                return {
                    "success": False,
                    "error": result.get("error", "Search failed"),
                    "query": query
                }
                
        except Exception as e:
            print(f"   âŒ Search error: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query
            }
    
    async def _analyze_data(self, task: Task, context: dict) -> dict:
        """Analyze search data."""
        data = task.parameters.get("data", {})
        depth = task.parameters.get("depth", "basic")
        
        print(f"   ğŸ“Š Analyzing data (depth: {depth})")
        
        # Extract key information from search results
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "analysis_depth": depth,
            "insights": [],
            "themes": [],
            "quality_metrics": {}
        }
        
        if isinstance(data, dict) and "results" in data:
            results = data["results"]
            
            # Basic analysis
            analysis["insights"].append(f"Found {len(results)} search results")
            
            # Source diversity
            domains = set()
            for result in results:
                url = result.get("url", "")
                if url:
                    from urllib.parse import urlparse
                    domain = urlparse(url).netloc
                    domains.add(domain)
            
            analysis["insights"].append(f"Information from {len(domains)} different domains")
            
            # Content quality indicators
            high_relevance = sum(1 for r in results if r.get("relevance", 0) > 0.8)
            if high_relevance > 0:
                analysis["insights"].append(f"{high_relevance} highly relevant sources found")
            
            # Theme extraction (simple keyword analysis)
            all_text = " ".join([r.get("title", "") + " " + r.get("snippet", "") for r in results])
            words = all_text.lower().split()
            word_freq = {}
            for word in words:
                if len(word) > 4:  # Focus on meaningful words
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            # Top themes
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
            analysis["themes"] = [word for word, freq in top_words if freq > 1]
            
            # Quality metrics
            analysis["quality_metrics"] = {
                "total_results": len(results),
                "unique_domains": len(domains),
                "high_relevance_count": high_relevance,
                "average_relevance": sum(r.get("relevance", 0) for r in results) / len(results) if results else 0
            }
        
        # Save analysis
        analysis_file = self.output_dir / f"analysis_{task.id}.json"
        import json
        with open(analysis_file, "w") as f:
            json.dump(analysis, f, indent=2)
        
        print(f"   âœ… Analysis complete: {len(analysis['insights'])} insights, {len(analysis['themes'])} themes")
        
        return {
            "success": True,
            "analysis": analysis,
            "insights": analysis["insights"],
            "themes": analysis["themes"],
            "quality_metrics": analysis["quality_metrics"],
            "file": str(analysis_file)
        }
    
    async def _summarize(self, task: Task, context: dict) -> dict:
        """Generate summary using real ReportGeneratorTool."""
        content = task.parameters.get("content", {})
        format_type = task.parameters.get("format", "markdown")
        
        print(f"   ğŸ“ Generating summary (format: {format_type})")
        
        # Extract data for summarization
        if isinstance(content, dict):
            analysis = content.get("analysis", {})
            insights = content.get("insights", [])
            themes = content.get("themes", [])
            quality_metrics = content.get("quality_metrics", {})
        else:
            # Fallback for other content types
            analysis = {"summary": str(content)}
            insights = ["Content provided for summarization"]
            themes = []
            quality_metrics = {}
        
        # Generate summary using report tool
        try:
            summary_content = f"""
# Analysis Summary

## Key Insights
{chr(10).join(f"- {insight}" for insight in insights)}

## Main Themes
{chr(10).join(f"- {theme}" for theme in themes)}

## Quality Assessment
- Total results analyzed: {quality_metrics.get('total_results', 'N/A')}
- Unique sources: {quality_metrics.get('unique_domains', 'N/A')}
- High relevance sources: {quality_metrics.get('high_relevance_count', 'N/A')}
- Average relevance: {quality_metrics.get('average_relevance', 0):.2f}

## Conclusion
This analysis provides insights into the searched topic based on {quality_metrics.get('total_results', 'available')} sources.
"""
            
            # Save summary
            summary_file = self.output_dir / f"summary_{task.id}.md"
            with open(summary_file, "w") as f:
                f.write(summary_content)
            
            print(f"   âœ… Summary generated: {len(summary_content.split())} words")
            
            return {
                "success": True,
                "summary": summary_content,
                "format": format_type,
                "word_count": len(summary_content.split()),
                "file": str(summary_file)
            }
            
        except Exception as e:
            print(f"   âŒ Summary generation failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _generate_report(self, task: Task, context: dict) -> dict:
        """Generate comprehensive report."""
        title = task.parameters.get("title", "Pipeline Test Report")
        data = task.parameters.get("data", {})
        template = task.parameters.get("template", "research")
        
        print(f"   ğŸ“„ Generating report: '{title}' (template: {template})")
        
        try:
            # Use the real ReportGeneratorTool
            result = await self.tools["report_generator"].execute(
                title=title,
                template=template,
                data=data,
                findings=["Pipeline executed successfully", "Real tools integrated", "Data processed and analyzed"],
                recommendations=["Review generated outputs", "Verify data quality", "Optimize pipeline parameters"]
            )
            
            if result.get("success"):
                # Save report
                report_file = self.output_dir / f"report_{task.id}.md"
                with open(report_file, "w") as f:
                    f.write(result["markdown"])
                
                print(f"   âœ… Report generated: {result['word_count']} words")
                
                return {
                    "success": True,
                    "markdown": result["markdown"],
                    "word_count": result["word_count"],
                    "template": template,
                    "file": str(report_file)
                }
            else:
                print(f"   âŒ Report generation failed: {result.get('error', 'Unknown error')}")
                return {
                    "success": False,
                    "error": result.get("error", "Report generation failed")
                }
                
        except Exception as e:
            print(f"   âŒ Report error: {e}")
            return {
                "success": False,
                "error": str(e)
            }


class MockAutoModel(Model):
    """Mock model for AUTO tag resolution."""
    
    def __init__(self):
        capabilities = ModelCapabilities(
            supported_tasks=["reasoning", "generation"],
            context_window=4096,
            languages=["en"]
        )
        super().__init__(
            name="Mock Auto Model",
            provider="mock",
            capabilities=capabilities
        )
    
    async def generate(self, prompt, **kwargs):
        # Return appropriate responses for AUTO tags
        if "depth" in prompt.lower():
            return "comprehensive"
        elif "format" in prompt.lower():
            return "markdown"
        elif "style" in prompt.lower():
            return "professional"
        elif "max_results" in prompt.lower():
            return "10"
        return "auto"
    
    async def generate_structured(self, prompt, schema, **kwargs):
        return {"value": await self.generate(prompt, **kwargs)}
    
    async def validate_response(self, response, schema):
        return True
    
    def estimate_tokens(self, text):
        return len(text.split())
    
    def estimate_cost(self, input_tokens, output_tokens):
        return 0.0
    
    async def health_check(self):
        return True


async def run_pipeline_test(pipeline_file: Path, test_context: dict, output_subdir: str):
    """Run a single pipeline test with real models."""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª Testing Pipeline: {pipeline_file.name}")
    print(f"{'='*60}")
    
    # Create output directory for this test
    test_output_dir = Path("reports") / output_subdir
    
    # Read pipeline YAML
    with open(pipeline_file, 'r') as f:
        pipeline_yaml = f.read()
    
    print(f"ğŸ“‹ Pipeline: {pipeline_file.name}")
    print(f"ğŸ“ Output: {test_output_dir}")
    
    # Initialize control system
    control_system = PipelineTestControlSystem(output_dir=str(test_output_dir))
    
    # Initialize orchestrator
    orchestrator = Orchestrator(control_system=control_system)
    
    # Register models if API keys available
    config = {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY")
    }
    
    models_registered = 0
    
    if config["openai_api_key"]:
        try:
            gpt4 = OpenAIModel(
                model_name="gpt-4",
                api_key=config["openai_api_key"]
            )
            orchestrator.model_registry.register_model(gpt4)
            models_registered += 1
            print("âœ… Registered OpenAI GPT-4")
        except Exception as e:
            print(f"âš ï¸  Failed to register OpenAI: {e}")
    
    if config["anthropic_api_key"]:
        try:
            claude = AnthropicModel(
                model_name="claude-3-sonnet-20240229",
                api_key=config["anthropic_api_key"]
            )
            orchestrator.model_registry.register_model(claude)
            models_registered += 1
            print("âœ… Registered Anthropic Claude")
        except Exception as e:
            print(f"âš ï¸  Failed to register Anthropic: {e}")
    
    # Register mock model for AUTO resolution
    mock_model = MockAutoModel()
    orchestrator.model_registry.register_model(mock_model)
    orchestrator.yaml_compiler.ambiguity_resolver.model = mock_model
    
    if models_registered == 0:
        print("âš ï¸  No real models available, using mock implementations")
    
    try:
        # Execute pipeline
        print("\nğŸš€ Executing pipeline...")
        start_time = datetime.now()
        
        results = await orchestrator.execute_yaml(
            pipeline_yaml,
            context=test_context
        )
        
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"\nâœ… Pipeline completed in {execution_time:.2f} seconds")
        
        # Generate test summary
        summary = {
            "pipeline_file": str(pipeline_file),
            "execution_time": execution_time,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "test_context": test_context,
            "models_used": models_registered,
            "results": results,
            "success": True
        }
        
        # Save test summary
        summary_file = test_output_dir / "test_summary.json"
        import json
        with open(summary_file, "w") as f:
            json.dump(summary, f, indent=2, default=str)
        
        print(f"ğŸ“Š Test summary saved: {summary_file}")
        
        # Print key results
        print("\nğŸ“‹ Key Results:")
        for step_id, result in results.items():
            if isinstance(result, dict):
                if result.get("success"):
                    print(f"   âœ… {step_id}: Success")
                    if "word_count" in result:
                        print(f"      ğŸ“ {result['word_count']} words")
                    if "count" in result:
                        print(f"      ğŸ“Š {result['count']} items")
                else:
                    print(f"   âŒ {step_id}: {result.get('error', 'Failed')}")
            else:
                print(f"   â„¹ï¸  {step_id}: {str(result)[:50]}...")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Pipeline execution failed: {e}")
        import traceback
        traceback.print_exc()
        
        # Save error summary
        error_summary = {
            "pipeline_file": str(pipeline_file),
            "test_context": test_context,
            "error": str(e),
            "success": False,
            "timestamp": datetime.now().isoformat()
        }
        
        error_file = test_output_dir / "error_summary.json"
        import json
        with open(error_file, "w") as f:
            json.dump(error_summary, f, indent=2)
        
        return False


async def main():
    """Run all pipeline tests."""
    print("ğŸ§ª Orchestrator Pipeline Quality Verification")
    print("=" * 60)
    
    # Test configurations
    test_configs = [
        {
            "pipeline": "examples/pipelines/simple_research.yaml",
            "context": {"topic": "AI agent frameworks"},
            "output_dir": "simple_research_test"
        },
        {
            "pipeline": "examples/pipelines/research_assistant.yaml", 
            "context": {"topic": "latest developments in quantum computing", "depth": "standard"},
            "output_dir": "research_assistant_test"
        }
    ]
    
    # Check which pipelines exist
    available_tests = []
    for test_config in test_configs:
        pipeline_file = Path(test_config["pipeline"])
        if pipeline_file.exists():
            available_tests.append(test_config)
        else:
            print(f"âš ï¸  Pipeline not found: {pipeline_file}")
    
    if not available_tests:
        print("âŒ No test pipelines available")
        return
    
    print(f"ğŸ“‹ Found {len(available_tests)} pipeline tests to run")
    
    # Run tests
    results = []
    for i, test_config in enumerate(available_tests, 1):
        print(f"\nğŸ”„ Running test {i}/{len(available_tests)}")
        
        success = await run_pipeline_test(
            Path(test_config["pipeline"]),
            test_config["context"],
            test_config["output_dir"]
        )
        
        results.append({
            "pipeline": test_config["pipeline"],
            "success": success,
            "output_dir": test_config["output_dir"]
        })
    
    # Final summary
    print(f"\n{'='*60}")
    print("ğŸ FINAL TEST SUMMARY")
    print(f"{'='*60}")
    
    successful = sum(1 for r in results if r["success"])
    print(f"âœ… Successful tests: {successful}/{len(results)}")
    
    for result in results:
        status = "âœ… PASS" if result["success"] else "âŒ FAIL"
        print(f"   {status} {Path(result['pipeline']).name}")
        print(f"        ğŸ“ Output: reports/{result['output_dir']}")
    
    if successful == len(results):
        print("\nğŸ‰ All pipeline tests passed!")
    else:
        print(f"\nâš ï¸  {len(results) - successful} tests failed")


if __name__ == "__main__":
    asyncio.run(main())