[
  {
    "file": "CLAUDE.md",
    "line_start": 49,
    "line_end": 59,
    "type": "text",
    "description": "- Redis - For caching and session management",
    "content": "orchestrator/\n├── src/orchestrator/          # Core library code\n│   ├── compiler/             # YAML parsing and compilation\n│   ├── models/               # Model abstractions and registry\n│   ├── adapters/             # Control system adapters\n│   ├── executor/             # Sandboxed execution\n│   └── state/                # State management\n├── tests/                    # Unit and integration tests\n├── examples/                 # Example pipeline definitions\n├── docs/                     # Documentation\n└── config/                   # Configuration schemas",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "CLAUDE.md",
    "line_start": 67,
    "line_end": 73,
    "type": "yaml",
    "description": "Pipelines use <AUTO> tags for LLM-resolved ambiguities:",
    "content": "steps:\n  - id: analyze_data\n    action: analyze\n    parameters:\n      data: \"{{ input_data }}\"\n      method: <AUTO>Choose best analysis method for this data type</AUTO>\n      depth: <AUTO>Determine analysis depth based on data complexity</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 32,
    "line_end": 32,
    "type": "bash",
    "description": "- 💾 Lazy Model Loading: Models are downloaded only when needed, saving disk space",
    "content": "pip install py-orc",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 37,
    "line_end": 40,
    "type": "bash",
    "description": "For additional features:",
    "content": "pip install py-orc[ollama]      # Ollama model support\npip install py-orc[cloud]        # Cloud model providers\npip install py-orc[dev]          # Development tools\npip install py-orc[all]          # Everything",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 48,
    "line_end": 66,
    "type": "yaml",
    "description": "1. Create a simple pipeline (hello_world.yaml):",
    "content": "id: hello_world\nname: Hello World Pipeline\ndescription: A simple example pipeline\n\nsteps:\n  - id: greet\n    action: generate_text\n    parameters:\n      prompt: \"Say hello to the world in a creative way!\"\n\n  - id: translate\n    action: generate_text\n    parameters:\n      prompt: \"Translate this greeting to Spanish: {{ greet.result }}\"\n    dependencies: [greet]\n\noutputs:\n  greeting: \"{{ greet.result }}\"\n  spanish: \"{{ translate.result }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 72,
    "line_end": 81,
    "type": "python",
    "description": "2. Run the pipeline:",
    "content": "import orchestrator as orc\n\n# Initialize models (auto-detects available models)\norc.init_models()\n\n# Compile and run the pipeline\npipeline = orc.compile(\"hello_world.yaml\")\nresult = pipeline.run()\n\nprint(result)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 89,
    "line_end": 95,
    "type": "yaml",
    "description": "Orchestrator's <AUTO> tags let AI decide configuration details:",
    "content": "steps:\n  - id: analyze_data\n    action: analyze\n    parameters:\n      data: \"{{ input_data }}\"\n      method: <AUTO>Choose the best analysis method for this data type</AUTO>\n      visualization: <AUTO>Decide if we should create a chart</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 103,
    "line_end": 125,
    "type": "yaml",
    "description": "Configure available models in models.yaml:",
    "content": "models:\n  # Local models (via Ollama) - downloaded on first use\n  - source: ollama\n    name: llama3.1:8b\n    expertise: [general, reasoning, multilingual]\n    size: 8b\n\n  - source: ollama\n    name: qwen2.5-coder:7b\n    expertise: [code, programming]\n    size: 7b\n\n  # Cloud models\n  - source: openai\n    name: gpt-4o\n    expertise: [general, reasoning, code, analysis, vision]\n    size: 1760b  # Estimated\n\ndefaults:\n  expertise_preferences:\n    code: qwen2.5-coder:7b\n    reasoning: deepseek-r1:8b\n    fast: llama3.2:1b",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 135,
    "line_end": 192,
    "type": "yaml",
    "description": "Here's a more complex example showing model requirements and parallel execution:",
    "content": "id: research_pipeline\nname: AI Research Pipeline\ndescription: Research a topic and create a comprehensive report\n\ninputs:\n  - name: topic\n    type: string\n    description: Research topic\n\n  - name: depth\n    type: string\n    default: <AUTO>Determine appropriate research depth</AUTO>\n\nsteps:\n  # Parallel research from multiple sources\n  - id: web_search\n    action: search_web\n    parameters:\n      query: \"{{ topic }} latest research 2025\"\n      count: <AUTO>Decide how many results to fetch</AUTO>\n    requires_model:\n      expertise: [research, web]\n\n  - id: academic_search\n    action: search_academic\n    parameters:\n      query: \"{{ topic }}\"\n      filters: <AUTO>Set appropriate academic filters</AUTO>\n    requires_model:\n      expertise: [research, academic]\n\n  # Analyze findings with specialized model\n  - id: analyze_findings\n    action: analyze\n    parameters:\n      web_results: \"{{ web_search.results }}\"\n      academic_results: \"{{ academic_search.results }}\"\n      analysis_focus: <AUTO>Determine key aspects to analyze</AUTO>\n    dependencies: [web_search, academic_search]\n    requires_model:\n      expertise: [analysis, reasoning]\n      min_size: 20b  # Require large model for complex analysis\n\n  # Generate report\n  - id: write_report\n    action: generate_document\n    parameters:\n      topic: \"{{ topic }}\"\n      analysis: \"{{ analyze_findings.result }}\"\n      style: <AUTO>Choose appropriate writing style</AUTO>\n      length: <AUTO>Determine optimal report length</AUTO>\n    dependencies: [analyze_findings]\n    requires_model:\n      expertise: [writing, general]\n\noutputs:\n  report: \"{{ write_report.document }}\"\n  summary: \"{{ analyze_findings.summary }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 200,
    "line_end": 274,
    "type": "yaml",
    "description": "Here's a fully functional pipeline that generates research reports:",
    "content": "# research_report.yaml\nid: research_report\nname: Research Report Generator\ndescription: Generate comprehensive research reports with citations\n\ninputs:\n  - name: topic\n    type: string\n    description: Research topic\n  - name: instructions\n    type: string\n    description: Additional instructions for the report\n\noutputs:\n  - pdf: <AUTO>Generate appropriate filename for the research report PDF</AUTO>\n\nsteps:\n  - id: search\n    name: Web Search\n    action: search_web\n    parameters:\n      query: <AUTO>Create effective search query for {topic} with {instructions}</AUTO>\n      max_results: 10\n    requires_model:\n      expertise: fast\n\n  - id: compile_notes\n    name: Compile Research Notes\n    action: generate_text\n    parameters:\n      prompt: |\n        Compile comprehensive research notes from these search results:\n        {{ search.results }}\n\n        Topic: {{ topic }}\n        Instructions: {{ instructions }}\n\n        Create detailed notes with:\n        - Key findings\n        - Important quotes\n        - Source citations\n        - Relevant statistics\n    dependencies: [search]\n    requires_model:\n      expertise: [analysis, reasoning]\n      min_size: 7b\n\n  - id: write_report\n    name: Write Report\n    action: generate_document\n    parameters:\n      content: |\n        Write a comprehensive research report on \"{{ topic }}\"\n\n        Research notes:\n        {{ compile_notes.result }}\n\n        Requirements:\n        - Professional academic style\n        - Include introduction, body sections, and conclusion\n        - Cite sources properly\n        - {{ instructions }}\n      format: markdown\n    dependencies: [compile_notes]\n    requires_model:\n      expertise: [writing, general]\n      min_size: 20b\n\n  - id: create_pdf\n    name: Create PDF\n    action: convert_to_pdf\n    parameters:\n      markdown: \"{{ write_report.document }}\"\n      filename: \"{{ outputs.pdf }}\"\n    dependencies: [write_report]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 280,
    "line_end": 294,
    "type": "python",
    "description": "Run it with:",
    "content": "import orchestrator as orc\n\n# Initialize models\norc.init_models()\n\n# Compile pipeline\npipeline = orc.compile(\"research_report.yaml\")\n\n# Run with inputs\nresult = pipeline.run(\n    topic=\"quantum computing applications in medicine\",\n    instructions=\"Focus on recent breakthroughs and future potential\"\n)\n\nprint(f\"Report saved to: {result}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "README.md",
    "line_start": 353,
    "line_end": 359,
    "type": "bibtex",
    "description": "If you use Orchestrator in your research, please cite:",
    "content": "@software{orchestrator2025,\n  title = {Orchestrator: AI Pipeline Orchestration Framework},\n  author = {Manning, Jeremy R. and {Contextual Dynamics Lab}},\n  year = {2025},\n  url = {https://github.com/ContextLab/orchestrator},\n  organization = {Dartmouth College}\n}",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 12,
    "line_end": 47,
    "type": "text",
    "description": "The Orchestrator is a Python library that provides a unified interface for executing AI pipelines defined in YAML with automatic ambiguity resolution using LLMs. It transparently integrates multiple c",
    "content": "┌─────────────────────────────────────────────────────────────────┐\n│                         User Interface                          │\n│                    (YAML Pipeline Definition)                   │\n└───────────────────────┬─────────────────────────────────────────┘\n                        │\n┌───────────────────────▼───────────────────────────────────────┐\n│                    YAML Parser & Compiler                     │\n│  ┌─────────────┐  ┌──────────────┐  ┌────────────────────┐    │\n│  │ Schema      │  │ Ambiguity    │  │ Pipeline           │    │\n│  │ Validator   │  │ Detector     │  │ Optimizer          │    │\n│  └─────────────┘  └──────────────┘  └────────────────────┘    │\n└───────────────────────┬───────────────────────────────────────┘\n                        │\n┌───────────────────────▼───────────────────────────────────────┐\n│                  Orchestration Engine                         │\n│  ┌─────────────┐  ┌──────────────┐  ┌────────────────────┐    │\n│  │ Task        │  │ Dependency   │  │ Resource           │    │\n│  │ Scheduler   │  │ Manager      │  │ Allocator          │    │\n│  └─────────────┘  └──────────────┘  └────────────────────┘    │\n└───────────────────────┬───────────────────────────────────────┘\n                        │\n┌───────────────────────▼───────────────────────────────────────┐\n│                  Control System Adapters                      │\n│  ┌─────────────┐  ┌──────────────┐  ┌───────────────────┐     │\n│  │ LangGraph   │  │ MCP          │  │ Custom            │     │\n│  │ Adapter     │  │ Adapter      │  │ Adapters          │     │\n│  └─────────────┘  └──────────────┘  └───────────────────┘     │\n└───────────────────────┬───────────────────────────────────────┘\n                        │\n┌───────────────────────▼───────────────────────────────────────┐\n│                    Execution Layer                            │\n│  ┌─────────────┐  ┌──────────────┐  ┌────────────────────┐    │\n│  │ Sandboxed   │  │ Model        │  │ State              │    │\n│  │ Executors   │  │ Registry     │  │ Persistence        │    │\n│  └─────────────┘  └──────────────┘  └────────────────────┘    │\n└───────────────────────────────────────────────────────────────┘",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 63,
    "line_end": 89,
    "type": "python",
    "description": "5. Security by Default: Sandboxed execution and input validation",
    "content": "from dataclasses import dataclass\nfrom typing import Dict, Any, Optional, List\nfrom enum import Enum\n\nclass TaskStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n\n@dataclass\nclass Task:\n    \"\"\"Core task abstraction for the orchestrator\"\"\"\n    id: str\n    name: str\n    action: str\n    parameters: Dict[str, Any]\n    dependencies: List[str]\n    status: TaskStatus = TaskStatus.PENDING\n    result: Optional[Any] = None\n    error: Optional[Exception] = None\n    metadata: Dict[str, Any] = None\n\n    def is_ready(self, completed_tasks: set) -> bool:\n        \"\"\"Check if all dependencies are satisfied\"\"\"\n        return all(dep in completed_tasks for dep in self.dependencies)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 95,
    "line_end": 135,
    "type": "python",
    "description": "`",
    "content": "@dataclass\nclass Pipeline:\n    \"\"\"Pipeline represents a collection of tasks with dependencies\"\"\"\n    id: str\n    name: str\n    tasks: Dict[str, Task]\n    context: Dict[str, Any]\n    metadata: Dict[str, Any]\n\n    def get_execution_order(self) -> List[List[str]]:\n        \"\"\"Returns tasks grouped by execution level (parallel groups)\"\"\"\n        from collections import defaultdict, deque\n\n        # Build dependency graph\n        in_degree = {task_id: len(task.dependencies) for task_id, task in self.tasks.items()}\n        graph = defaultdict(list)\n\n        for task_id, task in self.tasks.items():\n            for dep in task.dependencies:\n                graph[dep].append(task_id)\n\n        # Topological sort with level grouping\n        levels = []\n        queue = deque([task_id for task_id, degree in in_degree.items() if degree == 0])\n\n        while queue:\n            current_level = []\n            level_size = len(queue)\n\n            for _ in range(level_size):\n                task_id = queue.popleft()\n                current_level.append(task_id)\n\n                for neighbor in graph[task_id]:\n                    in_degree[neighbor] -= 1\n                    if in_degree[neighbor] == 0:\n                        queue.append(neighbor)\n\n            levels.append(current_level)\n\n        return levels",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 141,
    "line_end": 171,
    "type": "python",
    "description": "`",
    "content": "@dataclass\nclass ModelCapabilities:\n    \"\"\"Defines what a model can do\"\"\"\n    supported_tasks: List[str]\n    context_window: int\n    supports_function_calling: bool\n    supports_structured_output: bool\n    supports_streaming: bool\n    languages: List[str]\n\n@dataclass\nclass ModelRequirements:\n    \"\"\"Resource requirements for a model\"\"\"\n    memory_gb: float\n    gpu_memory_gb: Optional[float]\n    cpu_cores: int\n    supports_quantization: List[str]  # [\"int8\", \"int4\", \"gptq\", \"awq\"]\n\nclass Model:\n    \"\"\"Abstract base class for all models\"\"\"\n    def __init__(self, name: str, provider: str):\n        self.name = name\n        self.provider = provider\n        self.capabilities = self._load_capabilities()\n        self.requirements = self._load_requirements()\n\n    async def generate(self, prompt: str, **kwargs) -> str:\n        raise NotImplementedError\n\n    async def generate_structured(self, prompt: str, schema: dict, **kwargs) -> dict:\n        raise NotImplementedError",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 177,
    "line_end": 200,
    "type": "python",
    "description": "`",
    "content": "from abc import ABC, abstractmethod\n\nclass ControlSystem(ABC):\n    \"\"\"Abstract base class for control system adapters\"\"\"\n\n    @abstractmethod\n    async def execute_task(self, task: Task, context: Dict[str, Any]) -> Any:\n        \"\"\"Execute a single task\"\"\"\n        pass\n\n    @abstractmethod\n    async def execute_pipeline(self, pipeline: Pipeline) -> Dict[str, Any]:\n        \"\"\"Execute an entire pipeline\"\"\"\n        pass\n\n    @abstractmethod\n    def get_capabilities(self) -> Dict[str, Any]:\n        \"\"\"Return system capabilities\"\"\"\n        pass\n\n    @abstractmethod\n    async def health_check(self) -> bool:\n        \"\"\"Check if the system is healthy\"\"\"\n        pass",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 208,
    "line_end": 271,
    "type": "python",
    "description": "`",
    "content": "import yaml\nfrom typing import Dict, Any, List\nimport jsonschema\nfrom jinja2 import Environment, StrictUndefined\n\nclass YAMLCompiler:\n    \"\"\"Compiles YAML definitions into executable pipelines\"\"\"\n\n    def __init__(self):\n        self.schema_validator = SchemaValidator()\n        self.ambiguity_resolver = AmbiguityResolver()\n        self.template_engine = Environment(undefined=StrictUndefined)\n\n    def compile(self, yaml_content: str, context: Dict[str, Any] = None) -> Pipeline:\n        \"\"\"Compile YAML to Pipeline object\"\"\"\n        # Step 1: Parse YAML safely\n        raw_pipeline = yaml.safe_load(yaml_content)\n\n        # Step 2: Validate against schema\n        self.schema_validator.validate(raw_pipeline)\n\n        # Step 3: Process templates\n        processed = self._process_templates(raw_pipeline, context or {})\n\n        # Step 4: Detect and resolve ambiguities\n        resolved = self._resolve_ambiguities(processed)\n\n        # Step 5: Build pipeline object\n        return self._build_pipeline(resolved)\n\n    def _process_templates(self, pipeline_def: dict, context: dict) -> dict:\n        \"\"\"Process Jinja2 templates in the pipeline definition\"\"\"\n        def process_value(value):\n            if isinstance(value, str):\n                template = self.template_engine.from_string(value)\n                return template.render(**context)\n            elif isinstance(value, dict):\n                return {k: process_value(v) for k, v in value.items()}\n            elif isinstance(value, list):\n                return [process_value(item) for item in value]\n            return value\n\n        return process_value(pipeline_def)\n\n    def _resolve_ambiguities(self, pipeline_def: dict) -> dict:\n        \"\"\"Detect and resolve <AUTO> tags\"\"\"\n        def process_auto_tags(obj, path=\"\"):\n            if isinstance(obj, dict):\n                result = {}\n                for key, value in obj.items():\n                    if isinstance(value, str) and value.startswith(\"<AUTO>\") and value.endswith(\"</AUTO>\"):\n                        # Extract ambiguous content\n                        content = value[6:-7]  # Remove <AUTO> tags\n                        # Resolve ambiguity\n                        resolved = self.ambiguity_resolver.resolve(content, path + \".\" + key)\n                        result[key] = resolved\n                    else:\n                        result[key] = process_auto_tags(value, path + \".\" + key)\n                return result\n            elif isinstance(obj, list):\n                return [process_auto_tags(item, f\"{path}[{i}]\") for i, item in enumerate(obj)]\n            return obj\n\n        return process_auto_tags(pipeline_def)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 277,
    "line_end": 338,
    "type": "python",
    "description": "`",
    "content": "class AmbiguityResolver:\n    \"\"\"Resolves ambiguous specifications using LLMs\"\"\"\n\n    def __init__(self):\n        self.model_selector = ModelSelector()\n        self.format_cache = FormatCache()\n        self.resolution_strategies = {\n            \"task_type\": self._resolve_task_type,\n            \"model_selection\": self._resolve_model_selection,\n            \"parameter_inference\": self._resolve_parameters,\n            \"dependency_detection\": self._resolve_dependencies\n        }\n\n    async def resolve(self, ambiguous_content: str, context_path: str) -> Any:\n        \"\"\"Main resolution method\"\"\"\n        # Step 1: Classify ambiguity type\n        ambiguity_type = await self._classify_ambiguity(ambiguous_content, context_path)\n\n        # Step 2: Check cache\n        cache_key = self._generate_cache_key(ambiguous_content, ambiguity_type)\n        if cached := self.format_cache.get(cache_key):\n            return cached\n\n        # Step 3: Select appropriate model\n        model = await self.model_selector.select_for_task(\"ambiguity_resolution\")\n\n        # Step 4: Generate format specification (two-step approach)\n        format_spec = await self._generate_format_spec(model, ambiguous_content, ambiguity_type)\n\n        # Step 5: Execute resolution with format spec\n        resolution_strategy = self.resolution_strategies[ambiguity_type]\n        result = await resolution_strategy(model, ambiguous_content, format_spec)\n\n        # Step 6: Cache result\n        self.format_cache.set(cache_key, result)\n\n        return result\n\n    async def _generate_format_spec(self, model, content: str, ambiguity_type: str) -> dict:\n        \"\"\"Generate output format specification\"\"\"\n        prompt = f\"\"\"\n        Analyze this ambiguous specification and generate a JSON schema for the expected output:\n\n        Ambiguity Type: {ambiguity_type}\n        Content: {content}\n\n        Return a JSON schema that describes the expected structure of the resolved output.\n        \"\"\"\n\n        schema = await model.generate_structured(\n            prompt,\n            schema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"schema\": {\"type\": \"object\"},\n                    \"description\": {\"type\": \"string\"},\n                    \"examples\": {\"type\": \"array\"}\n                }\n            }\n        )\n\n        return schema",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 344,
    "line_end": 435,
    "type": "python",
    "description": "`",
    "content": "import pickle\nimport json\nfrom datetime import datetime\nfrom typing import Optional\nimport asyncio\nfrom contextlib import asynccontextmanager\n\nclass StateManager:\n    \"\"\"Manages pipeline state and checkpointing\"\"\"\n\n    def __init__(self, backend: str = \"postgres\"):\n        self.backend = self._init_backend(backend)\n        self.checkpoint_strategy = AdaptiveCheckpointStrategy()\n\n    async def save_checkpoint(self, pipeline_id: str, state: dict, metadata: dict = None):\n        \"\"\"Save pipeline state checkpoint\"\"\"\n        checkpoint = {\n            \"pipeline_id\": pipeline_id,\n            \"state\": state,\n            \"metadata\": metadata or {},\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"version\": \"1.0\"\n        }\n\n        # Compress state if large\n        if self._should_compress(state):\n            checkpoint[\"state\"] = self._compress_state(state)\n            checkpoint[\"compressed\"] = True\n\n        await self.backend.save(checkpoint)\n\n    async def restore_checkpoint(self, pipeline_id: str,\n                                timestamp: Optional[datetime] = None) -> Optional[dict]:\n        \"\"\"Restore pipeline state from checkpoint\"\"\"\n        checkpoint = await self.backend.load(pipeline_id, timestamp)\n\n        if not checkpoint:\n            return None\n\n        # Decompress if needed\n        if checkpoint.get(\"compressed\"):\n            checkpoint[\"state\"] = self._decompress_state(checkpoint[\"state\"])\n\n        return checkpoint\n\n    @asynccontextmanager\n    async def checkpoint_context(self, pipeline_id: str, task_id: str):\n        \"\"\"Context manager for automatic checkpointing\"\"\"\n        start_time = datetime.utcnow()\n\n        try:\n            yield\n            # Save checkpoint on success\n            if self.checkpoint_strategy.should_checkpoint(pipeline_id, task_id):\n                await self.save_checkpoint(\n                    pipeline_id,\n                    {\"last_completed_task\": task_id},\n                    {\"execution_time\": (datetime.utcnow() - start_time).total_seconds()}\n                )\n        except Exception as e:\n            # Save error state\n            await self.save_checkpoint(\n                pipeline_id,\n                {\"last_failed_task\": task_id, \"error\": str(e)},\n                {\"failure_time\": datetime.utcnow().isoformat()}\n            )\n            raise\n\nclass AdaptiveCheckpointStrategy:\n    \"\"\"Determines when to create checkpoints based on various factors\"\"\"\n\n    def __init__(self):\n        self.task_history = {}\n        self.checkpoint_interval = 5  # Base interval\n\n    def should_checkpoint(self, pipeline_id: str, task_id: str) -> bool:\n        \"\"\"Decide if checkpoint is needed\"\"\"\n        # Always checkpoint after critical tasks\n        if self._is_critical_task(task_id):\n            return True\n\n        # Adaptive checkpointing based on task execution time\n        if pipeline_id not in self.task_history:\n            self.task_history[pipeline_id] = []\n\n        self.task_history[pipeline_id].append(task_id)\n\n        # Checkpoint every N tasks\n        if len(self.task_history[pipeline_id]) % self.checkpoint_interval == 0:\n            return True\n\n        return False",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 441,
    "line_end": 553,
    "type": "python",
    "description": "`",
    "content": "import numpy as np\nfrom typing import List, Dict, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass ModelMetrics:\n    \"\"\"Performance metrics for a model\"\"\"\n    latency_p50: float\n    latency_p95: float\n    throughput: float\n    accuracy: float\n    cost_per_token: float\n\nclass ModelRegistry:\n    \"\"\"Central registry for all available models\"\"\"\n\n    def __init__(self):\n        self.models: Dict[str, Model] = {}\n        self.metrics: Dict[str, ModelMetrics] = {}\n        self.bandit = UCBModelSelector()\n\n    def register_model(self, model: Model, metrics: ModelMetrics):\n        \"\"\"Register a new model\"\"\"\n        self.models[model.name] = model\n        self.metrics[model.name] = metrics\n\n    async def select_model(self, requirements: Dict[str, Any]) -> Model:\n        \"\"\"Select best model for given requirements\"\"\"\n        # Step 1: Filter by capabilities\n        eligible_models = self._filter_by_capabilities(requirements)\n\n        # Step 2: Filter by available resources\n        available_models = await self._filter_by_resources(eligible_models)\n\n        # Step 3: Use multi-armed bandit for selection\n        selected_model_name = self.bandit.select(\n            [m.name for m in available_models],\n            requirements\n        )\n\n        return self.models[selected_model_name]\n\n    def _filter_by_capabilities(self, requirements: Dict[str, Any]) -> List[Model]:\n        \"\"\"Filter models by required capabilities\"\"\"\n        eligible = []\n\n        for model in self.models.values():\n            if self._meets_requirements(model, requirements):\n                eligible.append(model)\n\n        return eligible\n\n    async def _filter_by_resources(self, models: List[Model]) -> List[Model]:\n        \"\"\"Filter models by available system resources\"\"\"\n        system_resources = await self._get_system_resources()\n        available = []\n\n        for model in models:\n            if self._can_run_on_system(model, system_resources):\n                available.append(model)\n\n        # If no models fit, try quantized versions\n        if not available:\n            available = await self._find_quantized_alternatives(models, system_resources)\n\n        return available\n\nclass UCBModelSelector:\n    \"\"\"Upper Confidence Bound algorithm for model selection\"\"\"\n\n    def __init__(self, exploration_factor: float = 2.0):\n        self.exploration_factor = exploration_factor\n        self.model_stats = {}  # Track performance per model\n\n    def select(self, model_names: List[str], context: Dict[str, Any]) -> str:\n        \"\"\"Select model using UCB algorithm\"\"\"\n        if not model_names:\n            raise ValueError(\"No models available\")\n\n        # Initialize stats for new models\n        for name in model_names:\n            if name not in self.model_stats:\n                self.model_stats[name] = {\n                    \"successes\": 0,\n                    \"attempts\": 0,\n                    \"total_reward\": 0.0\n                }\n\n        # Calculate UCB scores\n        scores = {}\n        total_attempts = sum(stats[\"attempts\"] for stats in self.model_stats.values())\n\n        for name in model_names:\n            stats = self.model_stats[name]\n            if stats[\"attempts\"] == 0:\n                scores[name] = float('inf')  # Explore untried models\n            else:\n                avg_reward = stats[\"total_reward\"] / stats[\"attempts\"]\n                exploration_bonus = self.exploration_factor * np.sqrt(\n                    np.log(total_attempts + 1) / stats[\"attempts\"]\n                )\n                scores[name] = avg_reward + exploration_bonus\n\n        # Select model with highest score\n        return max(scores, key=scores.get)\n\n    def update_reward(self, model_name: str, reward: float):\n        \"\"\"Update model statistics after execution\"\"\"\n        if model_name in self.model_stats:\n            self.model_stats[model_name][\"attempts\"] += 1\n            self.model_stats[model_name][\"total_reward\"] += reward\n            if reward > 0:\n                self.model_stats[model_name][\"successes\"] += 1",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 559,
    "line_end": 677,
    "type": "python",
    "description": "`",
    "content": "from enum import Enum\nfrom typing import Optional, Callable\nimport asyncio\n\nclass ErrorSeverity(Enum):\n    CRITICAL = \"critical\"\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nclass ErrorCategory(Enum):\n    RATE_LIMIT = \"rate_limit\"\n    TIMEOUT = \"timeout\"\n    RESOURCE_EXHAUSTION = \"resource_exhaustion\"\n    VALIDATION_ERROR = \"validation_error\"\n    SYSTEM_ERROR = \"system_error\"\n    UNKNOWN = \"unknown\"\n\nclass ErrorHandler:\n    \"\"\"Comprehensive error handling system\"\"\"\n\n    def __init__(self):\n        self.error_strategies = {\n            ErrorCategory.RATE_LIMIT: self._handle_rate_limit,\n            ErrorCategory.TIMEOUT: self._handle_timeout,\n            ErrorCategory.RESOURCE_EXHAUSTION: self._handle_resource_exhaustion,\n            ErrorCategory.VALIDATION_ERROR: self._handle_validation_error,\n            ErrorCategory.SYSTEM_ERROR: self._handle_system_error\n        }\n        self.circuit_breaker = CircuitBreaker()\n\n    async def handle_error(self, error: Exception, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Main error handling method\"\"\"\n        # Classify error\n        category = self._classify_error(error)\n        severity = self._determine_severity(error, context)\n\n        # Log error with full context\n        await self._log_error(error, category, severity, context)\n\n        # Apply circuit breaker\n        if self.circuit_breaker.is_open(context.get(\"system_id\")):\n            raise SystemUnavailableError(\"System circuit breaker is open\")\n\n        # Execute error handling strategy\n        strategy = self.error_strategies.get(category, self._handle_unknown)\n        result = await strategy(error, context, severity)\n\n        # Update circuit breaker\n        if severity == ErrorSeverity.CRITICAL:\n            self.circuit_breaker.record_failure(context.get(\"system_id\"))\n\n        return result\n\n    async def _handle_rate_limit(self, error: Exception, context: Dict[str, Any],\n                                severity: ErrorSeverity) -> Dict[str, Any]:\n        \"\"\"Handle rate limit errors\"\"\"\n        retry_after = self._extract_retry_after(error) or 60\n\n        if severity == ErrorSeverity.LOW:\n            # Wait and retry\n            await asyncio.sleep(retry_after)\n            return {\"action\": \"retry\", \"delay\": retry_after}\n        else:\n            # Switch to alternative system\n            return {\n                \"action\": \"switch_system\",\n                \"reason\": \"rate_limit_exceeded\",\n                \"retry_after\": retry_after\n            }\n\n    async def _handle_timeout(self, error: Exception, context: Dict[str, Any],\n                             severity: ErrorSeverity) -> Dict[str, Any]:\n        \"\"\"Handle timeout errors\"\"\"\n        if context.get(\"retry_count\", 0) < 3:\n            # Retry with increased timeout\n            new_timeout = context.get(\"timeout\", 30) * 2\n            return {\n                \"action\": \"retry\",\n                \"timeout\": new_timeout,\n                \"retry_count\": context.get(\"retry_count\", 0) + 1\n            }\n        else:\n            # Mark task as failed\n            return {\"action\": \"fail\", \"reason\": \"timeout_exceeded\"}\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker pattern implementation\"\"\"\n\n    def __init__(self, failure_threshold: int = 5, timeout: float = 60.0):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.failures = {}\n        self.last_failure_time = {}\n\n    def is_open(self, system_id: str) -> bool:\n        \"\"\"Check if circuit breaker is open for a system\"\"\"\n        if system_id not in self.failures:\n            return False\n\n        # Check if timeout has passed\n        if system_id in self.last_failure_time:\n            time_since_failure = time.time() - self.last_failure_time[system_id]\n            if time_since_failure > self.timeout:\n                # Reset circuit breaker\n                self.failures[system_id] = 0\n                return False\n\n        return self.failures[system_id] >= self.failure_threshold\n\n    def record_failure(self, system_id: str):\n        \"\"\"Record a failure for a system\"\"\"\n        self.failures[system_id] = self.failures.get(system_id, 0) + 1\n        self.last_failure_time[system_id] = time.time()\n\n    def record_success(self, system_id: str):\n        \"\"\"Record a success for a system\"\"\"\n        if system_id in self.failures:\n            self.failures[system_id] = max(0, self.failures[system_id] - 1)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 685,
    "line_end": 740,
    "type": "yaml",
    "description": "`",
    "content": "# schema/pipeline-schema.yaml\n$schema: \"http://json-schema.org/draft-07/schema#\"\ntype: object\nrequired:\n  - name\n  - version\n  - steps\nproperties:\n  name:\n    type: string\n    pattern: \"^[a-zA-Z][a-zA-Z0-9_-]*$\"\n  version:\n    type: string\n    pattern: \"^\\\\d+\\\\.\\\\d+\\\\.\\\\d+$\"\n  description:\n    type: string\n  metadata:\n    type: object\n  context:\n    type: object\n    properties:\n      timeout:\n        type: integer\n        minimum: 1\n      max_retries:\n        type: integer\n        minimum: 0\n      checkpoint_strategy:\n        type: string\n        enum: [\"adaptive\", \"fixed\", \"none\"]\n  steps:\n    type: array\n    minItems: 1\n    items:\n      type: object\n      required:\n        - id\n        - action\n      properties:\n        id:\n          type: string\n          pattern: \"^[a-zA-Z][a-zA-Z0-9_-]*$\"\n        action:\n          type: string\n        parameters:\n          type: object\n        dependencies:\n          type: array\n          items:\n            type: string\n        on_failure:\n          type: string\n          enum: [\"continue\", \"fail\", \"retry\", \"skip\"]\n        timeout:\n          type: integer\n          minimum: 1",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 746,
    "line_end": 794,
    "type": "yaml",
    "description": "`",
    "content": "# example-pipeline.yaml\nname: research_report_pipeline\nversion: 1.0.0\ndescription: Generate a comprehensive research report on a given topic\n\ncontext:\n  timeout: 3600\n  max_retries: 3\n  checkpoint_strategy: adaptive\n\nsteps:\n  - id: topic_analysis\n    action: analyze\n    parameters:\n      input: \"{{ topic }}\"\n      analysis_type: <AUTO>Determine the best analysis approach for this topic</AUTO>\n      output_format: <AUTO>Choose appropriate format: bullet_points, narrative, or structured</AUTO>\n\n  - id: research_planning\n    action: plan\n    parameters:\n      topic_analysis: \"{{ steps.topic_analysis.output }}\"\n      research_depth: <AUTO>Based on topic complexity, choose: shallow, medium, or deep</AUTO>\n      sources: <AUTO>Determine number and types of sources needed</AUTO>\n    dependencies: [topic_analysis]\n\n  - id: web_search\n    action: search\n    parameters:\n      queries: <AUTO>Generate search queries based on research plan</AUTO>\n      num_results: <AUTO>Determine optimal number of results per query</AUTO>\n    dependencies: [research_planning]\n\n  - id: content_synthesis\n    action: synthesize\n    parameters:\n      sources: \"{{ steps.web_search.results }}\"\n      style: <AUTO>Choose writing style: academic, business, or general</AUTO>\n      length: <AUTO>Determine appropriate length based on topic</AUTO>\n    dependencies: [web_search]\n\n  - id: report_generation\n    action: generate_report\n    parameters:\n      content: \"{{ steps.content_synthesis.output }}\"\n      format: markdown\n      sections: <AUTO>Organize content into appropriate sections</AUTO>\n    dependencies: [content_synthesis]\n    on_failure: retry",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 802,
    "line_end": 836,
    "type": "python",
    "description": "`",
    "content": "# main.py\nimport asyncio\nfrom orchestrator import Orchestrator, YAMLCompiler\nfrom orchestrator.models import ModelRegistry\nfrom orchestrator.storage import PostgresBackend\n\nasync def main():\n    # Initialize orchestrator\n    orchestrator = Orchestrator(\n        storage_backend=PostgresBackend(\"postgresql://localhost/orchestrator\"),\n        model_registry=ModelRegistry.from_config(\"models.yaml\")\n    )\n\n    # Load and compile pipeline\n    with open(\"research_pipeline.yaml\") as f:\n        yaml_content = f.read()\n\n    compiler = YAMLCompiler()\n    pipeline = await compiler.compile(\n        yaml_content,\n        context={\"topic\": \"quantum computing applications in cryptography\"}\n    )\n\n    # Execute pipeline\n    try:\n        result = await orchestrator.execute_pipeline(pipeline)\n        print(f\"Pipeline completed successfully: {result}\")\n    except Exception as e:\n        print(f\"Pipeline failed: {e}\")\n        # Attempt recovery\n        recovered_result = await orchestrator.recover_pipeline(pipeline.id)\n        print(f\"Recovery result: {recovered_result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 842,
    "line_end": 903,
    "type": "python",
    "description": "`",
    "content": "# adapters/custom_adapter.py\nfrom orchestrator.adapters import ControlSystem\nfrom orchestrator.models import Task\n\nclass CustomMLAdapter(ControlSystem):\n    \"\"\"Example adapter for a custom ML framework\"\"\"\n\n    def __init__(self, config: dict):\n        self.config = config\n        self.ml_engine = self._init_ml_engine()\n\n    async def execute_task(self, task: Task, context: dict) -> Any:\n        \"\"\"Execute task using custom ML framework\"\"\"\n        # Map task action to ML operation\n        operation = self._map_action_to_operation(task.action)\n\n        # Prepare inputs\n        inputs = self._prepare_inputs(task.parameters, context)\n\n        # Execute with monitoring\n        async with self._monitor_execution(task.id):\n            result = await self.ml_engine.run(operation, inputs)\n\n        # Post-process results\n        return self._process_results(result, task.metadata)\n\n    async def execute_pipeline(self, pipeline: Pipeline) -> dict:\n        \"\"\"Execute entire pipeline with optimizations\"\"\"\n        # Build execution graph\n        exec_graph = self._build_execution_graph(pipeline)\n\n        # Optimize graph (fusion, parallelization)\n        optimized_graph = self._optimize_graph(exec_graph)\n\n        # Execute with checkpointing\n        results = {}\n        for level in optimized_graph.levels:\n            # Execute tasks in parallel\n            level_results = await asyncio.gather(*[\n                self.execute_task(task, pipeline.context)\n                for task in level\n            ])\n\n            # Store results\n            for task, result in zip(level, level_results):\n                results[task.id] = result\n\n        return results\n\n    def get_capabilities(self) -> dict:\n        return {\n            \"supported_operations\": [\"train\", \"predict\", \"evaluate\", \"optimize\"],\n            \"parallel_execution\": True,\n            \"gpu_acceleration\": True,\n            \"distributed\": self.config.get(\"distributed\", False)\n        }\n\n    async def health_check(self) -> bool:\n        try:\n            return await self.ml_engine.ping()\n        except Exception:\n            return False",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 909,
    "line_end": 1012,
    "type": "python",
    "description": "`",
    "content": "# sandbox/executor.py\nimport docker\nimport asyncio\nfrom typing import Dict, Any, Optional\n\nclass SandboxedExecutor:\n    \"\"\"Secure sandboxed code execution\"\"\"\n\n    def __init__(self, docker_client=None):\n        self.docker = docker_client or docker.from_env()\n        self.containers = {}\n        self.resource_limits = {\n            \"memory\": \"1g\",\n            \"cpu_quota\": 50000,\n            \"pids_limit\": 100\n        }\n\n    async def execute_code(\n        self,\n        code: str,\n        language: str,\n        environment: Dict[str, str],\n        timeout: int = 30\n    ) -> Dict[str, Any]:\n        \"\"\"Execute code in sandboxed environment\"\"\"\n        # Create container\n        container = await self._create_container(language, environment)\n\n        try:\n            # Start container\n            await self._start_container(container)\n\n            # Execute code with timeout\n            result = await asyncio.wait_for(\n                self._run_code(container, code),\n                timeout=timeout\n            )\n\n            return {\n                \"success\": True,\n                \"output\": result[\"output\"],\n                \"errors\": result.get(\"errors\", \"\"),\n                \"execution_time\": result[\"execution_time\"]\n            }\n\n        except asyncio.TimeoutError:\n            return {\n                \"success\": False,\n                \"error\": \"Execution timeout exceeded\",\n                \"timeout\": timeout\n            }\n        finally:\n            # Cleanup\n            await self._cleanup_container(container)\n\n    async def _create_container(\n        self,\n        language: str,\n        environment: Dict[str, str]\n    ) -> docker.models.containers.Container:\n        \"\"\"Create isolated container for code execution\"\"\"\n        image = self._get_image_for_language(language)\n\n        container = self.docker.containers.create(\n            image=image,\n            command=\"sleep infinity\",  # Keep container running\n            detach=True,\n            mem_limit=self.resource_limits[\"memory\"],\n            cpu_quota=self.resource_limits[\"cpu_quota\"],\n            pids_limit=self.resource_limits[\"pids_limit\"],\n            network_mode=\"none\",  # No network access\n            read_only=True,\n            tmpfs={\"/tmp\": \"rw,noexec,nosuid,size=100m\"},\n            environment=environment,\n            security_opt=[\"no-new-privileges:true\"],\n            user=\"nobody\"  # Run as non-root\n        )\n\n        self.containers[container.id] = container\n        return container\n\n    async def _run_code(\n        self,\n        container: docker.models.containers.Container,\n        code: str\n    ) -> Dict[str, Any]:\n        \"\"\"Execute code inside container\"\"\"\n        # Write code to container\n        code_file = \"/tmp/code.py\"\n        container.exec_run(f\"sh -c 'cat > {code_file}'\", stdin=True).input = code.encode()\n\n        # Execute code\n        start_time = asyncio.get_event_loop().time()\n        result = container.exec_run(f\"python {code_file}\", demux=True)\n        execution_time = asyncio.get_event_loop().time() - start_time\n\n        stdout, stderr = result.output\n\n        return {\n            \"output\": stdout.decode() if stdout else \"\",\n            \"errors\": stderr.decode() if stderr else \"\",\n            \"exit_code\": result.exit_code,\n            \"execution_time\": execution_time\n        }",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1020,
    "line_end": 1062,
    "type": "python",
    "description": "`",
    "content": "# tests/test_ambiguity_resolver.py\nimport pytest\nfrom orchestrator.compiler import AmbiguityResolver\n\n@pytest.mark.asyncio\nasync def test_ambiguity_resolution():\n    resolver = AmbiguityResolver()\n\n    # Test simple ambiguity\n    result = await resolver.resolve(\n        \"Choose the best format for displaying data\",\n        \"steps.display.format\"\n    )\n\n    assert result in [\"table\", \"chart\", \"list\", \"json\"]\n\n    # Test complex ambiguity with context\n    result = await resolver.resolve(\n        \"Determine optimal batch size based on available memory\",\n        \"steps.processing.batch_size\"\n    )\n\n    assert isinstance(result, int)\n    assert 1 <= result <= 1000\n\n@pytest.mark.asyncio\nasync def test_nested_ambiguity_resolution():\n    resolver = AmbiguityResolver()\n\n    nested_content = {\n        \"query\": \"<AUTO>Generate search query for topic</AUTO>\",\n        \"filters\": {\n            \"date_range\": \"<AUTO>Choose appropriate date range</AUTO>\",\n            \"sources\": \"<AUTO>Select relevant sources</AUTO>\"\n        }\n    }\n\n    result = await resolver.resolve_nested(nested_content, \"steps.search\")\n\n    assert \"query\" in result\n    assert \"filters\" in result\n    assert \"date_range\" in result[\"filters\"]\n    assert \"sources\" in result[\"filters\"]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1068,
    "line_end": 1116,
    "type": "python",
    "description": "`",
    "content": "# tests/test_pipeline_execution.py\nimport pytest\nfrom orchestrator import Orchestrator\n\n@pytest.mark.integration\n@pytest.mark.asyncio\nasync def test_full_pipeline_execution():\n    orchestrator = Orchestrator()\n\n    pipeline_yaml = \"\"\"\n    name: test_pipeline\n    version: 1.0.0\n    steps:\n      - id: step1\n        action: generate\n        parameters:\n          prompt: \"Hello, world!\"\n      - id: step2\n        action: transform\n        parameters:\n          input: \"{{ steps.step1.output }}\"\n          transformation: uppercase\n        dependencies: [step1]\n    \"\"\"\n\n    result = await orchestrator.execute_yaml(pipeline_yaml)\n\n    assert result[\"status\"] == \"completed\"\n    assert \"HELLO, WORLD!\" in result[\"outputs\"][\"step2\"]\n\n@pytest.mark.integration\n@pytest.mark.asyncio\nasync def test_pipeline_recovery():\n    orchestrator = Orchestrator()\n\n    # Simulate failure and recovery\n    pipeline_id = \"test_recovery_pipeline\"\n\n    # Create checkpoint\n    await orchestrator.state_manager.save_checkpoint(\n        pipeline_id,\n        {\"completed_steps\": [\"step1\", \"step2\"], \"failed_step\": \"step3\"}\n    )\n\n    # Attempt recovery\n    result = await orchestrator.recover_pipeline(pipeline_id)\n\n    assert result[\"recovered\"] == True\n    assert result[\"resumed_from\"] == \"step3\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1124,
    "line_end": 1152,
    "type": "python",
    "description": "`",
    "content": "class NestedAmbiguityHandler:\n    \"\"\"Handles complex nested ambiguities\"\"\"\n\n    async def resolve_nested(self, obj: Any, path: str = \"\") -> Any:\n        \"\"\"Recursively resolve nested ambiguities\"\"\"\n        if isinstance(obj, dict):\n            # Check for circular dependencies\n            self._check_circular_deps(obj, path)\n\n            resolved = {}\n            for key, value in obj.items():\n                new_path = f\"{path}.{key}\" if path else key\n\n                if self._is_ambiguous(value):\n                    # Resolve with parent context\n                    context = self._build_context(obj, key, path)\n                    resolved[key] = await self._resolve_with_context(value, context)\n                else:\n                    resolved[key] = await self.resolve_nested(value, new_path)\n\n            return resolved\n\n        elif isinstance(obj, list):\n            return [\n                await self.resolve_nested(item, f\"{path}[{i}]\")\n                for i, item in enumerate(obj)\n            ]\n\n        return obj",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1158,
    "line_end": 1196,
    "type": "python",
    "description": "`",
    "content": "class DynamicModelSwitcher:\n    \"\"\"Handles model switching during pipeline execution\"\"\"\n\n    async def switch_model(\n        self,\n        current_model: str,\n        reason: str,\n        context: Dict[str, Any]\n    ) -> str:\n        \"\"\"Switch to alternative model mid-execution\"\"\"\n        # Save current state\n        checkpoint = await self._create_switching_checkpoint(\n            current_model, reason, context\n        )\n\n        # Find alternative model\n        alternatives = await self._find_alternatives(current_model, reason)\n\n        if not alternatives:\n            raise NoAlternativeModelError(\n                f\"No alternative found for {current_model}\"\n            )\n\n        # Select best alternative\n        new_model = await self._select_alternative(\n            alternatives,\n            context,\n            checkpoint\n        )\n\n        # Migrate state if needed\n        if self._needs_state_migration(current_model, new_model):\n            context = await self._migrate_state(\n                context,\n                current_model,\n                new_model\n            )\n\n        return new_model",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1202,
    "line_end": 1232,
    "type": "python",
    "description": "`",
    "content": "class DependencyValidator:\n    \"\"\"Validates and resolves pipeline dependencies\"\"\"\n\n    def detect_cycles(self, tasks: Dict[str, Task]) -> List[List[str]]:\n        \"\"\"Detect circular dependencies using DFS\"\"\"\n        WHITE, GRAY, BLACK = 0, 1, 2\n        color = {task_id: WHITE for task_id in tasks}\n        cycles = []\n\n        def dfs(task_id: str, path: List[str]):\n            color[task_id] = GRAY\n            path.append(task_id)\n\n            for dep in tasks[task_id].dependencies:\n                if dep not in tasks:\n                    raise InvalidDependencyError(f\"Unknown dependency: {dep}\")\n\n                if color[dep] == GRAY:\n                    # Found cycle\n                    cycle_start = path.index(dep)\n                    cycles.append(path[cycle_start:])\n                elif color[dep] == WHITE:\n                    dfs(dep, path[:])\n\n            color[task_id] = BLACK\n\n        for task_id in tasks:\n            if color[task_id] == WHITE:\n                dfs(task_id, [])\n\n        return cycles",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1240,
    "line_end": 1271,
    "type": "python",
    "description": "`",
    "content": "class MultiLevelCache:\n    \"\"\"Multi-level caching system for performance\"\"\"\n\n    def __init__(self):\n        self.memory_cache = LRUCache(maxsize=1000)\n        self.redis_cache = RedisCache()\n        self.disk_cache = DiskCache(\"/var/cache/orchestrator\")\n\n    async def get(self, key: str) -> Optional[Any]:\n        \"\"\"Get from cache with fallback hierarchy\"\"\"\n        # L1: Memory cache\n        if value := self.memory_cache.get(key):\n            return value\n\n        # L2: Redis cache\n        if value := await self.redis_cache.get(key):\n            self.memory_cache.set(key, value)\n            return value\n\n        # L3: Disk cache\n        if value := await self.disk_cache.get(key):\n            await self.redis_cache.set(key, value, ttl=3600)\n            self.memory_cache.set(key, value)\n            return value\n\n        return None\n\n    async def set(self, key: str, value: Any, ttl: Optional[int] = None):\n        \"\"\"Set in all cache levels\"\"\"\n        self.memory_cache.set(key, value)\n        await self.redis_cache.set(key, value, ttl=ttl or 3600)\n        await self.disk_cache.set(key, value)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1277,
    "line_end": 1320,
    "type": "python",
    "description": "`",
    "content": "class ParallelExecutor:\n    \"\"\"Optimized parallel task execution\"\"\"\n\n    def __init__(self, max_workers: int = 10):\n        self.semaphore = asyncio.Semaphore(max_workers)\n        self.task_queue = asyncio.Queue()\n\n    async def execute_level(\n        self,\n        tasks: List[Task],\n        context: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Execute tasks in parallel with resource management\"\"\"\n        # Group tasks by resource requirements\n        task_groups = self._group_by_resources(tasks)\n\n        results = {}\n        for group in task_groups:\n            # Execute group with resource limits\n            group_results = await self._execute_group(group, context)\n            results.update(group_results)\n\n        return results\n\n    async def _execute_group(\n        self,\n        tasks: List[Task],\n        context: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"Execute a group of similar tasks\"\"\"\n        async def execute_with_semaphore(task):\n            async with self.semaphore:\n                return await self._execute_single(task, context)\n\n        # Execute all tasks in parallel\n        results = await asyncio.gather(*[\n            execute_with_semaphore(task) for task in tasks\n        ], return_exceptions=True)\n\n        # Process results\n        return {\n            task.id: self._process_result(result)\n            for task, result in zip(tasks, results)\n        }",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1328,
    "line_end": 1444,
    "type": "yaml",
    "description": "`",
    "content": "# models.yaml\nmodels:\n  # Large models\n  gpt-4o:\n    provider: openai\n    capabilities:\n      tasks: [reasoning, code_generation, analysis, creative_writing]\n      context_window: 128000\n      supports_function_calling: true\n      supports_structured_output: true\n      languages: [en, es, fr, de, zh, ja, ko]\n    requirements:\n      memory_gb: 16\n      gpu_memory_gb: 24\n      cpu_cores: 8\n    metrics:\n      latency_p50: 2.1\n      latency_p95: 4.5\n      throughput: 10\n      accuracy: 0.95\n      cost_per_token: 0.00003\n\n  claude-4-opus:\n    provider: anthropic\n    capabilities:\n      tasks: [reasoning, analysis, creative_writing, code_review]\n      context_window: 200000\n      supports_function_calling: false\n      supports_structured_output: true\n      languages: [en, es, fr, de, zh, ja]\n    requirements:\n      memory_gb: 16\n      gpu_memory_gb: 32\n      cpu_cores: 8\n    metrics:\n      latency_p50: 2.5\n      latency_p95: 5.0\n      throughput: 8\n      accuracy: 0.94\n      cost_per_token: 0.00003\n\n  # Medium models\n  gpt-4o-mini:\n    provider: openai\n    capabilities:\n      tasks: [general, code_generation, summarization]\n      context_window: 16384\n      supports_function_calling: true\n      supports_structured_output: true\n      languages: [en, es, fr, de]\n    requirements:\n      memory_gb: 8\n      gpu_memory_gb: 8\n      cpu_cores: 4\n    metrics:\n      latency_p50: 0.8\n      latency_p95: 1.5\n      throughput: 50\n      accuracy: 0.85\n      cost_per_token: 0.000002\n\n  # Small models (local)\n  llama2-7b:\n    provider: local\n    capabilities:\n      tasks: [general, summarization]\n      context_window: 4096\n      supports_function_calling: false\n      supports_structured_output: false\n      languages: [en]\n    requirements:\n      memory_gb: 16\n      gpu_memory_gb: 8\n      cpu_cores: 4\n      supports_quantization: [int8, int4, gptq]\n    metrics:\n      latency_p50: 0.5\n      latency_p95: 1.0\n      throughput: 20\n      accuracy: 0.75\n      cost_per_token: 0\n\n  # Quantized versions\n  llama2-7b-int4:\n    provider: local\n    base_model: llama2-7b\n    quantization: int4\n    requirements:\n      memory_gb: 8\n      gpu_memory_gb: 4\n      cpu_cores: 4\n    metrics:\n      latency_p50: 0.6\n      latency_p95: 1.2\n      throughput: 15\n      accuracy: 0.72\n      cost_per_token: 0\n\n# Model selection policies\nselection_policies:\n  default:\n    strategy: ucb  # upper confidence bound\n    exploration_factor: 2.0\n\n  cost_optimized:\n    strategy: weighted\n    weights:\n      cost: 0.6\n      accuracy: 0.3\n      latency: 0.1\n\n  performance_optimized:\n    strategy: weighted\n    weights:\n      latency: 0.5\n      accuracy: 0.4\n      cost: 0.1",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1450,
    "line_end": 1527,
    "type": "yaml",
    "description": "`",
    "content": "# config.yaml\norchestrator:\n  # Core settings\n  version: 1.0.0\n  environment: production\n\n  # Storage backend\n  storage:\n    backend: postgres\n    connection_string: ${DATABASE_URL}\n    pool_size: 20\n    checkpoint_compression: true\n    retention_days: 30\n\n  # Execution settings\n  execution:\n    max_concurrent_pipelines: 10\n    max_concurrent_tasks: 50\n    default_timeout: 300\n    max_retries: 3\n    retry_backoff_factor: 2.0\n\n  # Model settings\n  models:\n    registry_path: models.yaml\n    selection_policy: default\n    fallback_enabled: true\n    local_models_path: /opt/models\n\n  # Sandbox settings\n  sandbox:\n    enabled: true\n    docker_socket: /var/run/docker.sock\n    images:\n      python: orchestrator/python:3.11-slim\n      nodejs: orchestrator/node:18-slim\n      custom: ${CUSTOM_SANDBOX_IMAGE}\n    resource_limits:\n      memory: 1GB\n      cpu: 0.5\n      disk: 100MB\n      network: none\n\n  # Security settings\n  security:\n    api_key_required: true\n    rate_limiting:\n      enabled: true\n      requests_per_minute: 60\n      burst_size: 10\n    allowed_actions:\n      - generate\n      - transform\n      - analyze\n      - search\n      - execute\n    forbidden_modules:\n      - os\n      - subprocess\n      - eval\n      - exec\n\n  # Monitoring settings\n  monitoring:\n    metrics_enabled: true\n    metrics_port: 9090\n    tracing_enabled: true\n    tracing_endpoint: ${JAEGER_ENDPOINT}\n    log_level: INFO\n    structured_logging: true\n\n  # Cache settings\n  cache:\n    enabled: true\n    redis_url: ${REDIS_URL}\n    memory_cache_size: 1000\n    ttl_seconds: 3600\n    compression_enabled: true",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1535,
    "line_end": 1577,
    "type": "yaml",
    "description": "`",
    "content": "# pipelines/code-review.yaml\nname: automated_code_review\nversion: 1.0.0\ndescription: Comprehensive code review with multiple analysis passes\n\nsteps:\n  - id: code_parsing\n    action: parse_code\n    parameters:\n      source: \"{{ github_pr_url }}\"\n      languages: <AUTO>Detect programming languages in PR</AUTO>\n\n  - id: security_scan\n    action: security_analysis\n    parameters:\n      code: \"{{ steps.code_parsing.parsed_code }}\"\n      severity_threshold: <AUTO>Based on project type, set threshold</AUTO>\n      scan_depth: <AUTO>Determine scan depth based on code size</AUTO>\n    dependencies: [code_parsing]\n\n  - id: style_check\n    action: style_analysis\n    parameters:\n      code: \"{{ steps.code_parsing.parsed_code }}\"\n      style_guide: <AUTO>Select appropriate style guide for language</AUTO>\n    dependencies: [code_parsing]\n\n  - id: complexity_analysis\n    action: analyze_complexity\n    parameters:\n      code: \"{{ steps.code_parsing.parsed_code }}\"\n      metrics: <AUTO>Choose relevant complexity metrics</AUTO>\n    dependencies: [code_parsing]\n\n  - id: generate_review\n    action: synthesize_review\n    parameters:\n      security_results: \"{{ steps.security_scan.findings }}\"\n      style_results: \"{{ steps.style_check.violations }}\"\n      complexity_results: \"{{ steps.complexity_analysis.metrics }}\"\n      review_tone: <AUTO>Professional, constructive, or educational</AUTO>\n      priority_order: <AUTO>Order findings by severity and impact</AUTO>\n    dependencies: [security_scan, style_check, complexity_analysis]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1583,
    "line_end": 1629,
    "type": "yaml",
    "description": "`",
    "content": "# pipelines/data-analysis.yaml\nname: intelligent_data_analysis\nversion: 1.0.0\ndescription: Automated data analysis with visualization\n\nsteps:\n  - id: data_ingestion\n    action: load_data\n    parameters:\n      source: \"{{ data_source }}\"\n      format: <AUTO>Detect data format (csv, json, parquet, etc)</AUTO>\n      sampling_strategy: <AUTO>Full load or sampling based on size</AUTO>\n\n  - id: data_profiling\n    action: profile_data\n    parameters:\n      data: \"{{ steps.data_ingestion.data }}\"\n      profile_depth: <AUTO>Basic, standard, or comprehensive</AUTO>\n      anomaly_detection: <AUTO>Enable based on data characteristics</AUTO>\n    dependencies: [data_ingestion]\n\n  - id: statistical_analysis\n    action: analyze_statistics\n    parameters:\n      data: \"{{ steps.data_ingestion.data }}\"\n      profile: \"{{ steps.data_profiling.profile }}\"\n      tests: <AUTO>Select appropriate statistical tests</AUTO>\n      confidence_level: <AUTO>Set based on data quality</AUTO>\n    dependencies: [data_profiling]\n\n  - id: visualization_planning\n    action: plan_visualizations\n    parameters:\n      data_profile: \"{{ steps.data_profiling.profile }}\"\n      insights: \"{{ steps.statistical_analysis.insights }}\"\n      num_visualizations: <AUTO>Determine optimal number</AUTO>\n      chart_types: <AUTO>Select appropriate chart types</AUTO>\n    dependencies: [statistical_analysis]\n\n  - id: generate_report\n    action: create_analysis_report\n    parameters:\n      visualizations: \"{{ steps.visualization_planning.charts }}\"\n      insights: \"{{ steps.statistical_analysis.insights }}\"\n      executive_summary: <AUTO>Generate executive summary</AUTO>\n      technical_depth: <AUTO>Adjust based on audience</AUTO>\n    dependencies: [visualization_planning]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "design.md",
    "line_start": 1635,
    "line_end": 1678,
    "type": "yaml",
    "description": "`",
    "content": "# pipelines/ensemble-prediction.yaml\nname: ensemble_prediction\nversion: 1.0.0\ndescription: Ensemble multiple models for robust predictions\n\nsteps:\n  - id: data_preparation\n    action: prepare_data\n    parameters:\n      input_data: \"{{ raw_data }}\"\n      preprocessing: <AUTO>Determine required preprocessing steps</AUTO>\n      feature_engineering: <AUTO>Identify useful feature transformations</AUTO>\n\n  - id: model_selection\n    action: select_models\n    parameters:\n      task_type: \"{{ prediction_task }}\"\n      num_models: <AUTO>Optimal ensemble size (3-7 models)</AUTO>\n      diversity_strategy: <AUTO>Ensure model diversity</AUTO>\n    dependencies: [data_preparation]\n\n  - id: parallel_predictions\n    action: batch_predict\n    parameters:\n      models: \"{{ steps.model_selection.selected_models }}\"\n      data: \"{{ steps.data_preparation.processed_data }}\"\n      execution_strategy: parallel\n    dependencies: [model_selection]\n\n  - id: ensemble_aggregation\n    action: aggregate_predictions\n    parameters:\n      predictions: \"{{ steps.parallel_predictions.results }}\"\n      aggregation_method: <AUTO>voting, averaging, or stacking</AUTO>\n      confidence_calculation: <AUTO>Method for confidence scores</AUTO>\n    dependencies: [parallel_predictions]\n\n  - id: result_validation\n    action: validate_results\n    parameters:\n      ensemble_predictions: \"{{ steps.ensemble_aggregation.final_predictions }}\"\n      validation_threshold: <AUTO>Set based on task criticality</AUTO>\n      fallback_strategy: <AUTO>Define fallback if validation fails</AUTO>\n    dependencies: [ensemble_aggregation]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 54,
    "line_end": 68,
    "type": "python",
    "description": "Edit config/models.yaml to customize model settings.",
    "content": "import asyncio\nfrom orchestrator import Orchestrator\n\nasync def run_example():\n    orchestrator = Orchestrator()\n\n    # Run simple pipeline\n    results = await orchestrator.execute_yaml_file(\n        \"examples/simple_pipeline.yaml\",\n        context={\"input_topic\": \"machine learning\"}\n    )\n\n    print(\"Pipeline results:\", results)\n\nasyncio.run(run_example())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 74,
    "line_end": 80,
    "type": "bash",
    "description": "`",
    "content": "# Simple pipeline\npython -m orchestrator run examples/simple_pipeline.yaml \\\n    --context input_topic=\"machine learning\"\n\n# Multi-model pipeline\npython -m orchestrator run examples/multi_model_pipeline.yaml \\\n    --context dataset_url=\"https://example.com/sales_data.csv\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 89,
    "line_end": 89,
    "type": "yaml",
    "description": "The examples demonstrate various AUTO tag patterns:",
    "content": "analysis_type: <AUTO>What type of analysis is most appropriate for this text?</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 94,
    "line_end": 94,
    "type": "yaml",
    "description": "`",
    "content": "format: <AUTO>Determine the best format for this data source</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 99,
    "line_end": 99,
    "type": "yaml",
    "description": "`",
    "content": "methods: <AUTO>Choose the most appropriate statistical methods</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 104,
    "line_end": 104,
    "type": "yaml",
    "description": "`",
    "content": "validation_rules: <AUTO>Generate appropriate validation rules for this dataset</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 112,
    "line_end": 131,
    "type": "yaml",
    "description": "`",
    "content": "id: my_pipeline\nname: My Custom Pipeline\ndescription: Description of what this pipeline does\nversion: \"1.0\"\n\ncontext:\n  # Global variables accessible to all tasks\n  variable_name: value\n\nsteps:\n  - id: task1\n    name: First Task\n    action: generate  # or analyze, transform, etc.\n    parameters:\n      # Task-specific parameters\n      prompt: \"Your prompt here\"\n    metadata:\n      # Optional metadata\n      requires_model: true\n      priority: 1.0",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 137,
    "line_end": 152,
    "type": "yaml",
    "description": "`",
    "content": "steps:\n  - id: advanced_task\n    name: Advanced Task\n    action: analyze\n    parameters:\n      data: \"{{ results.previous_task }}\"  # Reference previous results\n      method: <AUTO>Choose best method</AUTO>  # AUTO resolution\n    dependencies:\n      - previous_task  # Task dependencies\n    metadata:\n      requires_model: gpt-4  # Specific model requirement\n      cpu_cores: 4  # Resource requirements\n      memory_mb: 2048\n      timeout: 300\n      priority: 0.8\n      on_failure: continue  # Error handling",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 181,
    "line_end": 184,
    "type": "python",
    "description": "Enable debug logging to see detailed execution information:",
    "content": "import logging\nlogging.basicConfig(level=logging.DEBUG)\n\norchestrator = Orchestrator()",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/README.md",
    "line_start": 192,
    "line_end": 199,
    "type": "python",
    "description": "Verify system health before running pipelines:",
    "content": "async def check_health():\n    health = await orchestrator.health_check()\n    print(\"System health:\", health[\"overall\"])\n    return health\n\n# Run the health check\nimport asyncio\nhealth_status = asyncio.run(check_health())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 59,
    "line_end": 59,
    "type": "text",
    "description": "- Execution Engine: Action orchestration and error handling",
    "content": "Thought → Action → Observation → Thought → ...",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 68,
    "line_end": 68,
    "type": "text",
    "description": "- Observe results and adjust approach",
    "content": "Goal → Plan → Execute Steps → Evaluate → Replan if needed",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 74,
    "line_end": 76,
    "type": "text",
    "description": "Suitable for complex, multi-step tasks requiring upfront planning.",
    "content": "Agent A ↔ Agent B ↔ Agent C\n   ↓         ↓         ↓\nShared State/Communication",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 88,
    "line_end": 95,
    "type": "python",
    "description": "- Key Features: Chains, agents, tools, memory, callbacks",
    "content": "from langchain.agents import create_react_agent\nfrom langchain.tools import Tool\n\nagent = create_react_agent(\n    llm=llm,\n    tools=[search_tool, calculator_tool],\n    prompt=agent_prompt\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 104,
    "line_end": 107,
    "type": "python",
    "description": "- Key Features: Conversable agents, group chat, code execution",
    "content": "from autogen import AssistantAgent, UserProxyAgent\n\nassistant = AssistantAgent(\"assistant\", llm_config=llm_config)\nuser_proxy = UserProxyAgent(\"user\", code_execution_config=Ellipsis)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 116,
    "line_end": 120,
    "type": "python",
    "description": "- Key Features: Crews, roles, tasks, processes",
    "content": "from crewai import Agent, Task, Crew\n\nresearcher = Agent(role=\"Researcher\", goal=\"Find information\")\nwriter = Agent(role=\"Writer\", goal=\"Create content\")\ncrew = Crew(agents=[researcher, writer], tasks=[...])",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 128,
    "line_end": 144,
    "type": "python",
    "description": "`",
    "content": "# Using LangChain\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n\ntools = [\n    DuckDuckGoSearchRun(),\n    WikipediaQueryRun()\n]\n\nresearch_agent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\nresult = research_agent.run(\"What are the latest developments in quantum computing?\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 150,
    "line_end": 166,
    "type": "python",
    "description": "`",
    "content": "# Using AutoGen\ncoding_assistant = AssistantAgent(\n    \"coding_assistant\",\n    system_message=\"You are a helpful AI that writes and explains code.\",\n    llm_config={\"model\": \"gpt-4\"}\n)\n\nuser_proxy = UserProxyAgent(\n    \"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)\n\nuser_proxy.initiate_chat(\n    coding_assistant,\n    message=\"Write a Python function to calculate fibonacci numbers efficiently\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/draft_report.md",
    "line_start": 172,
    "line_end": 206,
    "type": "python",
    "description": "`",
    "content": "# Using CrewAI\nfrom crewai import Agent, Task, Crew, Process\n\n# Define agents\nresearcher = Agent(\n    role='Senior Research Analyst',\n    goal='Uncover cutting-edge developments in AI',\n    backstory=\"You're an expert researcher with a keen eye for detail.\"\n)\n\nwriter = Agent(\n    role='Tech Content Strategist',\n    goal='Create compelling content about AI developments',\n    backstory=\"You're a skilled writer who makes complex topics accessible.\"\n)\n\n# Define tasks\nresearch_task = Task(\n    description='Research the latest AI developments in the past month',\n    agent=researcher\n)\n\nwriting_task = Task(\n    description='Write a blog post about the research findings',\n    agent=writer\n)\n\n# Create crew\ncrew = Crew(\n    agents=[researcher, writer],\n    tasks=[research_task, writing_task],\n    process=Process.sequential\n)\n\nresult = crew.kickoff()",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 59,
    "line_end": 59,
    "type": "text",
    "description": "- Execution Engine: Action orchestration and error handling",
    "content": "Thought → Action → Observation → Thought → ...",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 68,
    "line_end": 68,
    "type": "text",
    "description": "- Observe results and adjust approach",
    "content": "Goal → Plan → Execute Steps → Evaluate → Replan if needed",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 74,
    "line_end": 76,
    "type": "text",
    "description": "Suitable for complex, multi-step tasks requiring upfront planning.",
    "content": "Agent A ↔ Agent B ↔ Agent C\n   ↓         ↓         ↓\nShared State/Communication",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 88,
    "line_end": 95,
    "type": "python",
    "description": "- Key Features: Chains, agents, tools, memory, callbacks",
    "content": "from langchain.agents import create_react_agent\nfrom langchain.tools import Tool\n\nagent = create_react_agent(\n    llm=llm,\n    tools=[search_tool, calculator_tool],\n    prompt=agent_prompt\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 104,
    "line_end": 107,
    "type": "python",
    "description": "- Key Features: Conversable agents, group chat, code execution",
    "content": "from autogen import AssistantAgent, UserProxyAgent\n\nassistant = AssistantAgent(\"assistant\", llm_config=llm_config)\nuser_proxy = UserProxyAgent(\"user\", code_execution_config=Ellipsis)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 116,
    "line_end": 120,
    "type": "python",
    "description": "- Key Features: Crews, roles, tasks, processes",
    "content": "from crewai import Agent, Task, Crew\n\nresearcher = Agent(role=\"Researcher\", goal=\"Find information\")\nwriter = Agent(role=\"Writer\", goal=\"Create content\")\ncrew = Crew(agents=[researcher, writer], tasks=[...])",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 128,
    "line_end": 144,
    "type": "python",
    "description": "`",
    "content": "# Using LangChain\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n\ntools = [\n    DuckDuckGoSearchRun(),\n    WikipediaQueryRun()\n]\n\nresearch_agent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\nresult = research_agent.run(\"What are the latest developments in quantum computing?\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 150,
    "line_end": 166,
    "type": "python",
    "description": "`",
    "content": "# Using AutoGen\ncoding_assistant = AssistantAgent(\n    \"coding_assistant\",\n    system_message=\"You are a helpful AI that writes and explains code.\",\n    llm_config={\"model\": \"gpt-4\"}\n)\n\nuser_proxy = UserProxyAgent(\n    \"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)\n\nuser_proxy.initiate_chat(\n    coding_assistant,\n    message=\"Write a Python function to calculate fibonacci numbers efficiently\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/final_report.md",
    "line_start": 172,
    "line_end": 206,
    "type": "python",
    "description": "`",
    "content": "# Using CrewAI\nfrom crewai import Agent, Task, Crew, Process\n\n# Define agents\nresearcher = Agent(\n    role='Senior Research Analyst',\n    goal='Uncover cutting-edge developments in AI',\n    backstory=\"You're an expert researcher with a keen eye for detail.\"\n)\n\nwriter = Agent(\n    role='Tech Content Strategist',\n    goal='Create compelling content about AI developments',\n    backstory=\"You're a skilled writer who makes complex topics accessible.\"\n)\n\n# Define tasks\nresearch_task = Task(\n    description='Research the latest AI developments in the past month',\n    agent=researcher\n)\n\nwriting_task = Task(\n    description='Write a blog post about the research findings',\n    agent=writer\n)\n\n# Create crew\ncrew = Crew(\n    agents=[researcher, writer],\n    tasks=[research_task, writing_task],\n    process=Process.sequential\n)\n\nresult = crew.kickoff()",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 59,
    "line_end": 59,
    "type": "text",
    "description": "- Execution Engine: Action orchestration and error handling",
    "content": "Thought → Action → Observation → Thought → ...",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 68,
    "line_end": 68,
    "type": "text",
    "description": "- Observe results and adjust approach",
    "content": "Goal → Plan → Execute Steps → Evaluate → Replan if needed",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 74,
    "line_end": 76,
    "type": "text",
    "description": "Suitable for complex, multi-step tasks requiring upfront planning.",
    "content": "Agent A ↔ Agent B ↔ Agent C\n   ↓         ↓         ↓\nShared State/Communication",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 88,
    "line_end": 95,
    "type": "python",
    "description": "- Key Features: Chains, agents, tools, memory, callbacks",
    "content": "from langchain.agents import create_react_agent\nfrom langchain.tools import Tool\n\nagent = create_react_agent(\n    llm=llm,\n    tools=[search_tool, calculator_tool],\n    prompt=agent_prompt\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 104,
    "line_end": 107,
    "type": "python",
    "description": "- Key Features: Conversable agents, group chat, code execution",
    "content": "from autogen import AssistantAgent, UserProxyAgent\n\nassistant = AssistantAgent(\"assistant\", llm_config=llm_config)\nuser_proxy = UserProxyAgent(\"user\", code_execution_config=Ellipsis)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 116,
    "line_end": 120,
    "type": "python",
    "description": "- Key Features: Crews, roles, tasks, processes",
    "content": "from crewai import Agent, Task, Crew\n\nresearcher = Agent(role=\"Researcher\", goal=\"Find information\")\nwriter = Agent(role=\"Writer\", goal=\"Create content\")\ncrew = Crew(agents=[researcher, writer], tasks=[...])",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 128,
    "line_end": 144,
    "type": "python",
    "description": "`",
    "content": "# Using LangChain\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n\ntools = [\n    DuckDuckGoSearchRun(),\n    WikipediaQueryRun()\n]\n\nresearch_agent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\nresult = research_agent.run(\"What are the latest developments in quantum computing?\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 150,
    "line_end": 166,
    "type": "python",
    "description": "`",
    "content": "# Using AutoGen\ncoding_assistant = AssistantAgent(\n    \"coding_assistant\",\n    system_message=\"You are a helpful AI that writes and explains code.\",\n    llm_config={\"model\": \"gpt-4\"}\n)\n\nuser_proxy = UserProxyAgent(\n    \"user_proxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n)\n\nuser_proxy.initiate_chat(\n    coding_assistant,\n    message=\"Write a Python function to calculate fibonacci numbers efficiently\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "examples/output/readme_report/research_report.md",
    "line_start": 172,
    "line_end": 206,
    "type": "python",
    "description": "`",
    "content": "# Using CrewAI\nfrom crewai import Agent, Task, Crew, Process\n\n# Define agents\nresearcher = Agent(\n    role='Senior Research Analyst',\n    goal='Uncover cutting-edge developments in AI',\n    backstory=\"You're an expert researcher with a keen eye for detail.\"\n)\n\nwriter = Agent(\n    role='Tech Content Strategist',\n    goal='Create compelling content about AI developments',\n    backstory=\"You're a skilled writer who makes complex topics accessible.\"\n)\n\n# Define tasks\nresearch_task = Task(\n    description='Research the latest AI developments in the past month',\n    agent=researcher\n)\n\nwriting_task = Task(\n    description='Write a blog post about the research findings',\n    agent=writer\n)\n\n# Create crew\ncrew = Crew(\n    agents=[researcher, writer],\n    tasks=[research_task, writing_task],\n    process=Process.sequential\n)\n\nresult = crew.kickoff()",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 57,
    "line_end": 64,
    "type": "bash",
    "description": "2. Dependencies: Install the orchestrator package and dependencies",
    "content": "# Install the package\npip install -e .\n\n# Install Jupyter (if not already installed)\npip install jupyter\n\n# Start Jupyter\njupyter notebook",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 79,
    "line_end": 81,
    "type": "python",
    "description": "If you're running from the source repository:",
    "content": "# Add the src directory to your Python path (included in notebooks)\nimport sys\nsys.path.insert(0, '../src')",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 109,
    "line_end": 115,
    "type": "python",
    "description": "The tutorials use mock models for demonstration. To work with real AI models:",
    "content": "from orchestrator.integrations.openai_model import OpenAIModel\n\nmodel = OpenAIModel(\n    name=\"gpt-4\",\n    api_key=\"your-openai-api-key\",\n    model=\"gpt-4\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 120,
    "line_end": 126,
    "type": "python",
    "description": "`",
    "content": "from orchestrator.integrations.anthropic_model import AnthropicModel\n\nmodel = AnthropicModel(\n    name=\"claude-3-sonnet\",\n    api_key=\"your-anthropic-api-key\",\n    model=\"claude-3-sonnet-20240229\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 131,
    "line_end": 136,
    "type": "python",
    "description": "`",
    "content": "from orchestrator.integrations.huggingface_model import HuggingFaceModel\n\nmodel = HuggingFaceModel(\n    name=\"llama-7b\",\n    model_path=\"meta-llama/Llama-2-7b-chat-hf\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 155,
    "line_end": 157,
    "type": "python",
    "description": "Import Errors",
    "content": "# Make sure the src path is correctly added\nimport sys\nsys.path.insert(0, '../src')",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 162,
    "line_end": 163,
    "type": "python",
    "description": "Mock Model Responses",
    "content": "# Mock models require explicit response configuration\nmodel.set_response(\"your prompt\", \"expected response\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notebooks/README.md",
    "line_start": 168,
    "line_end": 169,
    "type": "python",
    "description": "Async/Await Issues",
    "content": "# Use await in Jupyter notebook cells (notebook-only syntax)\nresult = await orchestrator.execute_pipeline(pipeline)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/design_compliance_achievement.md",
    "line_start": 53,
    "line_end": 60,
    "type": "text",
    "description": "- Clarity: High - Includes tutorials and documentation",
    "content": "src/orchestrator/\n├── core/                    # Core abstractions (Task, Pipeline, Model, etc.)\n├── compiler/               # YAML parsing and compilation\n├── executor/              # Execution engines (sandboxed, parallel)\n├── adapters/              # Control system adapters (LangGraph, MCP)\n├── models/                # Model registry and selection\n├── state/                 # State management and checkpointing\n└── orchestrator.py        # Main orchestration engine",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/phase2_completion_summary.md",
    "line_start": 73,
    "line_end": 77,
    "type": "bash",
    "description": "- Real-world Patterns: Demonstrates dependency management, error handling, resource allocation",
    "content": "✅ OpenAI model integration loads successfully\n✅ Anthropic model integration loads successfully\n✅ Google model integration loads successfully\n✅ HuggingFace model integration loads successfully\n✅ All model integrations imported successfully",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/phase2_completion_summary.md",
    "line_start": 82,
    "line_end": 85,
    "type": "bash",
    "description": "`",
    "content": "✅ YAML compilation successful\n  - Pipeline ID: test_pipeline\n  - Tasks: 2\n  - AUTO resolved method: Mock response for: You are an AI pipeline orchestration expert...",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/phase2_completion_summary.md",
    "line_start": 90,
    "line_end": 92,
    "type": "bash",
    "description": "`",
    "content": "🚀 Starting orchestrator test...\n❌ Pipeline execution failed: Task 'hello' failed and policy is 'fail'\nError: NoEligibleModelsError - No models meet the specified requirements",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/phase2_completion_summary.md",
    "line_start": 135,
    "line_end": 139,
    "type": "yaml",
    "description": "Revolutionary approach to pipeline ambiguity resolution:",
    "content": "# Before: Manual specification required\nanalysis_method: \"statistical\"\n\n# After: AI-resolved automatically\nanalysis_method: <AUTO>Choose the best analysis method for this data</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/phase2_completion_summary.md",
    "line_start": 145,
    "line_end": 152,
    "type": "python",
    "description": "Seamless integration across providers:",
    "content": "# Automatically selects best model for each task\n# Note: Use in Jupyter notebooks or wrap in async function\npipeline = await orchestrator.execute_yaml(\"\"\"\nsteps:\n  - action: generate    # Uses GPT for generation\n  - action: analyze     # Uses Claude for analysis\n  - action: transform   # Uses Gemini for transformation\n\"\"\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/session_summary_context_limit.md",
    "line_start": 45,
    "line_end": 49,
    "type": "text",
    "description": "Core Framework Structure:",
    "content": "# Main abstractions in src/orchestrator/core/\n- task.py:Task, TaskStatus (lines 1-200+)\n- pipeline.py:Pipeline (lines 1-300+)\n- model.py:Model, ModelCapabilities (lines 1-250+)\n- control_system.py:ControlSystem, ControlAction (lines 1-150+)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/session_summary_context_limit.md",
    "line_start": 54,
    "line_end": 60,
    "type": "text",
    "description": "Advanced Components:",
    "content": "# Advanced features in src/orchestrator/\n- core/error_handler.py:ErrorHandler, CircuitBreaker (lines 1-400+)\n- core/cache.py:MultiLevelCache (lines 1-550+)\n- core/resource_allocator.py:ResourceAllocator (lines 1-450+)\n- executor/parallel_executor.py:ParallelExecutor (lines 1-425+)\n- executor/sandboxed_executor.py:SandboxManager (lines 1-345+)\n- state/adaptive_checkpoint.py:AdaptiveCheckpointManager (lines 1-400+)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "notes/session_summary_context_limit.md",
    "line_start": 65,
    "line_end": 67,
    "type": "text",
    "description": "Control System Adapters:",
    "content": "# Adapters in src/orchestrator/adapters/\n- langgraph_adapter.py:LangGraphAdapter (lines 1-350+)\n- mcp_adapter.py:MCPAdapter (lines 1-450+)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/api/compiler.rst",
    "line_start": 22,
    "line_end": 26,
    "type": "python",
    "description": "**Example Usage:**",
    "content": "from orchestrator.compiler import YAMLCompiler\n\ncompiler = YAMLCompiler()\npipeline = compiler.compile_file(\"my_pipeline.yaml\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/api/core.rst",
    "line_start": 108,
    "line_end": 131,
    "type": "python",
    "description": "~~~~~~~~~~~",
    "content": "import asyncio\nfrom orchestrator import Task, Pipeline, Orchestrator\n\nasync def main():\n    # Create a task\n    task = Task(\n        id=\"hello\",\n        name=\"Hello Task\",\n        action=\"generate_text\",\n        parameters={\"prompt\": \"Hello, world!\"}\n    )\n\n    # Create a pipeline\n    pipeline = Pipeline(id=\"demo\", name=\"Demo Pipeline\")\n    pipeline.add_task(task)\n\n    # Execute with orchestrator\n    orchestrator = Orchestrator()\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run the pipeline\nresult = asyncio.run(main())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/api/core.rst",
    "line_start": 137,
    "line_end": 146,
    "type": "yaml",
    "description": "~~~~~~~~~~~~~~~~~~",
    "content": "id: demo_pipeline\nname: Demo Pipeline\n\ntasks:\n  - id: hello\n    name: Hello Task\n    action: generate_text\n    parameters:\n      prompt: \"Hello, world!\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/api/core.rst",
    "line_start": 152,
    "line_end": 165,
    "type": "python",
    "description": "~~~~~~~~~~~~~~",
    "content": "import asyncio\nfrom orchestrator.core.error_handler import ErrorHandler\n\nasync def run_with_error_handling():\n    error_handler = ErrorHandler()\n    orchestrator = Orchestrator(error_handler=error_handler)\n\n    # Tasks will automatically retry on failure\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with error handling\nresult = asyncio.run(run_with_error_handling())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/development/github_actions.rst",
    "line_start": 67,
    "line_end": 72,
    "type": "bash",
    "description": "If you prefer to update badges manually, you can extract coverage from the test output:",
    "content": "# Run tests with coverage\npytest --cov=src/orchestrator --cov-report=term\n\n# The output will show coverage percentage\n# Update the README badge URL with the percentage",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/development/github_actions.rst",
    "line_start": 93,
    "line_end": 100,
    "type": "markdown",
    "description": "You can customize badge colors and styles:",
    "content": "# Different styles\n![Badge](https://img.shields.io/badge/style-flat-green)\n![Badge](https://img.shields.io/badge/style-flat--square-green?style=flat-square)\n![Badge](https://img.shields.io/badge/style-for--the--badge-green?style=for-the-badge)\n\n# Custom colors\n![Badge](https://img.shields.io/badge/custom-color-ff69b4)\n![Badge](https://img.shields.io/badge/custom-color-blueviolet)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 28,
    "line_end": 40,
    "type": "python",
    "description": "* **Dependencies** - Other tasks that must complete first",
    "content": "from orchestrator import Task\n\ntask = Task(\n    id=\"summarize\",\n    name=\"Summarize Document\",\n    action=\"generate_text\",\n    parameters={\n        \"prompt\": \"Summarize this document: {document}\",\n        \"max_tokens\": 150\n    },\n    dependencies=[\"extract_document\"]\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 48,
    "line_end": 59,
    "type": "python",
    "description": "A **Pipeline** is a collection of tasks with defined dependencies. It represents your complete workflow:",
    "content": "from orchestrator import Pipeline\n\npipeline = Pipeline(\n    id=\"document_processing\",\n    name=\"Document Processing Pipeline\"\n)\n\n# Add tasks to pipeline\npipeline.add_task(extract_task)\npipeline.add_task(summarize_task)\npipeline.add_task(classify_task)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 71,
    "line_end": 78,
    "type": "python",
    "description": "* **Custom models** (your own implementations)",
    "content": "from orchestrator.models import OpenAIModel\n\nmodel = OpenAIModel(\n    name=\"gpt-4\",\n    api_key=\"your-api-key\",\n    model=\"gpt-4\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 91,
    "line_end": 103,
    "type": "python",
    "description": "* Manages state and checkpointing",
    "content": "import asyncio\nfrom orchestrator import Orchestrator\n\nasync def run_pipeline():\n    orchestrator = Orchestrator()\n    orchestrator.register_model(model)\n\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run the pipeline\nresult = asyncio.run(run_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 111,
    "line_end": 121,
    "type": "python",
    "description": "Tasks can depend on other tasks, creating a directed acyclic graph (DAG):",
    "content": "# Task A (no dependencies)\ntask_a = Task(id=\"a\", name=\"Task A\", action=\"generate_text\")\n\n# Task B depends on A\ntask_b = Task(id=\"b\", name=\"Task B\", action=\"generate_text\",\n              dependencies=[\"a\"])\n\n# Task C depends on A and B\ntask_c = Task(id=\"c\", name=\"Task C\", action=\"generate_text\",\n              dependencies=[\"a\", \"b\"])",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 129,
    "line_end": 132,
    "type": "text",
    "description": "The orchestrator automatically determines execution order based on dependencies:",
    "content": "Level 0: [Task A]           # No dependencies\nLevel 1: [Task B]           # Depends on A\nLevel 2: [Task C]           # Depends on A and B",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 142,
    "line_end": 156,
    "type": "python",
    "description": "Tasks can reference outputs from other tasks using template syntax:",
    "content": "task_a = Task(\n    id=\"extract\",\n    name=\"Extract Information\",\n    action=\"generate_text\",\n    parameters={\"prompt\": \"Extract key facts from: {document}\"}\n)\n\ntask_b = Task(\n    id=\"summarize\",\n    name=\"Summarize Facts\",\n    action=\"generate_text\",\n    parameters={\"prompt\": \"Summarize these facts: {extract}\"},\n    dependencies=[\"extract\"]\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 171,
    "line_end": 184,
    "type": "python",
    "description": "6. **Returns** results from all tasks",
    "content": "import asyncio\n\nasync def execute_and_process():\n    # Execute pipeline\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    # Access individual task results\n    print(result[\"extract\"])    # Output from extract task\n    print(result[\"summarize\"])  # Output from summarize task\n    return result\n\n# Run the execution\nresult = asyncio.run(execute_and_process())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 197,
    "line_end": 211,
    "type": "python",
    "description": "* **Cost** - Resource usage and API costs",
    "content": "import asyncio\n\nasync def run_with_model_selection():\n    # Register multiple models\n    orchestrator.register_model(gpt4_model)\n    orchestrator.register_model(claude_model)\n    orchestrator.register_model(local_model)\n\n    # Orchestrator will select best model for each task\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with model selection\nresult = asyncio.run(run_with_model_selection())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 222,
    "line_end": 235,
    "type": "python",
    "description": "~~~~~~~~~~~~~~~~",
    "content": "import asyncio\nfrom orchestrator.core.error_handler import ErrorHandler\n\nasync def run_with_retry():\n    error_handler = ErrorHandler()\n    orchestrator = Orchestrator(error_handler=error_handler)\n\n    # Tasks will automatically retry on failure\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with retry handling\nresult = asyncio.run(run_with_retry())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 241,
    "line_end": 253,
    "type": "python",
    "description": "~~~~~~~~~~~~~~~~",
    "content": "import asyncio\n\nasync def run_with_circuit_breaker():\n    # Circuit breaker prevents cascading failures\n    breaker = error_handler.get_circuit_breaker(\"openai_api\")\n\n    # Executes with circuit breaker protection\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with circuit breaker\nresult = asyncio.run(run_with_circuit_breaker())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 259,
    "line_end": 272,
    "type": "python",
    "description": "~~~~~~~~~~~~~~~",
    "content": "import asyncio\n\nasync def run_with_fallback():\n    # Register models in order of preference\n    orchestrator.register_model(primary_model)\n    orchestrator.register_model(fallback_model)\n\n    # Will use fallback if primary fails\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with fallback support\nresult = asyncio.run(run_with_fallback())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 283,
    "line_end": 296,
    "type": "python",
    "description": "~~~~~~~~~~~~~",
    "content": "import asyncio\nfrom orchestrator.state import StateManager\n\nasync def run_with_checkpointing():\n    state_manager = StateManager(storage_path=\"./checkpoints\")\n    orchestrator = Orchestrator(state_manager=state_manager)\n\n    # Automatically saves checkpoints during execution\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with checkpointing\nresult = asyncio.run(run_with_checkpointing())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 302,
    "line_end": 311,
    "type": "python",
    "description": "~~~~~~~~",
    "content": "import asyncio\n\nasync def resume_from_checkpoint():\n    # Resume from last checkpoint\n    result = await orchestrator.resume_pipeline(\"pipeline_id\")\n    return result\n\n# Resume execution\nresult = asyncio.run(resume_from_checkpoint())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 319,
    "line_end": 336,
    "type": "yaml",
    "description": "Define pipelines declaratively in YAML:",
    "content": "id: document_pipeline\nname: Document Processing Pipeline\n\ntasks:\n  - id: extract\n    name: Extract Information\n    action: generate_text\n    parameters:\n      prompt: \"Extract key facts from: {document}\"\n\n  - id: summarize\n    name: Summarize Facts\n    action: generate_text\n    parameters:\n      prompt: \"Summarize these facts: {extract}\"\n    dependencies:\n      - extract",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 341,
    "line_end": 353,
    "type": "python",
    "description": "Load and execute:",
    "content": "import asyncio\nfrom orchestrator.compiler import YAMLCompiler\n\nasync def run_yaml_pipeline():\n    compiler = YAMLCompiler()\n    pipeline = compiler.compile_file(\"document_pipeline.yaml\")\n\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run YAML pipeline\nresult = asyncio.run(run_yaml_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 362,
    "line_end": 375,
    "type": "python",
    "description": "~~~~~~~~~~~~~~~~~~",
    "content": "import asyncio\nfrom orchestrator.core.resource_allocator import ResourceAllocator\n\nasync def run_with_resource_management():\n    allocator = ResourceAllocator()\n    orchestrator = Orchestrator(resource_allocator=allocator)\n\n    # Automatically manages CPU, memory, and API quotas\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with resource management\nresult = asyncio.run(run_with_resource_management())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 381,
    "line_end": 394,
    "type": "python",
    "description": "~~~~~~~~~~~~~~~~~~",
    "content": "import asyncio\nfrom orchestrator.executor import ParallelExecutor\n\nasync def run_parallel_execution():\n    executor = ParallelExecutor(max_workers=4)\n    orchestrator = Orchestrator(executor=executor)\n\n    # Independent tasks run in parallel\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with parallel execution\nresult = asyncio.run(run_parallel_execution())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/basic_concepts.rst",
    "line_start": 400,
    "line_end": 413,
    "type": "python",
    "description": "~~~~~~~",
    "content": "import asyncio\nfrom orchestrator.core.cache import MultiLevelCache\n\nasync def run_with_caching():\n    cache = MultiLevelCache()\n    orchestrator = Orchestrator(cache=cache)\n\n    # Results are cached for faster subsequent runs\n    result = await orchestrator.execute_pipeline(pipeline)\n    return result\n\n# Run with caching\nresult = asyncio.run(run_with_caching())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 26,
    "line_end": 27,
    "type": "bash",
    "description": "Install Orchestrator using pip:",
    "content": "pip install py-orc",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 37,
    "line_end": 40,
    "type": "bash",
    "description": "To install from source for development:",
    "content": "git clone https://github.com/ContextLab/orchestrator.git\ncd orchestrator\npip install -e .",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 56,
    "line_end": 57,
    "type": "bash",
    "description": "For sandboxed execution with Docker:",
    "content": "pip install py-orc[docker]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 64,
    "line_end": 65,
    "type": "bash",
    "description": "For persistent state storage:",
    "content": "pip install py-orc[database]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 72,
    "line_end": 73,
    "type": "bash",
    "description": "For all optional dependencies:",
    "content": "pip install py-orc[all]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 81,
    "line_end": 92,
    "type": "python",
    "description": "Verify your installation by running:",
    "content": "import orchestrator\nprint(f\"Orchestrator version: {orchestrator.__version__}\")\n\n# Test basic functionality\nfrom orchestrator import Task, Pipeline\n\ntask = Task(id=\"test\", name=\"Test Task\", action=\"echo\", parameters={\"message\": \"Hello!\"})\npipeline = Pipeline(id=\"test_pipeline\", name=\"Test Pipeline\")\npipeline.add_task(task)\n\nprint(\"✅ Installation successful!\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 105,
    "line_end": 113,
    "type": "bash",
    "description": "Set these environment variables for optimal performance:",
    "content": "# Optional: Set cache directory\nexport ORCHESTRATOR_CACHE_DIR=/path/to/cache\n\n# Optional: Set checkpoint directory\nexport ORCHESTRATOR_CHECKPOINT_DIR=/path/to/checkpoints\n\n# Optional: Set log level\nexport ORCHESTRATOR_LOG_LEVEL=INFO",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 121,
    "line_end": 129,
    "type": "bash",
    "description": "Configure API keys for external services:",
    "content": "# OpenAI\nexport OPENAI_API_KEY=your_openai_key\n\n# Anthropic\nexport ANTHROPIC_API_KEY=your_anthropic_key\n\n# Google\nexport GOOGLE_API_KEY=your_google_key",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 137,
    "line_end": 139,
    "type": "bash",
    "description": "If using Docker features, ensure Docker is running:",
    "content": "docker --version\ndocker run hello-world",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 157,
    "line_end": 166,
    "type": "bash",
    "description": "Install system dependencies:",
    "content": "# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install python3-dev build-essential\n\n# macOS\nbrew install python\n\n# Windows\n# Use Python from python.org",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/installation.rst",
    "line_start": 172,
    "line_end": 174,
    "type": "bash",
    "description": "Ensure Docker is installed and running:",
    "content": "docker --version\ndocker info",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 13,
    "line_end": 53,
    "type": "python",
    "description": "Let's create a simple text generation pipeline:",
    "content": "from orchestrator import Orchestrator, Task, Pipeline\nfrom orchestrator.models.mock_model import MockModel\n\n# Create a mock model for testing\nmodel = MockModel(\"gpt-test\")\nmodel.set_response(\"Hello, world!\", \"Hello! How can I help you today?\")\n\n# Create a task\ntask = Task(\n    id=\"greeting\",\n    name=\"Generate Greeting\",\n    action=\"generate_text\",\n    parameters={\"prompt\": \"Hello, world!\"}\n)\n\n# Create a pipeline\npipeline = Pipeline(id=\"hello_pipeline\", name=\"Hello Pipeline\")\npipeline.add_task(task)\n\n# Create orchestrator and register model\norchestrator = Orchestrator()\norchestrator.register_model(model)\n\n# Execute pipeline\nimport asyncio\n\n\n\nasync def run_pipeline():\n\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    return result\n\n\n\n# Run the pipeline\n\nresult = asyncio.run(run_pipeline())\nprint(f\"Result: {result['greeting']}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 61,
    "line_end": 114,
    "type": "python",
    "description": "Let's create a more complex pipeline with multiple tasks:",
    "content": "from orchestrator import Task, Pipeline\n\n# Task 1: Generate story outline\noutline_task = Task(\n    id=\"outline\",\n    name=\"Generate Story Outline\",\n    action=\"generate_text\",\n    parameters={\"prompt\": \"Create a story outline about space exploration\"}\n)\n\n# Task 2: Write story (depends on outline)\nstory_task = Task(\n    id=\"story\",\n    name=\"Write Story\",\n    action=\"generate_text\",\n    parameters={\"prompt\": \"Write a story based on: {outline}\"},\n    dependencies=[\"outline\"]\n)\n\n# Task 3: Summarize story (depends on story)\nsummary_task = Task(\n    id=\"summary\",\n    name=\"Summarize Story\",\n    action=\"generate_text\",\n    parameters={\"prompt\": \"Summarize this story: {story}\"},\n    dependencies=[\"story\"]\n)\n\n# Create pipeline with all tasks\npipeline = Pipeline(id=\"story_pipeline\", name=\"Story Creation Pipeline\")\npipeline.add_task(outline_task)\npipeline.add_task(story_task)\npipeline.add_task(summary_task)\n\n# Execute pipeline\nimport asyncio\n\n\n\nasync def run_pipeline():\n\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    return result\n\n\n\n# Run the pipeline\n\nresult = asyncio.run(run_pipeline())\nprint(f\"Outline: {result['outline']}\")\nprint(f\"Story: {result['story']}\")\nprint(f\"Summary: {result['summary']}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 122,
    "line_end": 148,
    "type": "yaml",
    "description": "You can also define pipelines in YAML:",
    "content": "# story_pipeline.yaml\nid: story_pipeline\nname: Story Creation Pipeline\n\ntasks:\n  - id: outline\n    name: Generate Story Outline\n    action: generate_text\n    parameters:\n      prompt: \"Create a story outline about space exploration\"\n\n  - id: story\n    name: Write Story\n    action: generate_text\n    parameters:\n      prompt: \"Write a story based on: {outline}\"\n    dependencies:\n      - outline\n\n  - id: summary\n    name: Summarize Story\n    action: generate_text\n    parameters:\n      prompt: \"Summarize this story: {story}\"\n    dependencies:\n      - story",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 153,
    "line_end": 175,
    "type": "python",
    "description": "Load and execute the YAML pipeline:",
    "content": "from orchestrator.compiler import YAMLCompiler\n\n# Load pipeline from YAML\ncompiler = YAMLCompiler()\npipeline = compiler.compile_file(\"story_pipeline.yaml\")\n\n# Execute pipeline\nimport asyncio\n\n\n\nasync def run_pipeline():\n\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    return result\n\n\n\n# Run the pipeline\n\nresult = asyncio.run(run_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 183,
    "line_end": 211,
    "type": "python",
    "description": "Let's use a real AI model instead of the mock:",
    "content": "from orchestrator.models.openai_model import OpenAIModel\n\n# Create OpenAI model\nopenai_model = OpenAIModel(\n    name=\"gpt-4\",\n    api_key=\"your-api-key-here\",\n    model=\"gpt-4\"\n)\n\n# Register model\norchestrator.register_model(openai_model)\n\n# Execute pipeline (will use OpenAI)\nimport asyncio\n\n\n\nasync def run_pipeline():\n\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    return result\n\n\n\n# Run the pipeline\n\nresult = asyncio.run(run_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 219,
    "line_end": 246,
    "type": "python",
    "description": "Orchestrator provides built-in error handling:",
    "content": "from orchestrator.core.error_handler import ErrorHandler\n\n# Create error handler with retry strategy\nerror_handler = ErrorHandler()\n\n# Configure orchestrator with error handling\norchestrator = Orchestrator(error_handler=error_handler)\n\n# Execute pipeline with automatic retry on failures\ntry:\n    import asyncio\n\n\n\n    async def run_pipeline():\n\n        result = await orchestrator.execute_pipeline(pipeline)\n\n        return result\n\n\n\n    # Run the pipeline\n\n    result = asyncio.run(run_pipeline())\nexcept Exception as e:\n    print(f\"Pipeline failed: {e}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 254,
    "line_end": 278,
    "type": "python",
    "description": "Enable checkpointing for long-running pipelines:",
    "content": "from orchestrator.state import StateManager\n\n# Create state manager\nstate_manager = StateManager(storage_path=\"./checkpoints\")\n\n# Configure orchestrator with state management\norchestrator = Orchestrator(state_manager=state_manager)\n\n# Execute pipeline with automatic checkpointing\nimport asyncio\n\n\n\nasync def run_pipeline():\n\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    return result\n\n\n\n# Run the pipeline\n\nresult = asyncio.run(run_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/quickstart.rst",
    "line_start": 286,
    "line_end": 312,
    "type": "python",
    "description": "Enable monitoring to track pipeline execution:",
    "content": "import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Execute pipeline with logging\nimport asyncio\n\n\n\nasync def run_pipeline():\n\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    return result\n\n\n\n# Run the pipeline\n\nresult = asyncio.run(run_pipeline())\n\n# Get execution statistics\nstats = orchestrator.get_execution_stats()\nprint(f\"Execution time: {stats['total_time']:.2f}s\")\nprint(f\"Tasks completed: {stats['completed_tasks']}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 24,
    "line_end": 46,
    "type": "python",
    "description": "First, let's set up our environment:",
    "content": "import asyncio\nfrom orchestrator import Orchestrator, Task, Pipeline\nfrom orchestrator.models.mock_model import MockModel\n\n# Create a mock model for testing\nmodel = MockModel(\"research_assistant\")\n\n# Set up responses for our mock model\nmodel.set_response(\n    \"Generate 3 research questions about: artificial intelligence\",\n    \"1. How does AI impact job markets?\\n2. What are the ethical implications of AI?\\n3. How can AI be made more accessible?\"\n)\n\nmodel.set_response(\n    \"Analyze these questions and identify key themes: 1. How does AI impact job markets?\\n2. What are the ethical implications of AI?\\n3. How can AI be made more accessible?\",\n    \"Key themes identified: Economic Impact, Ethics and Responsibility, Accessibility and Democratization\"\n)\n\nmodel.set_response(\n    \"Write a comprehensive report on artificial intelligence covering these themes: Economic Impact, Ethics and Responsibility, Accessibility and Democratization\",\n    \"# AI Research Report\\n\\n## Economic Impact\\nAI is reshaping job markets...\\n\\n## Ethics and Responsibility\\nAI systems must be developed responsibly...\\n\\n## Accessibility and Democratization\\nMaking AI tools accessible to all...\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 54,
    "line_end": 88,
    "type": "python",
    "description": "Now let's create our three tasks:",
    "content": "# Task 1: Generate research questions\nresearch_task = Task(\n    id=\"research_questions\",\n    name=\"Generate Research Questions\",\n    action=\"generate_text\",\n    parameters={\n        \"prompt\": \"Generate 3 research questions about: {topic}\",\n        \"max_tokens\": 200\n    }\n)\n\n# Task 2: Analyze questions for themes\nanalysis_task = Task(\n    id=\"analyze_themes\",\n    name=\"Analyze Key Themes\",\n    action=\"generate_text\",\n    parameters={\n        \"prompt\": \"Analyze these questions and identify key themes: {research_questions}\",\n        \"max_tokens\": 150\n    },\n    dependencies=[\"research_questions\"]  # Depends on research task\n)\n\n# Task 3: Write comprehensive report\nreport_task = Task(\n    id=\"write_report\",\n    name=\"Write Research Report\",\n    action=\"generate_text\",\n    parameters={\n        \"prompt\": \"Write a comprehensive report on {topic} covering these themes: {analyze_themes}\",\n        \"max_tokens\": 500\n    },\n    dependencies=[\"analyze_themes\"]  # Depends on analysis task\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 96,
    "line_end": 114,
    "type": "python",
    "description": "Combine tasks into a pipeline:",
    "content": "# Create pipeline\npipeline = Pipeline(\n    id=\"research_assistant\",\n    name=\"Research Assistant Pipeline\",\n    description=\"Generates research questions, analyzes themes, and writes a report\"\n)\n\n# Add tasks to pipeline\npipeline.add_task(research_task)\npipeline.add_task(analysis_task)\npipeline.add_task(report_task)\n\n# Set initial context\npipeline.set_context(\"topic\", \"artificial intelligence\")\n\nprint(\"Pipeline created successfully!\")\nprint(f\"Tasks: {list(pipeline.tasks.keys())}\")\nprint(f\"Execution order: {pipeline.get_execution_order()}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 122,
    "line_end": 148,
    "type": "python",
    "description": "Now let's execute our pipeline:",
    "content": "async def run_pipeline():\n    # Create orchestrator\n    orchestrator = Orchestrator()\n\n    # Register our model\n    orchestrator.register_model(model)\n\n    print(\"Starting pipeline execution...\")\n\n    # Execute pipeline\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    print(\"\\n=== Pipeline Results ===\")\n    print(f\"Research Questions:\\n{result['research_questions']}\\n\")\n    print(f\"Key Themes:\\n{result['analyze_themes']}\\n\")\n    print(f\"Final Report:\\n{result['write_report']}\\n\")\n\n    return result\n\n# Run the pipeline\n# Note: In Jupyter notebooks, you can use top-level await:\n# result = await run_pipeline()\n\n# In regular Python scripts, use asyncio.run():\nimport asyncio\nresult = asyncio.run(run_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 156,
    "line_end": 188,
    "type": "python",
    "description": "Let's make our pipeline more robust:",
    "content": "from orchestrator.core.error_handler import ErrorHandler\nfrom orchestrator.core.error_handler import ExponentialBackoffRetry\n\nasync def run_robust_pipeline():\n    # Create error handler with retry strategy\n    error_handler = ErrorHandler()\n    error_handler.register_retry_strategy(\n        \"research_retry\",\n        ExponentialBackoffRetry(max_retries=3, base_delay=1.0)\n    )\n\n    # Create orchestrator with error handling\n    orchestrator = Orchestrator(error_handler=error_handler)\n    orchestrator.register_model(model)\n\n    try:\n        print(\"Starting robust pipeline execution...\")\n        result = await orchestrator.execute_pipeline(pipeline)\n        print(\"✅ Pipeline completed successfully!\")\n        return result\n\n    except Exception as e:\n        print(f\"❌ Pipeline failed: {e}\")\n        # Get execution statistics\n        stats = error_handler.get_error_statistics()\n        print(f\"Errors encountered: {stats['total_errors']}\")\n        return None\n\n# Run robust pipeline\n# In Jupyter notebooks: result = await run_robust_pipeline()\n# In regular Python scripts:\nresult = asyncio.run(run_robust_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 196,
    "line_end": 223,
    "type": "python",
    "description": "For longer pipelines, add checkpointing:",
    "content": "from orchestrator.state import StateManager\n\nasync def run_stateful_pipeline():\n    # Create state manager\n    state_manager = StateManager(storage_path=\"./checkpoints\")\n\n    # Create orchestrator with state management\n    orchestrator = Orchestrator(state_manager=state_manager)\n    orchestrator.register_model(model)\n\n    print(\"Starting stateful pipeline execution...\")\n\n    # Execute with automatic checkpointing\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    print(\"✅ Pipeline completed with checkpointing!\")\n\n    # List checkpoints created\n    checkpoints = await state_manager.list_checkpoints(\"research_assistant\")\n    print(f\"Checkpoints created: {len(checkpoints)}\")\n\n    return result\n\n# Run stateful pipeline\n# In Jupyter notebooks: result = await run_stateful_pipeline()\n# In regular Python scripts:\nresult = asyncio.run(run_stateful_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 231,
    "line_end": 264,
    "type": "yaml",
    "description": "Let's convert our pipeline to YAML:",
    "content": "# research_pipeline.yaml\nid: research_assistant\nname: Research Assistant Pipeline\ndescription: Generates research questions, analyzes themes, and writes a report\n\ncontext:\n  topic: artificial intelligence\n\ntasks:\n  - id: research_questions\n    name: Generate Research Questions\n    action: generate_text\n    parameters:\n      prompt: \"Generate 3 research questions about: {topic}\"\n      max_tokens: 200\n\n  - id: analyze_themes\n    name: Analyze Key Themes\n    action: generate_text\n    parameters:\n      prompt: \"Analyze these questions and identify key themes: {research_questions}\"\n      max_tokens: 150\n    dependencies:\n      - research_questions\n\n  - id: write_report\n    name: Write Research Report\n    action: generate_text\n    parameters:\n      prompt: \"Write a comprehensive report on {topic} covering these themes: {analyze_themes}\"\n      max_tokens: 500\n    dependencies:\n      - analyze_themes",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 269,
    "line_end": 292,
    "type": "python",
    "description": "Load and execute the YAML pipeline:",
    "content": "from orchestrator.compiler import YAMLCompiler\n\nasync def run_yaml_pipeline():\n    # Create compiler and load pipeline\n    compiler = YAMLCompiler()\n    pipeline = compiler.compile_file(\"research_pipeline.yaml\")\n\n    # Create orchestrator\n    orchestrator = Orchestrator()\n    orchestrator.register_model(model)\n\n    print(\"Starting YAML pipeline execution...\")\n\n    # Execute pipeline\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    print(\"✅ YAML pipeline completed!\")\n    return result\n\n# Run YAML pipeline\n# In Jupyter notebooks: result = await run_yaml_pipeline()\n# In regular Python scripts:\nresult = asyncio.run(run_yaml_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 300,
    "line_end": 325,
    "type": "python",
    "description": "Replace mock model with real AI:",
    "content": "from orchestrator.models.openai_model import OpenAIModel\n\nasync def run_with_real_ai():\n    # Create OpenAI model\n    openai_model = OpenAIModel(\n        name=\"gpt-4\",\n        api_key=\"your-openai-api-key\",\n        model=\"gpt-4\"\n    )\n\n    # Create orchestrator with real AI\n    orchestrator = Orchestrator()\n    orchestrator.register_model(openai_model)\n\n    print(\"Starting pipeline with real AI...\")\n\n    # Execute pipeline with real AI\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    print(\"✅ Real AI pipeline completed!\")\n    return result\n\n# Run with real AI (uncomment when you have API keys)\n# In Jupyter notebooks: result = await run_with_real_ai()\n# In regular Python scripts: result = asyncio.run(run_with_real_ai())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 333,
    "line_end": 365,
    "type": "python",
    "description": "Add monitoring to track performance:",
    "content": "import time\nfrom orchestrator.core.resource_allocator import ResourceAllocator\n\nasync def run_monitored_pipeline():\n    # Create resource allocator for monitoring\n    allocator = ResourceAllocator()\n\n    # Create orchestrator with monitoring\n    orchestrator = Orchestrator(resource_allocator=allocator)\n    orchestrator.register_model(model)\n\n    print(\"Starting monitored pipeline execution...\")\n    start_time = time.time()\n\n    # Execute pipeline\n    result = await orchestrator.execute_pipeline(pipeline)\n\n    end_time = time.time()\n    execution_time = end_time - start_time\n\n    print(f\"✅ Pipeline completed in {execution_time:.2f} seconds\")\n\n    # Get resource statistics\n    stats = allocator.get_overall_statistics()\n    print(f\"Resource utilization: {stats['overall_utilization']:.2f}\")\n\n    return result\n\n# Run monitored pipeline\n# In Jupyter notebooks: result = await run_monitored_pipeline()\n# In regular Python scripts:\nresult = asyncio.run(run_monitored_pipeline())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/getting_started/your_first_pipeline.rst",
    "line_start": 373,
    "line_end": 488,
    "type": "python",
    "description": "Here's the complete, production-ready pipeline:",
    "content": "import asyncio\nimport logging\nfrom orchestrator import Orchestrator, Task, Pipeline\nfrom orchestrator.models.mock_model import MockModel\nfrom orchestrator.core.error_handler import ErrorHandler\nfrom orchestrator.state import StateManager\nfrom orchestrator.core.resource_allocator import ResourceAllocator\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def create_research_pipeline():\n    \"\"\"Create a production-ready research assistant pipeline.\"\"\"\n\n    # Create mock model with responses\n    model = MockModel(\"research_assistant\")\n    model.set_response(\n        \"Generate 3 research questions about: artificial intelligence\",\n        \"1. How does AI impact job markets?\\n2. What are the ethical implications of AI?\\n3. How can AI be made more accessible?\"\n    )\n    model.set_response(\n        \"Analyze these questions and identify key themes: 1. How does AI impact job markets?\\n2. What are the ethical implications of AI?\\n3. How can AI be made more accessible?\",\n        \"Key themes: Economic Impact, Ethics and Responsibility, Accessibility\"\n    )\n    model.set_response(\n        \"Write a comprehensive report on artificial intelligence covering these themes: Economic Impact, Ethics and Responsibility, Accessibility\",\n        \"# AI Research Report\\n\\n## Economic Impact\\nAI is transforming industries...\\n\\n## Ethics\\nResponsible AI development...\\n\\n## Accessibility\\nDemocratizing AI tools...\"\n    )\n\n    # Create tasks\n    tasks = [\n        Task(\n            id=\"research_questions\",\n            name=\"Generate Research Questions\",\n            action=\"generate_text\",\n            parameters={\n                \"prompt\": \"Generate 3 research questions about: {topic}\",\n                \"max_tokens\": 200\n            }\n        ),\n        Task(\n            id=\"analyze_themes\",\n            name=\"Analyze Key Themes\",\n            action=\"generate_text\",\n            parameters={\n                \"prompt\": \"Analyze these questions and identify key themes: {research_questions}\",\n                \"max_tokens\": 150\n            },\n            dependencies=[\"research_questions\"]\n        ),\n        Task(\n            id=\"write_report\",\n            name=\"Write Research Report\",\n            action=\"generate_text\",\n            parameters={\n                \"prompt\": \"Write a comprehensive report on {topic} covering these themes: {analyze_themes}\",\n                \"max_tokens\": 500\n            },\n            dependencies=[\"analyze_themes\"]\n        )\n    ]\n\n    # Create pipeline\n    pipeline = Pipeline(\n        id=\"research_assistant\",\n        name=\"Research Assistant Pipeline\"\n    )\n\n    for task in tasks:\n        pipeline.add_task(task)\n\n    pipeline.set_context(\"topic\", \"artificial intelligence\")\n\n    # Create components\n    error_handler = ErrorHandler()\n    state_manager = StateManager(storage_path=\"./checkpoints\")\n    resource_allocator = ResourceAllocator()\n\n    # Create orchestrator\n    orchestrator = Orchestrator(\n        error_handler=error_handler,\n        state_manager=state_manager,\n        resource_allocator=resource_allocator\n    )\n\n    orchestrator.register_model(model)\n\n    return orchestrator, pipeline\n\nasync def main():\n    \"\"\"Main execution function.\"\"\"\n    logger.info(\"Creating research assistant pipeline...\")\n\n    orchestrator, pipeline = await create_research_pipeline()\n\n    logger.info(\"Executing pipeline...\")\n\n    try:\n        result = await orchestrator.execute_pipeline(pipeline)\n\n        logger.info(\"Pipeline completed successfully!\")\n\n        print(\"\\n=== Results ===\")\n        for task_id, output in result.items():\n            print(f\"\\n{task_id}:\")\n            print(f\"{output}\")\n\n    except Exception as e:\n        logger.error(f\"Pipeline failed: {e}\")\n        raise\n\n# Run the complete example\nif __name__ == \"__main__\":\n    asyncio.run(main())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/index.rst",
    "line_start": 27,
    "line_end": 28,
    "type": "bash",
    "description": "Get started with Orchestrator in just a few minutes:",
    "content": "pip install py-orc",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/index.rst",
    "line_start": 107,
    "line_end": 124,
    "type": "text",
    "description": "The Orchestrator Framework is built with a modular architecture that separates concerns and promotes extensibility:",
    "content": "┌─────────────────────────────────────────────────────────────┐\n│                    Orchestrator Engine                      │\n└─────────────────────────────────────────────────────────────┘\n\n┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐\n│  YAML Compiler  │  │ Model Registry  │  │ State Manager   │\n│  - Parser       │  │ - Selection     │  │ - Checkpoints   │\n│  - Validation   │  │ - Load Balance  │  │ - Recovery      │\n│  - Templates    │  │ - Health Check  │  │ - Persistence   │\n└─────────────────┘  └─────────────────┘  └─────────────────┘\n\n┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐\n│ Execution Layer │  │ Error Handler   │  │ Resource Mgmt   │\n│ - Parallel      │  │ - Circuit Break │  │ - Allocation    │\n│ - Sandboxed     │  │ - Retry Logic   │  │ - Monitoring    │\n│ - Distributed   │  │ - Recovery      │  │ - Optimization  │\n└─────────────────┘  └─────────────────┘  └─────────────────┘",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 55,
    "line_end": 64,
    "type": "bash",
    "description": "~~~~~~~~~~~~",
    "content": "# Install Orchestrator Framework\npip install py-orc\n\n# Install Jupyter (if not already installed)\npip install jupyter\n\n# Clone the repository for tutorials\ngit clone https://github.com/ContextLab/orchestrator.git\ncd orchestrator",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 70,
    "line_end": 75,
    "type": "bash",
    "description": "~~~~~~~~~~~~~~~~~",
    "content": "# Start Jupyter Notebook\njupyter notebook\n\n# Or start JupyterLab\njupyter lab",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 112,
    "line_end": 131,
    "type": "python",
    "description": "* Add state management for reliability",
    "content": "# Example from Tutorial 01\nfrom orchestrator import Orchestrator, Task, Pipeline\nfrom orchestrator.models.mock_model import MockModel\n\n# Create your first task\ntask = Task(\n    id=\"hello_world\",\n    name=\"Hello World Task\",\n    action=\"generate_text\",\n    parameters={\"prompt\": \"Hello, Orchestrator!\"}\n)\n\n# Build and execute pipeline\npipeline = Pipeline(id=\"first_pipeline\", name=\"First Pipeline\")\npipeline.add_task(task)\n\norchestrator = Orchestrator()\n# Note: This code is for Jupyter notebooks which support top-level await\nresult = await orchestrator.execute_pipeline(pipeline)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 166,
    "line_end": 187,
    "type": "yaml",
    "description": "* Create reusable pipeline templates",
    "content": "# Example from Tutorial 02\nid: research_pipeline\nname: Research Assistant Pipeline\n\ncontext:\n  topic: artificial intelligence\n\ntasks:\n  - id: research\n    name: Generate Research Questions\n    action: generate_text\n    parameters:\n      prompt: \"Research questions about: {topic}\"\n\n  - id: analyze\n    name: Analyze Themes\n    action: generate_text\n    parameters:\n      prompt: \"Analyze themes in: {research}\"\n    dependencies:\n      - research",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 222,
    "line_end": 236,
    "type": "python",
    "description": "* Optimize for cost and latency",
    "content": "# Example from Tutorial 03\nfrom orchestrator.models.openai_model import OpenAIModel\nfrom orchestrator.models.anthropic_model import AnthropicModel\n\n# Register multiple models\ngpt4 = OpenAIModel(name=\"gpt-4\", api_key=\"your-key\")\nclaude = AnthropicModel(name=\"claude-3\", api_key=\"your-key\")\n\norchestrator.register_model(gpt4)\norchestrator.register_model(claude)\n\n# Orchestrator automatically selects best model\n# Note: This code is for Jupyter notebooks which support top-level await\nresult = await orchestrator.execute_pipeline(pipeline)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 277,
    "line_end": 293,
    "type": "text",
    "description": "The tutorials come with supporting files:",
    "content": "notebooks/\n├── 01_getting_started.ipynb\n├── 02_yaml_configuration.ipynb\n├── 03_advanced_model_integration.ipynb\n├── README.md                           # Tutorial guide\n├── data/                               # Sample data files\n│   ├── sample_pipeline.yaml\n│   ├── complex_workflow.yaml\n│   └── test_data.json\n├── images/                             # Tutorial images\n│   ├── architecture_diagram.png\n│   └── workflow_visualization.png\n└── solutions/                          # Exercise solutions\n    ├── 01_solutions.ipynb\n    ├── 02_solutions.ipynb\n    └── 03_solutions.ipynb",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 312,
    "line_end": 317,
    "type": "bash",
    "description": "**Jupyter Not Starting**",
    "content": "# Try updating Jupyter\npip install --upgrade jupyter\n\n# Or install JupyterLab\npip install jupyterlab",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 321,
    "line_end": 326,
    "type": "bash",
    "description": "**Import Errors**",
    "content": "# Make sure Orchestrator is installed\npip install py-orc\n\n# Or install in development mode\npip install -e .",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 330,
    "line_end": 332,
    "type": "python",
    "description": "**Mock Model Issues**",
    "content": "# Mock models need explicit responses\nmodel.set_response(\"your prompt\", \"expected response\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/tutorials/notebooks.rst",
    "line_start": 336,
    "line_end": 338,
    "type": "python",
    "description": "**Async/Await Problems**",
    "content": "# Use await in notebook cells (Jupyter notebooks only)\nresult = await orchestrator.execute_pipeline(pipeline)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 33,
    "line_end": 35,
    "type": "bash",
    "description": "When you install Orchestrator via pip, default configuration files are available but not automatically installed to avoid overwriting existing configurations. To install the default configurations:",
    "content": "# Install default configs to ~/.orchestrator/\norchestrator-install-configs",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 53,
    "line_end": 81,
    "type": "yaml",
    "description": "The models configuration file defines available AI models:",
    "content": "models:\n  # Local models (via Ollama)\n  - source: ollama\n    name: llama3.1:8b\n    expertise: [general, reasoning, multilingual]\n    size: 8b\n\n  # Cloud models\n  - source: openai\n    name: gpt-4o\n    expertise: [general, reasoning, code, analysis, vision]\n    size: 1760b  # Estimated\n\n  # HuggingFace models\n  - source: huggingface\n    name: microsoft/Phi-3.5-mini-instruct\n    expertise: [reasoning, code, compact]\n    size: 3.8b\n\ndefaults:\n  expertise_preferences:\n    code: qwen2.5-coder:7b\n    reasoning: deepseek-r1:8b\n    fast: llama3.2:1b\n  fallback_chain:\n    - llama3.1:8b\n    - mistral:7b\n    - llama3.2:1b",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 86,
    "line_end": 91,
    "type": "yaml",
    "description": "You can add new models by editing this file:",
    "content": "# Add a new Ollama model\n- source: ollama\n  name: my-custom-model:13b\n  expertise: [domain-specific, analysis]\n  size: 13b",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 99,
    "line_end": 129,
    "type": "yaml",
    "description": "The main configuration file controls framework behavior:",
    "content": "# Execution settings\nexecution:\n  parallel_tasks: 10\n  timeout_seconds: 300\n  retry_attempts: 3\n  retry_delay: 1.0\n\n# Resource limits\nresources:\n  max_memory_mb: 8192\n  max_cpu_percent: 80\n  gpu_enabled: true\n\n# Caching\ncache:\n  enabled: true\n  ttl_seconds: 3600\n  max_size_mb: 1024\n\n# Monitoring\nmonitoring:\n  log_level: INFO\n  metrics_enabled: true\n  trace_enabled: false\n\n# Error handling\nerror_handling:\n  circuit_breaker_threshold: 5\n  circuit_breaker_timeout: 60\n  fallback_enabled: true",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 137,
    "line_end": 144,
    "type": "bash",
    "description": "You can override configuration settings using environment variables:",
    "content": "# Set custom config location\nexport ORCHESTRATOR_HOME=/path/to/configs\n\n# Override specific settings\nexport ORCHESTRATOR_LOG_LEVEL=DEBUG\nexport ORCHESTRATOR_PARALLEL_TASKS=20\nexport ORCHESTRATOR_CACHE_ENABLED=false",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 161,
    "line_end": 167,
    "type": "python",
    "description": "Orchestrator validates configuration files on startup:",
    "content": "import orchestrator as orc\n\n# Validate configuration files\nconfig_valid, errors = orc.validate_config()\nif not config_valid:\n    print(\"Configuration errors:\", errors)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 176,
    "line_end": 187,
    "type": "yaml",
    "description": "~~~~~~~~~~~~~~~~~~~~~~",
    "content": "# orchestrator.yaml for development\nexecution:\n  parallel_tasks: 2\n  timeout_seconds: 60\n\nmonitoring:\n  log_level: DEBUG\n  trace_enabled: true\n\ncache:\n  enabled: false  # Disable cache for testing",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 193,
    "line_end": 206,
    "type": "yaml",
    "description": "~~~~~~~~~~~~~~~~~~~~~~",
    "content": "# orchestrator.yaml for production\nexecution:\n  parallel_tasks: 50\n  timeout_seconds: 600\n  retry_attempts: 5\n\nmonitoring:\n  log_level: WARNING\n  metrics_enabled: true\n\nerror_handling:\n  circuit_breaker_threshold: 10\n  fallback_enabled: true",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 212,
    "line_end": 224,
    "type": "yaml",
    "description": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "content": "# models.yaml for limited resources\nmodels:\n  # Only small, efficient models\n  - source: ollama\n    name: llama3.2:1b\n    expertise: [general, fast]\n    size: 1b\n\n  - source: ollama\n    name: phi-3-mini:3.8b\n    expertise: [reasoning, compact]\n    size: 3.8b",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/configuration.rst",
    "line_start": 230,
    "line_end": 242,
    "type": "yaml",
    "description": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "content": "# orchestrator.yaml for high performance\nexecution:\n  parallel_tasks: 100\n  use_gpu: true\n\nresources:\n  max_memory_mb: 65536\n  gpu_memory_fraction: 0.9\n\ncache:\n  backend: redis\n  redis_url: redis://localhost:6379",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 16,
    "line_end": 61,
    "type": "yaml",
    "description": "~~~~~~~~~~~~~~~~~~~~~~~",
    "content": "models:\n  # Ollama models (automatically installed if not present)\n  - source: ollama\n    name: gemma2:27b\n    expertise:\n      - general\n      - reasoning\n      - analysis\n    size: 27b\n\n  - source: ollama\n    name: codellama:7b\n    expertise:\n      - code\n      - programming\n    size: 7b\n\n  # HuggingFace models (automatically downloaded)\n  - source: huggingface\n    name: microsoft/phi-2\n    expertise:\n      - reasoning\n      - code\n    size: 2.7b\n\n  # Cloud models (require API keys)\n  - source: openai\n    name: gpt-4o\n    expertise:\n      - general\n      - reasoning\n      - code\n      - analysis\n      - vision\n    size: 1760b\n\ndefaults:\n  expertise_preferences:\n    code: codellama:7b\n    reasoning: gemma2:27b\n    fast: llama3.2:1b\n  fallback_chain:\n    - gemma2:27b\n    - llama3.2:1b\n    - TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 104,
    "line_end": 112,
    "type": "python",
    "description": "The framework uses lazy loading for both Ollama and HuggingFace models to avoid downloading large models until they're actually needed:",
    "content": "import orchestrator as orc\n\n# This registers models but doesn't download them yet\nregistry = orc.init_models()\n\n# Models are downloaded only when first used by a pipeline\npipeline = orc.compile(\"my_pipeline.yaml\")\nresult = pipeline.run()  # Model downloads happen here if needed",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 130,
    "line_end": 133,
    "type": "yaml",
    "description": "HuggingFace models are also downloaded on first use:",
    "content": "- source: huggingface\n  name: microsoft/Phi-3.5-mini-instruct\n  expertise: [reasoning, code]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 156,
    "line_end": 162,
    "type": "yaml",
    "description": "Specify a model by name:",
    "content": "steps:\n  - id: summarize\n    action: generate_text\n    parameters:\n      prompt: \"Summarize this text...\"\n    requires_model: gemma2:27b  # Use specific model",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 170,
    "line_end": 178,
    "type": "yaml",
    "description": "Specify requirements and let the framework choose:",
    "content": "steps:\n  - id: generate_code\n    action: generate_text\n    parameters:\n      prompt: \"Write a Python function...\"\n    requires_model:\n      expertise: code\n      min_size: 7b  # At least 7B parameters",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 186,
    "line_end": 196,
    "type": "yaml",
    "description": "Specify multiple expertise areas (any match will qualify):",
    "content": "steps:\n  - id: analyze\n    action: analyze\n    parameters:\n      content: \"{input_data}\"\n    requires_model:\n      expertise:\n        - reasoning\n        - analysis\n      min_size: 20b",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 204,
    "line_end": 240,
    "type": "yaml",
    "description": "Here's a complete pipeline demonstrating model requirements:",
    "content": "id: multi_model_pipeline\nname: Multi-Model Processing Pipeline\n\ninputs:\n  - name: topic\n    type: string\n\nsteps:\n  # Fast task with small model\n  - id: quick_check\n    action: generate_text\n    parameters:\n      prompt: \"Is this topic related to programming: {topic}?\"\n    requires_model:\n      expertise: fast\n      min_size: 0  # Any size\n\n  # Code generation with specialized model\n  - id: code_example\n    action: generate_text\n    parameters:\n      prompt: \"Generate example code for: {topic}\"\n    requires_model:\n      expertise: code\n      min_size: 7b\n    dependencies: [quick_check]\n\n  # Complex reasoning with large model\n  - id: deep_analysis\n    action: analyze\n    parameters:\n      content: \"{topic} with code: {code_example.result}\"\n    requires_model:\n      expertise: [reasoning, analysis]\n      min_size: 27b\n    dependencies: [code_example]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 280,
    "line_end": 291,
    "type": "python",
    "description": "Check which models are being used:",
    "content": "import orchestrator as orc\n\n# Initialize and list available models\nregistry = orc.init_models()\nprint(\"Available models:\")\nfor model_key in registry.list_models():\n    print(f\"  - {model_key}\")\n\n# Run pipeline and check model selection\npipeline = orc.compile(\"pipeline.yaml\")\nresult = pipeline.run(topic=\"AI agents\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 296,
    "line_end": 299,
    "type": "text",
    "description": "The framework logs model selection decisions:",
    "content": ">> Using model for task 'quick_check': ollama:llama3.2:1b (fast, 1B params)\n>> Using model for task 'code_example': ollama:codellama:7b (code, 7B params)\n>> Using model for task 'deep_analysis': ollama:gemma2:27b (reasoning, 27B params)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 337,
    "line_end": 338,
    "type": "text",
    "description": "**Model Installation Fails**:",
    "content": ">> ❌ Failed to install gemma2:27b: connection timeout",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 348,
    "line_end": 349,
    "type": "text",
    "description": "**No Models Match Requirements**:",
    "content": "NoEligibleModelsError: No models meet the specified requirements",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/model_configuration.rst",
    "line_start": 359,
    "line_end": 360,
    "type": "text",
    "description": "**API Key Missing**:",
    "content": ">> ⚠️  OpenAI models configured but OPENAI_API_KEY not set",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 34,
    "line_end": 50,
    "type": "python",
    "description": "~~~~~~~~~~~~~~~~~~~",
    "content": "import orchestrator as orc\n\n# Initialize and discover available models\nregistry = orc.init_models()\n\n# List all detected models\navailable_models = registry.list_models()\nprint(\"Available models:\", available_models)\n\n# Check specific model availability\nif any(\"gemma2:27b\" in model for model in available_models):\n    print(\"Large Ollama model available\")\nelif any(\"llama3.2:1b\" in model for model in available_models):\n    print(\"Lightweight Ollama model available\")\nelse:\n    print(\"Using fallback models\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 60,
    "line_end": 67,
    "type": "bash",
    "description": "Install Ollama and pull recommended models:",
    "content": "# Install Ollama\nbrew install ollama  # macOS\n# or visit https://ollama.ai for other platforms\n\n# Pull recommended models\nollama pull gemma2:27b    # Large model for complex tasks\nollama pull llama3.2:1b   # Lightweight fallback",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 74,
    "line_end": 75,
    "type": "bash",
    "description": "Install the transformers library:",
    "content": "pip install transformers torch",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 82,
    "line_end": 85,
    "type": "bash",
    "description": "Set up API keys as environment variables:",
    "content": "export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GOOGLE_API_KEY=\"...\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 96,
    "line_end": 103,
    "type": "python",
    "description": "~~~~~~~~~~~~~",
    "content": "from orchestrator.models.openai_model import OpenAIModel\n\nmodel = OpenAIModel(\n    name=\"gpt-4o\",\n    api_key=\"your-api-key\",\n    model=\"gpt-4o\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 109,
    "line_end": 116,
    "type": "python",
    "description": "~~~~~~~~~~~~~~~~",
    "content": "from orchestrator.models.anthropic_model import AnthropicModel\n\nmodel = AnthropicModel(\n    name=\"claude-3.5-sonnet\",\n    api_key=\"your-api-key\",\n    model=\"claude-3.5-sonnet\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 122,
    "line_end": 128,
    "type": "python",
    "description": "~~~~~~~~~~~~",
    "content": "from orchestrator.models.huggingface_model import HuggingFaceModel\n\nmodel = HuggingFaceModel(\n    name=\"llama-3.2-3b\",\n    model_path=\"meta-llama/Llama-3.2-3B-Instruct\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/models_and_adapters.rst",
    "line_start": 136,
    "line_end": 144,
    "type": "python",
    "description": "The model registry manages model selection and load balancing:",
    "content": "from orchestrator.models.model_registry import ModelRegistry\n\nregistry = ModelRegistry()\nregistry.register_model(gpt4_model)\nregistry.register_model(claude_model)\n\n# Automatic selection based on task requirements\nselected_model = registry.select_model(task)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/yaml_configuration.rst",
    "line_start": 26,
    "line_end": 44,
    "type": "yaml",
    "description": "A basic pipeline YAML file contains:",
    "content": "id: my_pipeline\nname: My Pipeline\ndescription: A sample pipeline\n\ntasks:\n  - id: task1\n    name: First Task\n    action: generate_text\n    parameters:\n      prompt: \"Hello, world!\"\n\n  - id: task2\n    name: Second Task\n    action: generate_text\n    parameters:\n      prompt: \"Process this: {task1}\"\n    dependencies:\n      - task1",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/yaml_configuration.rst",
    "line_start": 52,
    "line_end": 65,
    "type": "yaml",
    "description": "Use template variables for dynamic content:",
    "content": "id: research_pipeline\nname: Research Pipeline\n\ncontext:\n  topic: artificial intelligence\n  depth: detailed\n\ntasks:\n  - id: research\n    name: Research Task\n    action: generate_text\n    parameters:\n      prompt: \"Research {topic} with {depth} analysis\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs/user_guide/yaml_configuration.rst",
    "line_start": 73,
    "line_end": 80,
    "type": "yaml",
    "description": "The AUTO tag automatically resolves ambiguous parameters:",
    "content": "tasks:\n  - id: analysis\n    name: Analysis Task\n    action: <AUTO>\n    parameters:\n      data: {previous_task}\n      model: <AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 118,
    "line_end": 128,
    "type": "python",
    "description": "-----------",
    "content": "import orchestrator as orc\n\n# Initialize models\nregistry = orc.init_models()\n\n# Compile pipeline\npipeline = orc.compile(\"my_pipeline.yaml\")\n\n# Execute\nresult = pipeline.run(input_param=\"value\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 134,
    "line_end": 151,
    "type": "python",
    "description": "--------------",
    "content": "from orchestrator import Orchestrator\nfrom orchestrator.core.control_system import MockControlSystem\nfrom orchestrator.models.model_registry import ModelRegistry\n\n# Create custom orchestrator\ncontrol_system = MockControlSystem()\norchestrator = Orchestrator(control_system=control_system)\n\n# Use custom model registry\nregistry = ModelRegistry()\n# ... configure models\n\n# Compile with custom settings\npipeline = orchestrator.compile(\n    yaml_content,\n    config={\"timeout\": 3600}\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 159,
    "line_end": 168,
    "type": "python",
    "description": "The Orchestrator framework uses comprehensive type annotations for better IDE support and type checking:",
    "content": "from typing import Dict, Any, List, Optional\nfrom orchestrator import Pipeline, Task\n\ndef process_pipeline(\n    pipeline: Pipeline,\n    inputs: Dict[str, Any],\n    timeout: Optional[int] = None\n) -> Dict[str, Any]:\n    return pipeline.run(**inputs)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 198,
    "line_end": 215,
    "type": "yaml",
    "description": "Default configuration can be overridden using a config file at ``~/.orchestrator/config.yaml``:",
    "content": "models:\n  default: \"ollama:gemma2:27b\"\n  fallback: \"ollama:llama3.2:1b\"\n  timeout: 300\n\ntools:\n  mcp_port: 8000\n  auto_start: true\n\nexecution:\n  parallel: true\n  checkpoint: true\n  timeout: 3600\n\nlogging:\n  level: \"INFO\"\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 226,
    "line_end": 235,
    "type": "python",
    "description": "Models are loaded lazily and cached. For better performance:",
    "content": "# Initialize models once at startup\norc.init_models()\n\n# Reuse compiled pipelines\npipeline = orc.compile(\"pipeline.yaml\")\n\n# Multiple executions reuse the same pipeline\nfor inputs in input_batches:\n    result = pipeline.run(**inputs)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 243,
    "line_end": 253,
    "type": "python",
    "description": "Large pipelines and datasets can consume significant memory:",
    "content": "# Enable checkpointing for long pipelines\npipeline = orc.compile(\"pipeline.yaml\", config={\n    \"checkpoint\": True,\n    \"memory_limit\": \"8GB\"\n})\n\n# Process data in batches\nfor batch in data_batches:\n    result = pipeline.run(data=batch)\n    # Results are automatically checkpointed",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 261,
    "line_end": 273,
    "type": "python",
    "description": "The framework provides structured error handling:",
    "content": "from orchestrator import CompilationError, ExecutionError\n\ntry:\n    pipeline = orc.compile(\"pipeline.yaml\")\n    result = pipeline.run(input=\"value\")\nexcept CompilationError as e:\n    print(f\"Pipeline compilation failed: {e}\")\n    print(f\"Error details: {e.details}\")\nexcept ExecutionError as e:\n    print(f\"Pipeline execution failed: {e}\")\n    print(f\"Failed step: {e.step_id}\")\n    print(f\"Error context: {e.context}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 281,
    "line_end": 291,
    "type": "python",
    "description": "Enable detailed logging for debugging:",
    "content": "import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Compile with debug information\npipeline = orc.compile(\"pipeline.yaml\", debug=True)\n\n# Execute with verbose output\nresult = pipeline.run(input=\"value\", _verbose=True)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 302,
    "line_end": 308,
    "type": "python",
    "description": "----------------------",
    "content": "from orchestrator.core.control_system import ControlSystem\n\nclass MyControlSystem(ControlSystem):\n    async def execute_task(self, task: Task, context: dict) -> dict:\n        # Custom execution logic\n        pass",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 314,
    "line_end": 323,
    "type": "python",
    "description": "------------",
    "content": "from orchestrator.tools.base import Tool\n\nclass MyTool(Tool):\n    def __init__(self):\n        super().__init__(\"my-tool\", \"Description\")\n\n    async def execute(self, **kwargs) -> dict:\n        # Tool implementation\n        pass",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 329,
    "line_end": 335,
    "type": "python",
    "description": "-------------",
    "content": "from orchestrator.core.model import Model\n\nclass MyModel(Model):\n    async def generate(self, prompt: str, **kwargs) -> str:\n        # Model implementation\n        pass",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 343,
    "line_end": 356,
    "type": "python",
    "description": "The framework is designed to be thread-safe:",
    "content": "import concurrent.futures\n\n# Safe to use across threads\npipeline = orc.compile(\"pipeline.yaml\")\n\ndef process_input(input_data):\n    return pipeline.run(**input_data)\n\n# Parallel execution\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(process_input, data)\n              for data in input_datasets]\n    results = [f.result() for f in futures]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/api/index.rst",
    "line_start": 364,
    "line_end": 381,
    "type": "python",
    "description": "Testing utilities and patterns:",
    "content": "from orchestrator.testing import MockModel, TestRunner\n\ndef test_my_pipeline():\n    # Use mock model for testing\n    with MockModel() as mock:\n        mock.set_response(\"test response\")\n\n        pipeline = orc.compile(\"test_pipeline.yaml\")\n        result = pipeline.run(input=\"test\")\n\n        assert result == \"expected\"\n\n# Test runner for pipeline validation\nrunner = TestRunner(\"pipelines/\")\nrunner.validate_all()  # Validates all YAML files\nrunner.test_compilation()  # Tests compilation\nrunner.run_smoke_tests()  # Basic execution tests",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 19,
    "line_end": 31,
    "type": "yaml",
    "description": "One of Orchestrator's core innovations is **input-agnostic pipelines**. This means a single pipeline definition can work with different inputs to produce different outputs:",
    "content": "# One pipeline definition\nname: research-pipeline\n\ninputs:\n  topic: { type: string, required: true }\n  depth: { type: string, default: \"medium\" }\n\nsteps:\n  - id: research\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 55,
    "line_end": 62,
    "type": "yaml",
    "description": "Every task has these key components:",
    "content": "- id: unique_identifier        # Required: Unique name\n  action: what_to_do           # Required: Action to perform\n  description: \"What it does\"  # Optional: Human description\n  parameters:                  # Optional: Input parameters\n    key: value\n  depends_on: [other_task]     # Optional: Dependencies\n  condition: \"when_to_run\"     # Optional: Conditional execution",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 70,
    "line_end": 87,
    "type": "yaml",
    "description": "Tasks can depend on other tasks, creating execution ordering:",
    "content": "steps:\n  - id: fetch_data\n    action: download_file\n    parameters:\n      url: \"{{ inputs.data_url }}\"\n\n  - id: process_data\n    depends_on: [fetch_data]   # Runs after fetch_data\n    action: transform_data\n    parameters:\n      data: \"$results.fetch_data\"\n\n  - id: save_results\n    depends_on: [process_data] # Runs after process_data\n    action: write_file\n    parameters:\n      content: \"$results.process_data\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 98,
    "line_end": 109,
    "type": "yaml",
    "description": "---------------",
    "content": "# Access input values\nquery: \"{{ inputs.search_term }}\"\n\n# Reference results from other tasks\ndata: \"$results.previous_task\"\n\n# Use filters and functions\nfilename: \"{{ inputs.name | slugify }}.pdf\"\n\n# Conditional expressions\nmode: \"{{ 'advanced' if inputs.premium else 'basic' }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 120,
    "line_end": 129,
    "type": "yaml",
    "description": "- **Runtime**: Dynamic values resolved during execution",
    "content": "steps:\n  - id: example\n    parameters:\n      # Compile-time: resolved once during compilation\n      timestamp: \"{{ compile_time.now }}\"\n\n      # Runtime: resolved during each execution\n      user_input: \"{{ inputs.query }}\"\n      previous_result: \"$results.other_task\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 137,
    "line_end": 153,
    "type": "yaml",
    "description": "**AUTO tags** are Orchestrator's solution to ambiguous or uncertain values. When you're not sure what value to use, let an AI model decide:",
    "content": "parameters:\n  # Simple AUTO tag\n  style: <AUTO>Choose appropriate writing style</AUTO>\n\n  # Context-aware AUTO tag\n  method: <AUTO>Based on data type {{ results.data.type }}, choose best analysis method</AUTO>\n\n  # Complex AUTO tag with instructions\n  sections: |\n    <AUTO>\n    For a report about {{ inputs.topic }}, determine which sections to include:\n    - Executive Summary: yes/no\n    - Technical Details: yes/no\n    - Future Outlook: yes/no\n    Return as JSON object\n    </AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 197,
    "line_end": 215,
    "type": "yaml",
    "description": "Actions are how you invoke tools in pipelines:",
    "content": "# Web search\n- action: search_web\n  parameters:\n    query: \"machine learning\"\n\n# File operations\n- action: write_file\n  parameters:\n    path: \"output.txt\"\n    content: \"Hello world\"\n\n# Shell commands (prefix with !)\n- action: \"!ls -la\"\n\n# AI generation\n- action: generate_content\n  parameters:\n    prompt: \"Write a summary about {{ topic }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 223,
    "line_end": 227,
    "type": "yaml",
    "description": "Orchestrator automatically detects required tools from your pipeline:",
    "content": "steps:\n  - action: search_web        # → Requires web tool\n  - action: \"!python script.py\"  # → Requires terminal tool\n  - action: write_file        # → Requires filesystem tool",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 265,
    "line_end": 271,
    "type": "python",
    "description": "- **Cost considerations** (API costs, efficiency)",
    "content": "# Models are selected automatically\nregistry = orc.init_models()\n\n# Available models are ranked by capability\nprint(registry.list_models())\n# ['ollama:gemma2:27b', 'ollama:llama3.2:1b', 'huggingface:gpt2']",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 284,
    "line_end": 291,
    "type": "yaml",
    "description": "Orchestrator can save pipeline state at task boundaries:",
    "content": "config:\n  checkpoint: true  # Enable automatic checkpointing\n\nsteps:\n  - id: expensive_task\n    action: long_running_process\n    checkpoint: true  # Force checkpoint after this step",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 299,
    "line_end": 304,
    "type": "python",
    "description": "If a pipeline fails, it can resume from the last checkpoint:",
    "content": "# Pipeline fails at step 5\npipeline.run(inputs)  # Fails\n\n# Resume from last checkpoint\npipeline.resume()  # Continues from step 4",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 335,
    "line_end": 341,
    "type": "python",
    "description": "You can create custom control systems for specific needs:",
    "content": "from orchestrator.core.control_system import ControlSystem\n\nclass MyControlSystem(ControlSystem):\n    async def execute_task(self, task, context):\n        # Custom execution logic\n        pass",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 352,
    "line_end": 366,
    "type": "yaml",
    "description": "---------------",
    "content": "imports:\n  - common/validation.yaml as validator\n  - workflows/analysis.yaml as analyzer\n\nsteps:\n  - id: validate\n    pipeline: validator\n    inputs:\n      data: \"{{ inputs.raw_data }}\"\n\n  - id: analyze\n    pipeline: analyzer\n    inputs:\n      validated_data: \"$results.validate\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 386,
    "line_end": 401,
    "type": "yaml",
    "description": "---------------",
    "content": "steps:\n  - id: risky_task\n    action: external_api_call\n    error_handling:\n      # Retry with backoff\n      retry:\n        max_attempts: 3\n        backoff: exponential\n\n      # Fallback action\n      fallback:\n        action: use_cached_data\n\n      # Continue pipeline on error\n      continue_on_error: true",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 422,
    "line_end": 434,
    "type": "yaml",
    "description": "Tasks without dependencies can run in parallel:",
    "content": "steps:\n  # These run in parallel\n  - id: source1\n    action: fetch_data_a\n\n  - id: source2\n    action: fetch_data_b\n\n  # This waits for both\n  - id: combine\n    depends_on: [source1, source2]\n    action: merge_data",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 442,
    "line_end": 449,
    "type": "yaml",
    "description": "Expensive operations can be cached:",
    "content": "steps:\n  - id: expensive_computation\n    action: complex_analysis\n    cache:\n      enabled: true\n      key: \"{{ inputs.data_hash }}\"\n      ttl: 3600  # 1 hour",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 457,
    "line_end": 462,
    "type": "yaml",
    "description": "Control resource usage:",
    "content": "config:\n  resources:\n    max_memory: \"8GB\"\n    max_threads: 4\n    gpu_enabled: false",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 483,
    "line_end": 494,
    "type": "yaml",
    "description": "All inputs are validated:",
    "content": "inputs:\n  email:\n    type: string\n    validation:\n      pattern: \"^[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+$\"\n\n  amount:\n    type: number\n    validation:\n      min: 0\n      max: 10000",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 502,
    "line_end": 505,
    "type": "yaml",
    "description": "Sensitive data is handled securely:",
    "content": "parameters:\n  api_key: \"{{ env.SECRET_API_KEY }}\"  # From environment\n  password: \"{{ vault.db_password }}\"   # From secret vault",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/concepts.rst",
    "line_start": 523,
    "line_end": 533,
    "type": "",
    "description": "--------------------",
    "content": "pipelines/\n├── common/           # Shared components\n│   ├── validation.yaml\n│   └── formatting.yaml\n├── workflows/        # Complete workflows\n│   ├── research.yaml\n│   └── analysis.yaml\n└── specialized/      # Domain-specific\n    ├── finance.yaml\n    └── healthcare.yaml",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 29,
    "line_end": 43,
    "type": "yaml",
    "description": "A pipeline is a collection of tasks that work together to achieve a goal. Pipelines are defined in YAML and can include:",
    "content": "name: research-report\ndescription: Generate comprehensive research reports\n\ninputs:\n  topic:\n    type: string\n    description: Research topic\n    required: true\n\nsteps:\n  - id: search\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }} latest research\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 61,
    "line_end": 64,
    "type": "yaml",
    "description": "When you're unsure about a value, use ``<AUTO>`` tags to let AI models decide:",
    "content": "parameters:\n  method: <AUTO>Choose best analysis method for this data</AUTO>\n  depth: <AUTO>Determine appropriate depth level</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 83,
    "line_end": 108,
    "type": "yaml",
    "description": "1. **Create a pipeline definition** (``research.yaml``):",
    "content": "name: quick-research\ndescription: Quick research on any topic\n\ninputs:\n  topic:\n    type: string\n    required: true\n\noutputs:\n  report:\n    type: string\n    value: \"{{ inputs.topic }}_report.md\"\n\nsteps:\n  - id: search\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }}\"\n      max_results: 5\n\n  - id: summarize\n    action: generate_summary\n    parameters:\n      content: \"$results.search\"\n      style: <AUTO>Choose appropriate summary style</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 113,
    "line_end": 126,
    "type": "python",
    "description": "2. **Run the pipeline**:",
    "content": "import orchestrator as orc\n\n# Initialize models\norc.init_models()\n\n# Compile the pipeline\npipeline = orc.compile(\"research.yaml\")\n\n# Execute with different topics\nresult1 = pipeline.run(topic=\"artificial intelligence\")\nresult2 = pipeline.run(topic=\"climate change\")\n\nprint(f\"Reports generated: {result1}, {result2}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 141,
    "line_end": 148,
    "type": "python",
    "description": "The same pipeline works with different inputs:",
    "content": "# One pipeline, many uses\npipeline = orc.compile(\"report-template.yaml\")\n\n# Generate different reports\nai_report = pipeline.run(topic=\"AI\", style=\"technical\")\nbio_report = pipeline.run(topic=\"Biology\", style=\"educational\")\neco_report = pipeline.run(topic=\"Economics\", style=\"executive\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 153,
    "line_end": 160,
    "type": "yaml",
    "description": "eco_report = pipeline.run(topic=\"Economics\", style=\"executive\")",
    "content": "inputs:\n  topic:\n    type: string\n    required: true\n  style:\n    type: string\n    default: \"technical\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 168,
    "line_end": 177,
    "type": "yaml",
    "description": "Tools are automatically detected and made available:",
    "content": "steps:\n  - id: fetch_data\n    action: search_web        # Auto-detects web tool\n\n  - id: save_results\n    action: write_file        # Auto-detects filesystem tool\n\n  - id: run_analysis\n    action: \"!python analyze.py\"  # Auto-detects terminal tool",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/getting_started.rst",
    "line_start": 185,
    "line_end": 193,
    "type": "python",
    "description": "The framework intelligently selects the best model for each task:",
    "content": "# Models are selected based on:\n# - Task requirements (reasoning, coding, etc.)\n# - Available resources\n# - Performance history\n\nregistry = orc.init_models()\nprint(registry.list_models())\n# Output: ['ollama:gemma2:27b', 'ollama:llama3.2:1b', 'huggingface:gpt2']",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/index.rst",
    "line_start": 43,
    "line_end": 56,
    "type": "python",
    "description": "-------------",
    "content": "import orchestrator as orc\n\n# Initialize models\norc.init_models()\n\n# Compile a pipeline\npipeline = orc.compile(\"pipelines/research-report.yaml\")\n\n# Execute with different inputs\nresult = pipeline.run(\n    topic=\"quantum_computing\",\n    instructions=\"Focus on error correction\"\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 34,
    "line_end": 41,
    "type": "bash",
    "description": "-----------------------",
    "content": "# Install from PyPI (when available)\npip install py-orc\n\n# Or install from source\ngit clone https://github.com/ContextLab/orchestrator.git\ncd orchestrator\npip install -e .",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 47,
    "line_end": 53,
    "type": "bash",
    "description": "-----------",
    "content": "# Create conda environment\nconda create -n py-orc python=3.11\nconda activate py-orc\n\n# Install orchestrator\npip install py-orc",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 59,
    "line_end": 64,
    "type": "bash",
    "description": "------------",
    "content": "# Pull the official image\ndocker pull contextlab/py-orc:latest\n\n# Run with volume mount\ndocker run -v $(pwd):/workspace contextlab/py-orc",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 72,
    "line_end": 85,
    "type": "bash",
    "description": "For contributors and developers:",
    "content": "# Clone the repository\ngit clone https://github.com/ContextLab/orchestrator.git\ncd orchestrator\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install in development mode with extras\npip install -e \".[dev,test,docs]\"\n\n# Install pre-commit hooks\npre-commit install",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 96,
    "line_end": 101,
    "type": "bash",
    "description": "1. **Install Ollama**:",
    "content": "# macOS\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.ai/install.sh | sh",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 106,
    "line_end": 114,
    "type": "bash",
    "description": "2. **Pull recommended models**:",
    "content": "# Large model for complex tasks\nollama pull gemma2:27b\n\n# Small model for simple tasks\nollama pull llama3.2:1b\n\n# Code-focused model\nollama pull codellama:7b",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 119,
    "line_end": 124,
    "type": "python",
    "description": "3. **Verify installation**:",
    "content": "import orchestrator as orc\n\n# Initialize and check models\nregistry = orc.init_models()\nprint(registry.list_models())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 132,
    "line_end": 137,
    "type": "bash",
    "description": "For HuggingFace models, set up your token:",
    "content": "# Set environment variable\nexport HUGGINGFACE_TOKEN=\"your-token-here\"\n\n# Or create .env file\necho \"HUGGINGFACE_TOKEN=your-token-here\" > .env",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 145,
    "line_end": 150,
    "type": "bash",
    "description": "For cloud models, configure API keys:",
    "content": "# OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 161,
    "line_end": 169,
    "type": "bash",
    "description": "For headless browser functionality:",
    "content": "# Install Playwright\npip install playwright\n# Install Playwright browser\nplaywright install chromium\n\n# Or use Selenium\npip install selenium\n# Download appropriate driver",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 182,
    "line_end": 187,
    "type": "bash",
    "description": "Install optional data processing libraries:",
    "content": "# For advanced data processing\npip install pandas numpy scipy\n\n# For data validation\npip install pydantic jsonschema",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 195,
    "line_end": 215,
    "type": "yaml",
    "description": "Create a configuration file at ``~/.orchestrator/config.yaml``:",
    "content": "# Model preferences\nmodels:\n  default: \"ollama:gemma2:27b\"\n  fallback: \"ollama:llama3.2:1b\"\n\n# Resource limits\nresources:\n  max_memory: \"16GB\"\n  max_threads: 8\n  gpu_enabled: true\n\n# Tool settings\ntools:\n  mcp_port: 8000\n  sandbox_enabled: true\n\n# State management\nstate:\n  backend: \"postgresql\"\n  connection: \"postgresql://user:pass@localhost/orchestrator\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 223,
    "line_end": 234,
    "type": "bash",
    "description": "Set these environment variables for additional configuration:",
    "content": "# Core settings\nexport ORCHESTRATOR_HOME=\"$HOME/.orchestrator\"\nexport ORCHESTRATOR_LOG_LEVEL=\"INFO\"\n\n# Model settings\nexport ORCHESTRATOR_MODEL_TIMEOUT=\"300\"\nexport ORCHESTRATOR_MODEL_RETRIES=\"3\"\n\n# Tool settings\nexport ORCHESTRATOR_TOOL_TIMEOUT=\"60\"\nexport ORCHESTRATOR_MCP_AUTO_START=\"true\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 242,
    "line_end": 267,
    "type": "python",
    "description": "Run the verification script:",
    "content": "import orchestrator as orc\n\n# Check version\nprint(f\"Orchestrator version: {orc.__version__}\")\n\n# Check models\ntry:\n    registry = orc.init_models()\n    models = registry.list_models()\n    print(f\"Available models: {models}\")\nexcept Exception as e:\n    print(f\"Model initialization failed: {e}\")\n\n# Check tools\nfrom orchestrator.tools.base import default_registry\ntools = default_registry.list_tools()\nprint(f\"Available tools: {tools}\")\n\n# Run test pipeline\ntry:\n    pipeline = orc.compile(\"examples/hello-world.yaml\")\n    result = pipeline.run(message=\"Hello, Orchestrator!\")\n    print(f\"Test pipeline result: {result}\")\nexcept Exception as e:\n    print(f\"Pipeline test failed: {e}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 278,
    "line_end": 279,
    "type": "text",
    "description": "**Import Error**:",
    "content": "ModuleNotFoundError: No module named 'orchestrator'",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 286,
    "line_end": 287,
    "type": "text",
    "description": "**Model Connection Error**:",
    "content": "Failed to connect to Ollama at http://localhost:11434",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 294,
    "line_end": 295,
    "type": "text",
    "description": "**Permission Error**:",
    "content": "Permission denied: '/home/user/.orchestrator'",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/installation.rst",
    "line_start": 300,
    "line_end": 302,
    "type": "bash",
    "description": "Solution: Create directory with proper permissions:",
    "content": "mkdir -p ~/.orchestrator\nchmod 755 ~/.orchestrator",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 19,
    "line_end": 59,
    "type": "yaml",
    "description": "Create a file called ``summarize.yaml``:",
    "content": "name: topic-summarizer\ndescription: Generate a concise summary of any topic\n\ninputs:\n  topic:\n    type: string\n    description: The topic to summarize\n    required: true\n\n  length:\n    type: integer\n    description: Approximate word count for the summary\n    default: 200\n\noutputs:\n  summary:\n    type: string\n    value: \"{{ inputs.topic }}_summary.txt\"\n\nsteps:\n  - id: research\n    action: generate_content\n    parameters:\n      prompt: |\n        Research and provide key information about: {{ inputs.topic }}\n        Focus on the most important and interesting aspects.\n      max_length: 500\n\n  - id: summarize\n    action: generate_summary\n    parameters:\n      content: \"$results.research\"\n      target_length: \"{{ inputs.length }}\"\n      style: <AUTO>Choose appropriate style for the topic</AUTO>\n\n  - id: save_summary\n    action: write_file\n    parameters:\n      path: \"{{ outputs.summary }}\"\n      content: \"$results.summarize\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 67,
    "line_end": 87,
    "type": "python",
    "description": "Create a Python script to run your pipeline:",
    "content": "import orchestrator as orc\n\n# Initialize the model pool\norc.init_models()\n\n# Compile the pipeline\npipeline = orc.compile(\"summarize.yaml\")\n\n# Run with different topics\nresult1 = pipeline.run(\n    topic=\"quantum computing\",\n    length=150\n)\n\nresult2 = pipeline.run(\n    topic=\"sustainable energy\",\n    length=250\n)\n\nprint(f\"Created summaries: {result1}, {result2}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 107,
    "line_end": 179,
    "type": "yaml",
    "description": "Let's create a more sophisticated pipeline that generates research reports:",
    "content": "name: research-report-generator\ndescription: Generate comprehensive research reports with citations\n\ninputs:\n  topic:\n    type: string\n    required: true\n  focus_areas:\n    type: array\n    description: Specific areas to focus on\n    default: []\n\noutputs:\n  report_pdf:\n    type: string\n    value: \"reports/{{ inputs.topic }}_report.pdf\"\n\nsteps:\n  # Web search for recent information\n  - id: search_recent\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }} 2024 latest developments\"\n      max_results: 10\n\n  # Search academic sources\n  - id: search_academic\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }} research papers scholarly\"\n      max_results: 5\n\n  # Compile all sources\n  - id: compile_sources\n    action: compile_markdown\n    parameters:\n      sources:\n        - \"$results.search_recent\"\n        - \"$results.search_academic\"\n      include_citations: true\n\n  # Generate the report\n  - id: write_report\n    action: generate_report\n    parameters:\n      research: \"$results.compile_sources\"\n      topic: \"{{ inputs.topic }}\"\n      focus_areas: \"{{ inputs.focus_areas }}\"\n      style: \"academic\"\n      sections:\n        - \"Executive Summary\"\n        - \"Introduction\"\n        - \"Current State\"\n        - \"Recent Developments\"\n        - \"Future Outlook\"\n        - \"Conclusions\"\n\n  # Quality check\n  - id: validate\n    action: validate_report\n    parameters:\n      report: \"$results.write_report\"\n      checks:\n        - completeness\n        - citation_accuracy\n        - readability\n\n  # Generate PDF\n  - id: create_pdf\n    action: \"!pandoc -o {{ outputs.report_pdf }} --pdf-engine=xelatex\"\n    parameters:\n      input: \"$results.write_report\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 192,
    "line_end": 201,
    "type": "yaml",
    "description": "**Web Tools**:",
    "content": "# Web search\n- action: search_web\n  parameters:\n    query: \"your search query\"\n\n# Scrape webpage\n- action: scrape_page\n  parameters:\n    url: \"https://example.com\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 206,
    "line_end": 218,
    "type": "yaml",
    "description": "**System Tools**:",
    "content": "# Run shell commands (prefix with !)\n- action: \"!ls -la\"\n\n# File operations\n- action: read_file\n  parameters:\n    path: \"data.txt\"\n\n- action: write_file\n  parameters:\n    path: \"output.txt\"\n    content: \"Your content\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 223,
    "line_end": 238,
    "type": "yaml",
    "description": "**Data Tools**:",
    "content": "# Process data\n- action: transform_data\n  parameters:\n    input: \"$results.previous_step\"\n    operations:\n      - type: filter\n        condition: \"value > 100\"\n\n# Validate data\n- action: validate_data\n  parameters:\n    data: \"$results.data\"\n    schema:\n      type: object\n      required: [\"name\", \"value\"]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 246,
    "line_end": 254,
    "type": "yaml",
    "description": "AUTO tags let AI models make intelligent decisions:",
    "content": "steps:\n  - id: analyze\n    action: analyze_data\n    parameters:\n      data: \"$results.fetch\"\n      method: <AUTO>Choose best analysis method based on data type</AUTO>\n      visualization: <AUTO>Determine if visualization would be helpful</AUTO>\n      depth: <AUTO>Set analysis depth (shallow/medium/deep)</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 264,
    "line_end": 288,
    "type": "yaml",
    "description": "You can compose pipelines from smaller, reusable components:",
    "content": "name: composite-pipeline\n\nimports:\n  - common/data_fetcher.yaml as fetcher\n  - common/validator.yaml as validator\n\nsteps:\n  # Use imported pipeline\n  - id: fetch_data\n    pipeline: fetcher\n    parameters:\n      source: \"api\"\n\n  # Local step\n  - id: process\n    action: process_data\n    parameters:\n      data: \"$results.fetch_data\"\n\n  # Use another import\n  - id: validate\n    pipeline: validator\n    parameters:\n      data: \"$results.process\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 296,
    "line_end": 309,
    "type": "yaml",
    "description": "Add error handling to make pipelines robust:",
    "content": "steps:\n  - id: risky_operation\n    action: fetch_external_data\n    parameters:\n      url: \"{{ inputs.data_source }}\"\n    error_handling:\n      retry:\n        max_attempts: 3\n        backoff: exponential\n      fallback:\n        action: use_cached_data\n        parameters:\n          cache_key: \"{{ inputs.topic }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/quickstart.rst",
    "line_start": 317,
    "line_end": 332,
    "type": "python",
    "description": "Enable debug mode for detailed execution logs:",
    "content": "import logging\nimport orchestrator as orc\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Compile with debug flag\npipeline = orc.compile(\"pipeline.yaml\", debug=True)\n\n# Run with verbose output\nresult = pipeline.run(\n    topic=\"test\",\n    _verbose=True,\n    _step_callback=lambda step: print(f\"Executing: {step.id}\")\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 48,
    "line_end": 100,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Web search\n- id: search\n  action: search_web\n  parameters:\n    query: \"orchestrator framework tutorial\"    # Required: Search query\n    max_results: 10                            # Optional: Number of results (default: 10)\n    search_engine: \"google\"                    # Optional: google|bing|duckduckgo (default: google)\n    include_snippets: true                     # Optional: Include text snippets (default: true)\n    region: \"us\"                              # Optional: Region code (default: us)\n    language: \"en\"                            # Optional: Language code (default: en)\n    safe_search: \"moderate\"                   # Optional: off|moderate|strict (default: moderate)\n\n# Scrape webpage\n- id: scrape\n  action: scrape_page\n  parameters:\n    url: \"https://example.com/article\"        # Required: URL to scrape\n    selectors:                                # Optional: CSS selectors to extract\n      title: \"h1.main-title\"\n      content: \"div.article-body\"\n      author: \"span.author-name\"\n    wait_for: \"div.content-loaded\"            # Optional: Wait for element\n    timeout: 30                               # Optional: Timeout in seconds (default: 30)\n    javascript: true                          # Optional: Execute JavaScript (default: true)\n    clean_html: true                          # Optional: Clean extracted HTML (default: true)\n\n# Take screenshot\n- id: screenshot\n  action: screenshot_page\n  parameters:\n    url: \"https://example.com\"                # Required: URL to screenshot\n    full_page: true                           # Optional: Capture full page (default: false)\n    width: 1920                               # Optional: Viewport width (default: 1920)\n    height: 1080                              # Optional: Viewport height (default: 1080)\n    wait_for: \"img\"                           # Optional: Wait for element\n    output_path: \"screenshots/page.png\"       # Optional: Save path\n\n# Interact with page\n- id: interact\n  action: interact_with_page\n  parameters:\n    url: \"https://example.com/form\"           # Required: URL to interact with\n    actions:                                  # Required: List of interactions\n      - type: \"fill\"\n        selector: \"#username\"\n        value: \"testuser\"\n      - type: \"click\"\n        selector: \"#submit-button\"\n      - type: \"wait\"\n        duration: 2000\n      - type: \"extract\"\n        selector: \".result\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 105,
    "line_end": 138,
    "type": "yaml",
    "description": "**Example Pipeline**:",
    "content": "name: web-research-pipeline\ndescription: Comprehensive web research with validation\n\nsteps:\n  # Search for information\n  - id: search_topic\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }} latest news 2024\"\n      max_results: 20\n      search_engine: \"google\"\n\n  # Scrape top results\n  - id: scrape_articles\n    for_each: \"{{ results.search_topic.results[:5] }}\"\n    as: result\n    action: scrape_page\n    parameters:\n      url: \"{{ result.url }}\"\n      selectors:\n        title: \"h1, h2.article-title\"\n        content: \"main, article, div.content\"\n        date: \"time, .date, .published\"\n      clean_html: true\n\n  # Take screenshots for reference\n  - id: capture_pages\n    for_each: \"{{ results.search_topic.results[:3] }}\"\n    as: result\n    action: screenshot_page\n    parameters:\n      url: \"{{ result.url }}\"\n      output_path: \"research/{{ inputs.topic }}/{{ loop.index }}.png\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 154,
    "line_end": 189,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Quick search\n- id: search\n  action: quick_search\n  parameters:\n    query: \"machine learning basics\"          # Required: Search query\n    max_results: 5                           # Optional: Result count (default: 10)\n    format: \"json\"                           # Optional: json|text (default: json)\n\n# News search\n- id: news\n  action: search_news\n  parameters:\n    query: \"AI breakthroughs\"                # Required: Search query\n    date_range: \"last_week\"                  # Optional: last_day|last_week|last_month|last_year\n    sources: [\"reuters\", \"techcrunch\"]       # Optional: Preferred sources\n    sort_by: \"relevance\"                     # Optional: relevance|date (default: relevance)\n\n# Academic search\n- id: academic\n  action: search_academic\n  parameters:\n    query: \"quantum computing\"               # Required: Search query\n    databases: [\"arxiv\", \"pubmed\"]          # Optional: Databases to search\n    year_range: \"2020-2024\"                 # Optional: Year range\n    peer_reviewed: true                      # Optional: Only peer-reviewed (default: false)\n\n# Image search\n- id: images\n  action: search_images\n  parameters:\n    query: \"data visualization examples\"     # Required: Search query\n    max_results: 10                         # Optional: Number of images\n    size: \"large\"                           # Optional: small|medium|large|any\n    type: \"photo\"                           # Optional: photo|clipart|lineart|any\n    license: \"creative_commons\"             # Optional: License filter",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 207,
    "line_end": 233,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Direct command execution\n- id: list_files\n  action: \"!ls -la /data\"\n\n# Command with parameters\n- id: run_command\n  action: execute_command\n  parameters:\n    command: \"python analyze.py\"              # Required: Command to execute\n    arguments: [\"--input\", \"data.csv\"]       # Optional: Command arguments\n    working_dir: \"/project\"                  # Optional: Working directory\n    environment:                             # Optional: Environment variables\n      PYTHONPATH: \"/project/lib\"\n      DEBUG: \"true\"\n    timeout: 300                             # Optional: Timeout in seconds (default: 60)\n    capture_output: true                     # Optional: Capture stdout/stderr (default: true)\n    shell: true                              # Optional: Use shell execution (default: true)\n\n# Run script file\n- id: run_analysis\n  action: run_script\n  parameters:\n    script_path: \"scripts/analyze.sh\"        # Required: Path to script\n    arguments: [\"{{ inputs.data_file }}\"]    # Optional: Script arguments\n    interpreter: \"bash\"                      # Optional: bash|python|node (default: auto-detect)\n    working_dir: \"{{ execution.temp_dir }}\"  # Optional: Working directory",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 238,
    "line_end": 282,
    "type": "yaml",
    "description": "**Example Pipeline**:",
    "content": "name: data-processing-automation\ndescription: Automated data processing with shell commands\n\nsteps:\n  # Setup environment\n  - id: setup\n    action: \"!mkdir -p output/{{ inputs.project_name }}\"\n\n  # Download data\n  - id: download\n    action: execute_command\n    parameters:\n      command: \"wget\"\n      arguments:\n        - \"-O\"\n        - \"data/raw_data.csv\"\n        - \"{{ inputs.data_url }}\"\n      timeout: 600\n\n  # Process with Python\n  - id: process\n    action: execute_command\n    parameters:\n      command: \"python\"\n      arguments:\n        - \"scripts/process_data.py\"\n        - \"--input\"\n        - \"data/raw_data.csv\"\n        - \"--output\"\n        - \"output/{{ inputs.project_name }}/processed.csv\"\n      environment:\n        DATA_QUALITY: \"high\"\n        PROCESSING_MODE: \"{{ inputs.mode }}\"\n\n  # Generate report with R\n  - id: report\n    action: \"!Rscript reports/generate_report.R output/{{ inputs.project_name }}/processed.csv\"\n\n  # Package results\n  - id: package\n    action: execute_command\n    parameters:\n      command: \"tar\"\n      arguments: [\"-czf\", \"{{ outputs.package }}\", \"output/{{ inputs.project_name }}\"]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 302,
    "line_end": 367,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Read file\n- id: read_config\n  action: read_file\n  parameters:\n    path: \"config/settings.json\"             # Required: File path\n    encoding: \"utf-8\"                        # Optional: File encoding (default: utf-8)\n    parse: true                              # Optional: Parse JSON/YAML (default: false)\n\n# Write file\n- id: save_results\n  action: write_file\n  parameters:\n    path: \"output/results.json\"              # Required: File path\n    content: \"{{ results.analysis | json }}\" # Required: Content to write\n    encoding: \"utf-8\"                        # Optional: File encoding (default: utf-8)\n    create_dirs: true                        # Optional: Create parent dirs (default: true)\n    overwrite: true                          # Optional: Overwrite existing (default: false)\n\n# Copy file\n- id: backup\n  action: copy_file\n  parameters:\n    source: \"data/important.db\"              # Required: Source path\n    destination: \"backup/important_{{ execution.timestamp }}.db\"  # Required: Destination\n    overwrite: false                         # Optional: Overwrite existing (default: false)\n\n# Move file\n- id: archive\n  action: move_file\n  parameters:\n    source: \"temp/processed.csv\"             # Required: Source path\n    destination: \"archive/2024/processed.csv\" # Required: Destination\n    create_dirs: true                        # Optional: Create parent dirs (default: true)\n\n# Delete file\n- id: cleanup\n  action: delete_file\n  parameters:\n    path: \"temp/*\"                           # Required: Path or pattern\n    recursive: true                          # Optional: Delete recursively (default: false)\n    force: false                             # Optional: Force deletion (default: false)\n\n# List directory\n- id: scan_files\n  action: list_directory\n  parameters:\n    path: \"data/\"                            # Required: Directory path\n    pattern: \"*.csv\"                         # Optional: File pattern\n    recursive: true                          # Optional: Search subdirs (default: false)\n    include_hidden: false                    # Optional: Include hidden files (default: false)\n    details: true                            # Optional: Include file details (default: false)\n\n# Create directory\n- id: setup_dirs\n  action: create_directory\n  parameters:\n    path: \"output/{{ inputs.project }}/data\" # Required: Directory path\n    parents: true                            # Optional: Create parents (default: true)\n    exist_ok: true                           # Optional: Ok if exists (default: true)\n\n# Check existence\n- id: check_file\n  action: file_exists\n  parameters:\n    path: \"config/custom.yaml\"               # Required: Path to check",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 372,
    "line_end": 416,
    "type": "yaml",
    "description": "**Example Pipeline**:",
    "content": "name: file-organization-pipeline\ndescription: Organize and process files automatically\n\nsteps:\n  # Check for existing data\n  - id: check_existing\n    action: file_exists\n    parameters:\n      path: \"data/current_dataset.csv\"\n\n  # Backup if exists\n  - id: backup\n    condition: \"{{ results.check_existing }}\"\n    action: copy_file\n    parameters:\n      source: \"data/current_dataset.csv\"\n      destination: \"backups/dataset_{{ execution.timestamp }}.csv\"\n\n  # Read configuration\n  - id: read_config\n    action: read_file\n    parameters:\n      path: \"config/processing.yaml\"\n      parse: true\n\n  # Process files based on config\n  - id: process_files\n    for_each: \"{{ results.read_config.file_patterns }}\"\n    as: pattern\n    action: list_directory\n    parameters:\n      path: \"{{ pattern.directory }}\"\n      pattern: \"{{ pattern.glob }}\"\n      recursive: true\n\n  # Organize by type\n  - id: organize\n    for_each: \"{{ results.process_files | flatten }}\"\n    as: file\n    action: move_file\n    parameters:\n      source: \"{{ file.path }}\"\n      destination: \"organized/{{ file.extension }}/{{ file.name }}\"\n      create_dirs: true",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 436,
    "line_end": 507,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Transform data\n- id: transform\n  action: transform_data\n  parameters:\n    data: \"$results.load_data\"               # Required: Input data or path\n    operations:                              # Required: List of operations\n      - type: \"rename_columns\"\n        mapping:\n          old_name: \"new_name\"\n          price: \"cost\"\n      - type: \"add_column\"\n        name: \"total\"\n        expression: \"quantity * cost\"\n      - type: \"drop_columns\"\n        columns: [\"unnecessary_field\"]\n      - type: \"convert_types\"\n        conversions:\n          date: \"datetime\"\n          amount: \"float\"\n\n# Filter data\n- id: filter\n  action: filter_data\n  parameters:\n    data: \"$results.transform\"               # Required: Input data\n    conditions:                              # Required: Filter conditions\n      - field: \"status\"\n        operator: \"equals\"                   # equals|not_equals|contains|gt|lt|gte|lte\n        value: \"active\"\n      - field: \"amount\"\n        operator: \"gt\"\n        value: 1000\n    mode: \"and\"                              # Optional: and|or (default: and)\n\n# Aggregate data\n- id: aggregate\n  action: aggregate_data\n  parameters:\n    data: \"$results.filter\"                  # Required: Input data\n    group_by: [\"category\", \"region\"]        # Optional: Grouping columns\n    aggregations:                            # Required: Aggregation rules\n      total_amount:\n        column: \"amount\"\n        function: \"sum\"                      # sum|mean|median|min|max|count|std\n      average_price:\n        column: \"price\"\n        function: \"mean\"\n      item_count:\n        column: \"*\"\n        function: \"count\"\n\n# Merge data\n- id: merge\n  action: merge_data\n  parameters:\n    left: \"$results.main_data\"               # Required: Left dataset\n    right: \"$results.lookup_data\"            # Required: Right dataset\n    on: \"customer_id\"                        # Required: Join column(s)\n    how: \"left\"                              # Optional: left|right|inner|outer (default: left)\n    suffixes: [\"_main\", \"_lookup\"]          # Optional: Column suffixes\n\n# Convert format\n- id: convert\n  action: convert_format\n  parameters:\n    data: \"$results.final_data\"              # Required: Input data\n    from_format: \"json\"                      # Optional: Auto-detect if not specified\n    to_format: \"parquet\"                     # Required: Target format\n    options:                                 # Optional: Format-specific options\n      compression: \"snappy\"\n      index: false",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 512,
    "line_end": 582,
    "type": "yaml",
    "description": "**Example Pipeline**:",
    "content": "name: sales-data-analysis\ndescription: Process and analyze sales data\n\nsteps:\n  # Load raw data\n  - id: load_sales\n    action: read_file\n    parameters:\n      path: \"data/sales_2024.csv\"\n      parse: true\n\n  # Clean and transform\n  - id: clean_data\n    action: transform_data\n    parameters:\n      data: \"$results.load_sales\"\n      operations:\n        - type: \"rename_columns\"\n          mapping:\n            \"Sale Date\": \"sale_date\"\n            \"Customer Name\": \"customer_name\"\n            \"Product ID\": \"product_id\"\n            \"Sale Amount\": \"amount\"\n        - type: \"convert_types\"\n          conversions:\n            sale_date: \"datetime\"\n            amount: \"float\"\n        - type: \"add_column\"\n          name: \"quarter\"\n          expression: \"sale_date.quarter\"\n\n  # Filter valid sales\n  - id: filter_valid\n    action: filter_data\n    parameters:\n      data: \"$results.clean_data\"\n      conditions:\n        - field: \"amount\"\n          operator: \"gt\"\n          value: 0\n        - field: \"product_id\"\n          operator: \"not_equals\"\n          value: null\n\n  # Aggregate by quarter\n  - id: quarterly_summary\n    action: aggregate_data\n    parameters:\n      data: \"$results.filter_valid\"\n      group_by: [\"quarter\", \"product_id\"]\n      aggregations:\n        total_sales:\n          column: \"amount\"\n          function: \"sum\"\n        avg_sale:\n          column: \"amount\"\n          function: \"mean\"\n        num_transactions:\n          column: \"*\"\n          function: \"count\"\n\n  # Save results\n  - id: save_summary\n    action: convert_format\n    parameters:\n      data: \"$results.quarterly_summary\"\n      to_format: \"excel\"\n      options:\n        sheet_name: \"Quarterly Sales\"\n        index: false",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 598,
    "line_end": 692,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Validate against schema\n- id: validate_structure\n  action: validate_schema\n  parameters:\n    data: \"$results.processed_data\"          # Required: Data to validate\n    schema:                                  # Required: Validation schema\n      type: \"object\"\n      required: [\"id\", \"name\", \"email\"]\n      properties:\n        id:\n          type: \"integer\"\n          minimum: 1\n        name:\n          type: \"string\"\n          minLength: 2\n          maxLength: 100\n        email:\n          type: \"string\"\n          format: \"email\"\n        age:\n          type: \"integer\"\n          minimum: 0\n          maximum: 150\n    strict: false                            # Optional: Strict mode (default: false)\n\n# Business rule validation\n- id: validate_rules\n  action: validate_data\n  parameters:\n    data: \"$results.transactions\"            # Required: Data to validate\n    rules:                                   # Required: Validation rules\n      - name: \"positive_amounts\"\n        field: \"amount\"\n        condition: \"value > 0\"\n        severity: \"error\"                    # error|warning|info\n        message: \"Transaction amounts must be positive\"\n\n      - name: \"valid_date_range\"\n        field: \"transaction_date\"\n        condition: \"value >= '2024-01-01' and value <= today()\"\n        severity: \"error\"\n\n      - name: \"customer_exists\"\n        field: \"customer_id\"\n        condition: \"value in valid_customers\"\n        severity: \"warning\"\n        context:\n          valid_customers: \"$results.customer_list\"\n\n    stop_on_error: false                     # Optional: Stop on first error (default: false)\n\n# Data quality checks\n- id: quality_check\n  action: check_quality\n  parameters:\n    data: \"$results.dataset\"                 # Required: Data to check\n    checks:                                  # Required: Quality checks\n      - type: \"completeness\"\n        threshold: 0.95                      # 95% non-null required\n        columns: [\"id\", \"name\", \"email\"]\n\n      - type: \"uniqueness\"\n        columns: [\"id\", \"email\"]\n\n      - type: \"consistency\"\n        rules:\n          - \"start_date <= end_date\"\n          - \"total == sum(line_items)\"\n\n      - type: \"accuracy\"\n        validations:\n          email: \"regex:^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\"\n          phone: \"regex:^\\\\+?1?\\\\d{9,15}$\"\n\n      - type: \"timeliness\"\n        field: \"last_updated\"\n        max_age_days: 30\n\n# Report validation\n- id: validate_report\n  action: validate_report\n  parameters:\n    report: \"$results.generated_report\"      # Required: Report to validate\n    checks:                                  # Required: Report checks\n      - \"completeness\"                       # All sections present\n      - \"accuracy\"                           # Facts are accurate\n      - \"consistency\"                        # No contradictions\n      - \"readability\"                        # Appropriate reading level\n      - \"citations\"                          # Sources properly cited\n    requirements:                            # Optional: Specific requirements\n      min_words: 1000\n      max_words: 5000\n      required_sections: [\"intro\", \"analysis\", \"conclusion\"]\n      citation_style: \"APA\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 697,
    "line_end": 784,
    "type": "yaml",
    "description": "**Example Pipeline**:",
    "content": "name: data-quality-pipeline\ndescription: Comprehensive data validation and quality assurance\n\nsteps:\n  # Load data\n  - id: load\n    action: read_file\n    parameters:\n      path: \"{{ inputs.data_file }}\"\n      parse: true\n\n  # Schema validation\n  - id: validate_schema\n    action: validate_schema\n    parameters:\n      data: \"$results.load\"\n      schema:\n        type: \"array\"\n        items:\n          type: \"object\"\n          required: [\"order_id\", \"customer_id\", \"amount\", \"date\"]\n          properties:\n            order_id:\n              type: \"string\"\n              pattern: \"^ORD-[0-9]{6}$\"\n            customer_id:\n              type: \"integer\"\n              minimum: 1\n            amount:\n              type: \"number\"\n              minimum: 0\n            date:\n              type: \"string\"\n              format: \"date\"\n\n  # Business rules\n  - id: validate_business\n    action: validate_data\n    parameters:\n      data: \"$results.load\"\n      rules:\n        - name: \"valid_amounts\"\n          field: \"amount\"\n          condition: \"value > 0 and value < 10000\"\n          severity: \"error\"\n\n        - name: \"recent_orders\"\n          field: \"date\"\n          condition: \"days_between(value, today()) <= 365\"\n          severity: \"warning\"\n          message: \"Order is older than 1 year\"\n\n  # Quality assessment\n  - id: quality_report\n    action: check_quality\n    parameters:\n      data: \"$results.load\"\n      checks:\n        - type: \"completeness\"\n          threshold: 0.98\n        - type: \"uniqueness\"\n          columns: [\"order_id\"]\n        - type: \"consistency\"\n          rules:\n            - \"item_total == quantity * unit_price\"\n        - type: \"accuracy\"\n          validations:\n            email: \"regex:^[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+$\"\n\n  # Generate validation report\n  - id: create_report\n    action: generate_content\n    parameters:\n      template: |\n        # Data Validation Report\n\n        ## Schema Validation\n        {{ results.validate_schema.summary }}\n\n        ## Business Rules\n        {{ results.validate_business.summary }}\n\n        ## Quality Metrics\n        {{ results.quality_report | format_quality_metrics }}\n\n        ## Recommendations\n        <AUTO>Based on the validation results, provide recommendations</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 804,
    "line_end": 870,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Generate content\n- id: generate\n  action: generate_content\n  parameters:\n    prompt: \"{{ inputs.prompt }}\"            # Required: Generation prompt\n    model: <AUTO>Select best model</AUTO>    # Optional: Model selection\n    max_tokens: 1000                         # Optional: Maximum tokens\n    temperature: 0.7                         # Optional: Creativity (0-2)\n    system_prompt: \"You are a helpful AI\"    # Optional: System message\n    format: \"markdown\"                       # Optional: Output format\n    style: \"professional\"                    # Optional: Writing style\n\n# Analyze text\n- id: analyze\n  action: analyze_text\n  parameters:\n    text: \"$results.document\"                # Required: Text to analyze\n    analysis_types:                          # Required: Types of analysis\n      - sentiment                            # Positive/negative/neutral\n      - entities                             # Named entities\n      - topics                               # Main topics\n      - summary                              # Brief summary\n      - key_points                           # Bullet points\n      - language                             # Detect language\n    output_format: \"structured\"              # Optional: structured|narrative\n\n# Extract information\n- id: extract\n  action: extract_information\n  parameters:\n    content: \"$results.raw_text\"             # Required: Source content\n    extract:                                 # Required: What to extract\n      dates:\n        description: \"All mentioned dates\"\n        format: \"YYYY-MM-DD\"\n      people:\n        description: \"Person names with roles\"\n        include_context: true\n      organizations:\n        description: \"Company and organization names\"\n      numbers:\n        description: \"Numerical values with units\"\n        categories: [\"financial\", \"metrics\"]\n    output_format: \"json\"                    # Optional: json|table|text\n\n# Generate code\n- id: code_gen\n  action: generate_code\n  parameters:\n    description: \"{{ inputs.feature_request }}\" # Required: What to build\n    language: \"python\"                       # Required: Programming language\n    framework: \"fastapi\"                     # Optional: Framework/library\n    include_tests: true                      # Optional: Generate tests\n    include_docs: true                       # Optional: Generate docs\n    style_guide: \"PEP8\"                     # Optional: Code style\n    example_usage: true                      # Optional: Include examples\n\n# Reasoning task\n- id: reason\n  action: reason_about\n  parameters:\n    question: \"{{ inputs.problem }}\"         # Required: Problem/question\n    context: \"$results.research\"             # Optional: Additional context\n    approach: \"step_by_step\"                 # Optional: Reasoning approach\n    show_work: true                          # Optional: Show reasoning\n    confidence_level: true                   # Optional: Include confidence",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 889,
    "line_end": 898,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# Query database\n- id: fetch_data\n  action: query_database\n  parameters:\n    connection: \"postgresql://localhost/mydb\" # Required: Connection string\n    query: \"SELECT * FROM users WHERE active = true\" # Required: SQL query\n    parameters: []                           # Optional: Query parameters\n    fetch_size: 1000                         # Optional: Batch size\n    timeout: 30                              # Optional: Query timeout",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 913,
    "line_end": 927,
    "type": "yaml",
    "description": "**Parameters**:",
    "content": "# REST API call\n- id: api_call\n  action: call_api\n  parameters:\n    url: \"https://api.example.com/data\"     # Required: API endpoint\n    method: \"POST\"                           # Required: HTTP method\n    headers:                                 # Optional: Headers\n      Authorization: \"Bearer {{ env.API_TOKEN }}\"\n      Content-Type: \"application/json\"\n    body:                                    # Optional: Request body\n      query: \"{{ inputs.search_term }}\"\n      limit: 100\n    timeout: 60                              # Optional: Request timeout\n    retry: 3                                 # Optional: Retry attempts",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 936,
    "line_end": 1002,
    "type": "yaml",
    "description": "----------------------------",
    "content": "name: comprehensive-research-tool-chain\ndescription: Chain multiple tools for research and reporting\n\nsteps:\n  # 1. Search multiple sources\n  - id: web_search\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }} latest research 2024\"\n      max_results: 20\n\n  # 2. Scrape promising articles\n  - id: scrape_articles\n    for_each: \"{{ results.web_search.results[:5] }}\"\n    as: article\n    action: scrape_page\n    parameters:\n      url: \"{{ article.url }}\"\n      selectors:\n        content: \"article, main, .content\"\n\n  # 3. Extract key information\n  - id: extract_facts\n    action: extract_information\n    parameters:\n      content: \"$results.scrape_articles\"\n      extract:\n        facts:\n          description: \"Key facts and findings\"\n        statistics:\n          description: \"Numerical data with context\"\n        quotes:\n          description: \"Notable quotes with attribution\"\n\n  # 4. Validate information\n  - id: cross_validate\n    action: validate_data\n    parameters:\n      data: \"$results.extract_facts\"\n      rules:\n        - name: \"source_diversity\"\n          condition: \"count(unique(sources)) >= 3\"\n          severity: \"warning\"\n\n  # 5. Generate report\n  - id: create_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Create a comprehensive report about {{ inputs.topic }}\n        using the following validated information:\n        {{ results.extract_facts | json }}\n      style: \"academic\"\n      format: \"markdown\"\n      max_tokens: 2000\n\n  # 6. Save report\n  - id: save_report\n    action: write_file\n    parameters:\n      path: \"reports/{{ inputs.topic }}_{{ execution.date }}.md\"\n      content: \"$results.create_report\"\n\n  # 7. Generate PDF\n  - id: create_pdf\n    action: \"!pandoc -f markdown -t pdf -o reports/{{ inputs.topic }}.pdf reports/{{ inputs.topic }}_{{ execution.date }}.md\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 1008,
    "line_end": 1099,
    "type": "yaml",
    "description": "------------------------",
    "content": "name: etl-tool-chain\ndescription: Extract, transform, and load data using tool chain\n\nsteps:\n  # Extract from multiple sources\n  - id: extract_database\n    action: query_database\n    parameters:\n      connection: \"{{ env.DB_CONNECTION }}\"\n      query: \"SELECT * FROM sales WHERE date >= '2024-01-01'\"\n\n  - id: extract_api\n    action: call_api\n    parameters:\n      url: \"https://api.company.com/v2/transactions\"\n      method: \"GET\"\n      headers:\n        Authorization: \"Bearer {{ env.API_KEY }}\"\n      params:\n        start_date: \"2024-01-01\"\n        page_size: 1000\n\n  - id: extract_files\n    action: list_directory\n    parameters:\n      path: \"data/uploads/\"\n      pattern: \"sales_*.csv\"\n      recursive: true\n\n  # Load file data\n  - id: load_files\n    for_each: \"{{ results.extract_files }}\"\n    as: file\n    action: read_file\n    parameters:\n      path: \"{{ file.path }}\"\n      parse: true\n\n  # Transform all data\n  - id: merge_all\n    action: merge_data\n    parameters:\n      datasets:\n        - \"$results.extract_database\"\n        - \"$results.extract_api.data\"\n        - \"$results.load_files\"\n      key: \"transaction_id\"\n\n  - id: clean_data\n    action: transform_data\n    parameters:\n      data: \"$results.merge_all\"\n      operations:\n        - type: \"remove_duplicates\"\n          columns: [\"transaction_id\"]\n        - type: \"fill_missing\"\n          strategy: \"forward\"\n        - type: \"standardize_formats\"\n          columns:\n            date: \"YYYY-MM-DD\"\n            amount: \"decimal(10,2)\"\n\n  # Validate\n  - id: validate_quality\n    action: check_quality\n    parameters:\n      data: \"$results.clean_data\"\n      checks:\n        - type: \"completeness\"\n          threshold: 0.99\n        - type: \"accuracy\"\n          validations:\n            amount: \"range:0,1000000\"\n            date: \"date_range:2024-01-01,today\"\n\n  # Load to destination\n  - id: save_processed\n    action: write_file\n    parameters:\n      path: \"processed/sales_cleaned_{{ execution.date }}.parquet\"\n      content: \"$results.clean_data\"\n      format: \"parquet\"\n\n  - id: update_database\n    condition: \"{{ results.validate_quality.passed }}\"\n    action: insert_data\n    parameters:\n      connection: \"{{ env.DW_CONNECTION }}\"\n      table: \"sales_fact\"\n      data: \"$results.clean_data\"\n      mode: \"append\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 1110,
    "line_end": 1152,
    "type": "python",
    "description": "To create your own tools:",
    "content": "from orchestrator.tools.base import Tool\n\nclass MyCustomTool(Tool):\n    def __init__(self):\n        super().__init__(\n            name=\"my-custom-tool\",\n            description=\"Does something special\"\n        )\n\n        # Define parameters\n        self.add_parameter(\n            name=\"input_data\",\n            type=\"string\",\n            description=\"Data to process\",\n            required=True\n        )\n\n        self.add_parameter(\n            name=\"mode\",\n            type=\"string\",\n            description=\"Processing mode\",\n            required=False,\n            default=\"standard\",\n            enum=[\"standard\", \"advanced\", \"expert\"]\n        )\n\n    async def execute(self, **kwargs):\n        \"\"\"Execute the tool action.\"\"\"\n        input_data = kwargs[\"input_data\"]\n        mode = kwargs.get(\"mode\", \"standard\")\n\n        # Your tool logic here\n        result = process_data(input_data, mode)\n\n        return {\n            \"status\": \"success\",\n            \"result\": result,\n            \"metadata\": {\n                \"mode\": mode,\n                \"timestamp\": datetime.now()\n            }\n        }",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tool_reference.rst",
    "line_start": 1160,
    "line_end": 1175,
    "type": "python",
    "description": "Register your tool to make it available:",
    "content": "from orchestrator.tools.base import default_registry\n\n# Register tool\ntool = MyCustomTool()\ndefault_registry.register(tool)\n\n# Use in pipeline\npipeline_yaml = \"\"\"\nsteps:\n  - id: custom_step\n    action: my-custom-tool\n    parameters:\n      input_data: \"{{ inputs.data }}\"\n      mode: \"advanced\"\n\"\"\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 35,
    "line_end": 239,
    "type": "yaml",
    "description": "Create ``sales_etl.yaml``:",
    "content": "name: sales-etl-pipeline\ndescription: Extract, transform, and load sales data\n\ninputs:\n  data_source:\n    type: string\n    description: \"Path to source data file\"\n    required: true\n\n  output_format:\n    type: string\n    description: \"Output format\"\n    default: \"parquet\"\n    validation:\n      enum: [\"csv\", \"json\", \"parquet\", \"excel\"]\n\n  date_range:\n    type: object\n    description: \"Date range for filtering\"\n    default:\n      start: \"2024-01-01\"\n      end: \"2024-12-31\"\n\noutputs:\n  processed_data:\n    type: string\n    value: \"processed/sales_{{ execution.date }}.{{ inputs.output_format }}\"\n\n  quality_report:\n    type: string\n    value: \"reports/quality_{{ execution.date }}.json\"\n\n  summary_stats:\n    type: string\n    value: \"reports/summary_{{ execution.date }}.md\"\n\nsteps:\n  # Extract: Load raw data\n  - id: extract_data\n    action: read_file\n    parameters:\n      path: \"{{ inputs.data_source }}\"\n      parse: true\n    error_handling:\n      retry:\n        max_attempts: 3\n      fallback:\n        action: generate_content\n        parameters:\n          prompt: \"Generate sample sales data for testing\"\n\n  # Transform: Clean and process data\n  - id: clean_data\n    action: transform_data\n    parameters:\n      data: \"$results.extract_data\"\n      operations:\n        # Standardize column names\n        - type: \"rename_columns\"\n          mapping:\n            \"Sale Date\": \"sale_date\"\n            \"Customer Name\": \"customer_name\"\n            \"Product ID\": \"product_id\"\n            \"Sale Amount\": \"amount\"\n            \"Quantity\": \"quantity\"\n            \"Sales Rep\": \"sales_rep\"\n\n        # Convert data types\n        - type: \"convert_types\"\n          conversions:\n            sale_date: \"datetime\"\n            amount: \"float\"\n            quantity: \"integer\"\n            product_id: \"string\"\n\n        # Remove duplicates\n        - type: \"remove_duplicates\"\n          columns: [\"product_id\", \"sale_date\", \"customer_name\"]\n\n        # Handle missing values\n        - type: \"fill_missing\"\n          strategy: \"forward\"\n          columns: [\"sales_rep\"]\n\n        # Add calculated fields\n        - type: \"add_column\"\n          name: \"total_value\"\n          expression: \"amount * quantity\"\n\n        - type: \"add_column\"\n          name: \"quarter\"\n          expression: \"quarter(sale_date)\"\n\n        - type: \"add_column\"\n          name: \"year\"\n          expression: \"year(sale_date)\"\n\n  # Filter data by date range\n  - id: filter_data\n    action: filter_data\n    parameters:\n      data: \"$results.clean_data\"\n      conditions:\n        - field: \"sale_date\"\n          operator: \"gte\"\n          value: \"{{ inputs.date_range.start }}\"\n        - field: \"sale_date\"\n          operator: \"lte\"\n          value: \"{{ inputs.date_range.end }}\"\n        - field: \"amount\"\n          operator: \"gt\"\n          value: 0\n\n  # Data quality validation\n  - id: validate_quality\n    action: check_quality\n    parameters:\n      data: \"$results.filter_data\"\n      checks:\n        - type: \"completeness\"\n          threshold: 0.95\n          columns: [\"product_id\", \"amount\", \"sale_date\"]\n\n        - type: \"uniqueness\"\n          columns: [\"product_id\", \"sale_date\", \"customer_name\"]\n\n        - type: \"consistency\"\n          rules:\n            - \"total_value == amount * quantity\"\n            - \"amount > 0\"\n            - \"quantity > 0\"\n\n        - type: \"accuracy\"\n          validations:\n            product_id: \"regex:^PROD-[0-9]{6}$\"\n            amount: \"range:1,50000\"\n            quantity: \"range:1,1000\"\n\n  # Generate summary statistics\n  - id: calculate_summary\n    action: aggregate_data\n    parameters:\n      data: \"$results.filter_data\"\n      group_by: [\"year\", \"quarter\"]\n      aggregations:\n        total_sales:\n          column: \"total_value\"\n          function: \"sum\"\n        avg_sale:\n          column: \"amount\"\n          function: \"mean\"\n        num_transactions:\n          column: \"*\"\n          function: \"count\"\n        unique_customers:\n          column: \"customer_name\"\n          function: \"nunique\"\n        top_product:\n          column: \"product_id\"\n          function: \"mode\"\n\n  # Load: Save processed data\n  - id: save_processed_data\n    action: convert_format\n    parameters:\n      data: \"$results.filter_data\"\n      to_format: \"{{ inputs.output_format }}\"\n      output_path: \"{{ outputs.processed_data }}\"\n      options:\n        compression: \"snappy\"\n        index: false\n\n  # Save quality report\n  - id: save_quality_report\n    action: write_file\n    parameters:\n      path: \"{{ outputs.quality_report }}\"\n      content: \"{{ results.validate_quality | json }}\"\n\n  # Generate readable summary\n  - id: create_summary_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Create a summary report for sales data processing:\n\n        Quality Results: {{ results.validate_quality | json }}\n        Summary Statistics: {{ results.calculate_summary | json }}\n\n        Include:\n        - Data quality assessment\n        - Key metrics and trends\n        - Any issues or recommendations\n        - Processing summary\n\n      style: \"professional\"\n      format: \"markdown\"\n\n  # Save summary report\n  - id: save_summary\n    action: write_file\n    parameters:\n      path: \"{{ outputs.summary_stats }}\"\n      content: \"$results.create_summary_report\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 245,
    "line_end": 264,
    "type": "python",
    "description": "----------------------------",
    "content": "import orchestrator as orc\n\n# Initialize\norc.init_models()\n\n# Compile pipeline\netl_pipeline = orc.compile(\"sales_etl.yaml\")\n\n# Process sales data\nresult = etl_pipeline.run(\n    data_source=\"data/raw/sales_2024.csv\",\n    output_format=\"parquet\",\n    date_range={\n        \"start\": \"2024-01-01\",\n        \"end\": \"2024-06-30\"\n    }\n)\n\nprint(f\"ETL completed: {result}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 277,
    "line_end": 521,
    "type": "yaml",
    "description": "Create ``data_integration.yaml``:",
    "content": "name: multi-source-integration\ndescription: Integrate data from multiple sources with validation\n\ninputs:\n  sources:\n    type: object\n    description: \"Data source configurations\"\n    required: true\n    # Example:\n    # database:\n    #   type: \"postgresql\"\n    #   connection: \"postgresql://...\"\n    #   query: \"SELECT * FROM sales\"\n    # api:\n    #   type: \"rest\"\n    #   url: \"https://api.company.com/data\"\n    #   headers: {...}\n    # files:\n    #   type: \"file\"\n    #   paths: [\"data1.csv\", \"data2.json\"]\n\n  merge_strategy:\n    type: string\n    description: \"How to merge data sources\"\n    default: \"outer\"\n    validation:\n      enum: [\"inner\", \"outer\", \"left\", \"right\"]\n\n  deduplication_fields:\n    type: array\n    description: \"Fields to use for deduplication\"\n    default: [\"id\", \"timestamp\"]\n\noutputs:\n  integrated_data:\n    type: string\n    value: \"integrated/master_data_{{ execution.timestamp }}.parquet\"\n\n  integration_report:\n    type: string\n    value: \"reports/integration_{{ execution.timestamp }}.md\"\n\nsteps:\n  # Extract from database sources\n  - id: extract_database\n    condition: \"'database' in inputs.sources\"\n    action: query_database\n    parameters:\n      connection: \"{{ inputs.sources.database.connection }}\"\n      query: \"{{ inputs.sources.database.query }}\"\n      fetch_size: 10000\n    error_handling:\n      continue_on_error: true\n\n  # Extract from API sources\n  - id: extract_api\n    condition: \"'api' in inputs.sources\"\n    action: call_api\n    parameters:\n      url: \"{{ inputs.sources.api.url }}\"\n      method: \"GET\"\n      headers: \"{{ inputs.sources.api.headers | default({}) }}\"\n      params: \"{{ inputs.sources.api.params | default({}) }}\"\n      timeout: 300\n    error_handling:\n      retry:\n        max_attempts: 3\n        backoff: \"exponential\"\n\n  # Extract from file sources\n  - id: extract_files\n    condition: \"'files' in inputs.sources\"\n    for_each: \"{{ inputs.sources.files.paths }}\"\n    as: file_path\n    action: read_file\n    parameters:\n      path: \"{{ file_path }}\"\n      parse: true\n\n  # Standardize data schemas\n  - id: standardize_database\n    condition: \"results.extract_database is defined\"\n    action: transform_data\n    parameters:\n      data: \"$results.extract_database\"\n      operations:\n        - type: \"add_column\"\n          name: \"source\"\n          value: \"database\"\n        - type: \"standardize_schema\"\n          target_schema:\n            id: \"string\"\n            timestamp: \"datetime\"\n            value: \"float\"\n            category: \"string\"\n\n  - id: standardize_api\n    condition: \"results.extract_api is defined\"\n    action: transform_data\n    parameters:\n      data: \"$results.extract_api.data\"\n      operations:\n        - type: \"add_column\"\n          name: \"source\"\n          value: \"api\"\n        - type: \"flatten_nested\"\n          columns: [\"metadata\", \"attributes\"]\n        - type: \"standardize_schema\"\n          target_schema:\n            id: \"string\"\n            timestamp: \"datetime\"\n            value: \"float\"\n            category: \"string\"\n\n  - id: standardize_files\n    condition: \"results.extract_files is defined\"\n    action: transform_data\n    parameters:\n      data: \"$results.extract_files\"\n      operations:\n        - type: \"add_column\"\n          name: \"source\"\n          value: \"files\"\n        - type: \"combine_files\"\n          strategy: \"union\"\n        - type: \"standardize_schema\"\n          target_schema:\n            id: \"string\"\n            timestamp: \"datetime\"\n            value: \"float\"\n            category: \"string\"\n\n  # Merge all data sources\n  - id: merge_sources\n    action: merge_data\n    parameters:\n      datasets:\n        - \"$results.standardize_database\"\n        - \"$results.standardize_api\"\n        - \"$results.standardize_files\"\n      how: \"{{ inputs.merge_strategy }}\"\n      on: [\"id\"]\n      suffixes: [\"_db\", \"_api\", \"_file\"]\n\n  # Remove duplicates\n  - id: deduplicate\n    action: transform_data\n    parameters:\n      data: \"$results.merge_sources\"\n      operations:\n        - type: \"remove_duplicates\"\n          columns: \"{{ inputs.deduplication_fields }}\"\n          keep: \"last\"  # Keep most recent\n\n  # Data quality assessment\n  - id: assess_integration_quality\n    action: check_quality\n    parameters:\n      data: \"$results.deduplicate\"\n      checks:\n        - type: \"completeness\"\n          threshold: 0.90\n          critical_columns: [\"id\", \"timestamp\"]\n\n        - type: \"consistency\"\n          rules:\n            - \"value_db == value_api OR value_db IS NULL OR value_api IS NULL\"\n            - \"timestamp >= '2020-01-01'\"\n\n        - type: \"accuracy\"\n          validations:\n            id: \"not_null\"\n            timestamp: \"datetime_format\"\n            value: \"numeric_range:-1000000,1000000\"\n\n  # Resolve conflicts between sources\n  - id: resolve_conflicts\n    action: transform_data\n    parameters:\n      data: \"$results.deduplicate\"\n      operations:\n        - type: \"resolve_conflicts\"\n          strategy: \"priority\"\n          priority_order: [\"database\", \"api\", \"files\"]\n          conflict_columns: [\"value\", \"category\"]\n\n        - type: \"add_column\"\n          name: \"confidence_score\"\n          expression: \"calculate_confidence(source_count, data_age, validation_status)\"\n\n  # Create final integrated dataset\n  - id: finalize_integration\n    action: transform_data\n    parameters:\n      data: \"$results.resolve_conflicts\"\n      operations:\n        - type: \"select_columns\"\n          columns: [\"id\", \"timestamp\", \"value\", \"category\", \"source\", \"confidence_score\"]\n\n        - type: \"sort\"\n          columns: [\"timestamp\"]\n          ascending: [false]\n\n  # Save integrated data\n  - id: save_integrated\n    action: convert_format\n    parameters:\n      data: \"$results.finalize_integration\"\n      to_format: \"parquet\"\n      output_path: \"{{ outputs.integrated_data }}\"\n      options:\n        compression: \"snappy\"\n        partition_cols: [\"category\"]\n\n  # Generate integration report\n  - id: create_integration_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Create an integration report for multi-source data merge:\n\n        Sources processed:\n        {% for source in inputs.sources.keys() %}\n        - {{ source }}\n        {% endfor %}\n\n        Quality assessment: {{ results.assess_integration_quality | json }}\n        Final record count: {{ results.finalize_integration | length }}\n\n        Include:\n        - Source summary and statistics\n        - Data quality metrics\n        - Conflict resolution summary\n        - Recommendations for data improvement\n\n      style: \"technical\"\n      format: \"markdown\"\n\n  # Save integration report\n  - id: save_report\n    action: write_file\n    parameters:\n      path: \"{{ outputs.integration_report }}\"\n      content: \"$results.create_integration_report\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 527,
    "line_end": 558,
    "type": "python",
    "description": "-----------------------------------",
    "content": "import orchestrator as orc\n\n# Initialize\norc.init_models()\n\n# Compile integration pipeline\nintegration = orc.compile(\"data_integration.yaml\")\n\n# Integrate data from multiple sources\nresult = integration.run(\n    sources={\n        \"database\": {\n            \"type\": \"postgresql\",\n            \"connection\": \"postgresql://user:pass@localhost/mydb\",\n            \"query\": \"SELECT * FROM transactions WHERE date >= '2024-01-01'\"\n        },\n        \"api\": {\n            \"type\": \"rest\",\n            \"url\": \"https://api.external.com/v1/data\",\n            \"headers\": {\"Authorization\": \"Bearer token123\"}\n        },\n        \"files\": {\n            \"type\": \"file\",\n            \"paths\": [\"data/file1.csv\", \"data/file2.json\"]\n        }\n    },\n    merge_strategy=\"outer\",\n    deduplication_fields=[\"transaction_id\", \"timestamp\"]\n)\n\nprint(f\"Integration completed: {result}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 571,
    "line_end": 815,
    "type": "yaml",
    "description": "Create ``data_quality.yaml``:",
    "content": "name: data-quality-assessment\ndescription: Comprehensive data quality evaluation and reporting\n\ninputs:\n  dataset_path:\n    type: string\n    required: true\n\n  quality_rules:\n    type: object\n    description: \"Custom quality rules\"\n    default:\n      completeness_threshold: 0.95\n      uniqueness_fields: [\"id\"]\n      date_range_field: \"created_date\"\n      numeric_fields: [\"amount\", \"quantity\"]\n\n  remediation_mode:\n    type: string\n    description: \"How to handle quality issues\"\n    default: \"report\"\n    validation:\n      enum: [\"report\", \"fix\", \"quarantine\"]\n\noutputs:\n  quality_report:\n    type: string\n    value: \"quality/report_{{ execution.timestamp }}.html\"\n\n  cleaned_data:\n    type: string\n    value: \"quality/cleaned_{{ execution.timestamp }}.parquet\"\n\n  issues_log:\n    type: string\n    value: \"quality/issues_{{ execution.timestamp }}.json\"\n\nsteps:\n  # Load the dataset\n  - id: load_dataset\n    action: read_file\n    parameters:\n      path: \"{{ inputs.dataset_path }}\"\n      parse: true\n\n  # Basic data profiling\n  - id: profile_data\n    action: analyze_data\n    parameters:\n      data: \"$results.load_dataset\"\n      analysis_types:\n        - schema\n        - statistics\n        - distributions\n        - patterns\n        - outliers\n\n  # Completeness assessment\n  - id: check_completeness\n    action: check_quality\n    parameters:\n      data: \"$results.load_dataset\"\n      checks:\n        - type: \"completeness\"\n          threshold: \"{{ inputs.quality_rules.completeness_threshold }}\"\n          report_by_column: true\n\n        - type: \"null_patterns\"\n          identify_patterns: true\n\n  # Uniqueness validation\n  - id: check_uniqueness\n    action: validate_data\n    parameters:\n      data: \"$results.load_dataset\"\n      rules:\n        - name: \"primary_key_uniqueness\"\n          type: \"uniqueness\"\n          columns: \"{{ inputs.quality_rules.uniqueness_fields }}\"\n          severity: \"error\"\n\n        - name: \"near_duplicates\"\n          type: \"similarity\"\n          threshold: 0.9\n          columns: [\"name\", \"email\"]\n          severity: \"warning\"\n\n  # Consistency validation\n  - id: check_consistency\n    action: validate_data\n    parameters:\n      data: \"$results.load_dataset\"\n      rules:\n        - name: \"date_logic\"\n          condition: \"start_date <= end_date\"\n          severity: \"error\"\n\n        - name: \"numeric_consistency\"\n          condition: \"total == sum(line_items)\"\n          severity: \"error\"\n\n        - name: \"referential_integrity\"\n          type: \"foreign_key\"\n          reference_table: \"lookup_table\"\n          foreign_key: \"category_id\"\n          severity: \"warning\"\n\n  # Accuracy validation\n  - id: check_accuracy\n    action: validate_data\n    parameters:\n      data: \"$results.load_dataset\"\n      rules:\n        - name: \"email_format\"\n          field: \"email\"\n          validation: \"regex:^[\\\\w.-]+@[\\\\w.-]+\\\\.\\\\w+$\"\n          severity: \"warning\"\n\n        - name: \"phone_format\"\n          field: \"phone\"\n          validation: \"regex:^\\\\+?1?\\\\d{9,15}$\"\n          severity: \"info\"\n\n        - name: \"numeric_ranges\"\n          field: \"{{ inputs.quality_rules.numeric_fields }}\"\n          validation: \"range:0,999999\"\n          severity: \"error\"\n\n  # Timeliness assessment\n  - id: check_timeliness\n    action: validate_data\n    parameters:\n      data: \"$results.load_dataset\"\n      rules:\n        - name: \"data_freshness\"\n          field: \"{{ inputs.quality_rules.date_range_field }}\"\n          condition: \"date_diff(value, today()) <= 30\"\n          severity: \"warning\"\n          message: \"Data is older than 30 days\"\n\n  # Outlier detection\n  - id: detect_outliers\n    action: analyze_data\n    parameters:\n      data: \"$results.load_dataset\"\n      analysis_types:\n        - outliers\n      methods:\n        - statistical  # Z-score, IQR\n        - isolation_forest\n        - local_outlier_factor\n      numeric_columns: \"{{ inputs.quality_rules.numeric_fields }}\"\n\n  # Compile quality issues\n  - id: compile_issues\n    action: transform_data\n    parameters:\n      data:\n        completeness: \"$results.check_completeness\"\n        uniqueness: \"$results.check_uniqueness\"\n        consistency: \"$results.check_consistency\"\n        accuracy: \"$results.check_accuracy\"\n        timeliness: \"$results.check_timeliness\"\n        outliers: \"$results.detect_outliers\"\n      operations:\n        - type: \"consolidate_issues\"\n          prioritize: true\n        - type: \"categorize_severity\"\n          levels: [\"critical\", \"major\", \"minor\", \"info\"]\n\n  # Data remediation (if requested)\n  - id: remediate_data\n    condition: \"inputs.remediation_mode in ['fix', 'quarantine']\"\n    action: transform_data\n    parameters:\n      data: \"$results.load_dataset\"\n      operations:\n        # Fix common issues\n        - type: \"standardize_formats\"\n          columns:\n            email: \"lowercase\"\n            phone: \"normalize_phone\"\n            name: \"title_case\"\n\n        - type: \"fill_missing\"\n          strategy: \"smart\"  # Use ML-based imputation\n          columns: \"{{ inputs.quality_rules.numeric_fields }}\"\n\n        - type: \"remove_outliers\"\n          method: \"iqr\"\n          columns: \"{{ inputs.quality_rules.numeric_fields }}\"\n          action: \"{{ 'quarantine' if inputs.remediation_mode == 'quarantine' else 'remove' }}\"\n\n        - type: \"deduplicate\"\n          strategy: \"keep_best\"  # Keep record with highest completeness\n\n  # Generate comprehensive quality report\n  - id: create_quality_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Create a comprehensive data quality report:\n\n        Dataset: {{ inputs.dataset_path }}\n        Profile: {{ results.profile_data | json }}\n        Issues: {{ results.compile_issues | json }}\n\n        Include:\n        1. Executive Summary\n        2. Data Profile Overview\n        3. Quality Metrics Dashboard\n        4. Issue Analysis by Category\n        5. Impact Assessment\n        6. Remediation Recommendations\n        7. Quality Score Calculation\n\n        Format as HTML with charts and tables.\n\n      style: \"technical\"\n      format: \"html\"\n      max_tokens: 3000\n\n  # Save quality report\n  - id: save_quality_report\n    action: write_file\n    parameters:\n      path: \"{{ outputs.quality_report }}\"\n      content: \"$results.create_quality_report\"\n\n  # Save cleaned data (if remediation performed)\n  - id: save_cleaned_data\n    condition: \"inputs.remediation_mode in ['fix', 'quarantine']\"\n    action: write_file\n    parameters:\n      path: \"{{ outputs.cleaned_data }}\"\n      content: \"$results.remediate_data\"\n      format: \"parquet\"\n\n  # Save issues log\n  - id: save_issues_log\n    action: write_file\n    parameters:\n      path: \"{{ outputs.issues_log }}\"\n      content: \"{{ results.compile_issues | json }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 828,
    "line_end": 923,
    "type": "yaml",
    "description": "Create ``realtime_processing.yaml``:",
    "content": "name: realtime-data-processing\ndescription: Process streaming data with real-time analytics\n\ninputs:\n  stream_source:\n    type: object\n    description: \"Stream configuration\"\n    required: true\n    # Example:\n    # type: \"kafka\"\n    # topic: \"events\"\n    # batch_size: 1000\n    # window_size: \"5m\"\n\n  processing_rules:\n    type: array\n    description: \"Processing rules to apply\"\n    default:\n      - type: \"filter\"\n        condition: \"event_type in ['purchase', 'click']\"\n      - type: \"enrich\"\n        lookup_table: \"user_profiles\"\n      - type: \"aggregate\"\n        window: \"5m\"\n        metrics: [\"count\", \"sum\", \"avg\"]\n\noutputs:\n  processed_stream:\n    type: string\n    value: \"stream/processed_{{ execution.date }}\"\n\n  alerts:\n    type: string\n    value: \"alerts/stream_alerts_{{ execution.timestamp }}.json\"\n\nsteps:\n  # Connect to stream source\n  - id: connect_stream\n    action: connect_stream\n    parameters:\n      source: \"{{ inputs.stream_source }}\"\n      batch_size: \"{{ inputs.stream_source.batch_size | default(1000) }}\"\n      timeout: 30\n\n  # Process incoming batches\n  - id: process_batches\n    action: process_stream_batch\n    parameters:\n      stream: \"$results.connect_stream\"\n      processing_rules: \"{{ inputs.processing_rules }}\"\n      window_config:\n        size: \"{{ inputs.stream_source.window_size | default('5m') }}\"\n        type: \"tumbling\"  # or \"sliding\", \"session\"\n\n  # Real-time anomaly detection\n  - id: detect_anomalies\n    action: detect_anomalies\n    parameters:\n      data: \"$results.process_batches\"\n      methods:\n        - statistical_control\n        - machine_learning\n      thresholds:\n        statistical: 3.0  # standard deviations\n        ml_confidence: 0.95\n\n  # Generate alerts\n  - id: generate_alerts\n    condition: \"results.detect_anomalies.anomalies | length > 0\"\n    action: generate_content\n    parameters:\n      prompt: |\n        Generate alerts for detected anomalies:\n        {{ results.detect_anomalies.anomalies | json }}\n\n        Include severity, description, and recommended actions.\n\n      format: \"json\"\n\n  # Save processed data\n  - id: save_processed\n    action: write_stream\n    parameters:\n      data: \"$results.process_batches\"\n      destination: \"{{ outputs.processed_stream }}\"\n      format: \"parquet\"\n      partition_by: [\"date\", \"hour\"]\n\n  # Save alerts\n  - id: save_alerts\n    condition: \"results.generate_alerts is defined\"\n    action: write_file\n    parameters:\n      path: \"{{ outputs.alerts }}\"\n      content: \"$results.generate_alerts\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 932,
    "line_end": 990,
    "type": "yaml",
    "description": "---------------------------------",
    "content": "name: customer-data-platform\ndescription: Unified customer data processing and analytics\n\ninputs:\n  customer_sources:\n    type: object\n    required: true\n    # CRM, support tickets, web analytics, purchase history\n\nsteps:\n  # Extract from all customer touchpoints\n  - id: extract_crm\n    action: query_database\n    parameters:\n      connection: \"{{ inputs.customer_sources.crm.connection }}\"\n      query: \"SELECT * FROM customers WHERE updated_at >= CURRENT_DATE - INTERVAL '1 day'\"\n\n  - id: extract_support\n    action: call_api\n    parameters:\n      url: \"{{ inputs.customer_sources.support.api_url }}\"\n      headers:\n        Authorization: \"Bearer {{ env.SUPPORT_API_KEY }}\"\n\n  - id: extract_analytics\n    action: read_file\n    parameters:\n      path: \"{{ inputs.customer_sources.analytics.export_path }}\"\n      parse: true\n\n  # Create unified customer profiles\n  - id: merge_customer_data\n    action: merge_data\n    parameters:\n      datasets:\n        - \"$results.extract_crm\"\n        - \"$results.extract_support\"\n        - \"$results.extract_analytics\"\n      on: \"customer_id\"\n      how: \"outer\"\n\n  # Calculate customer metrics\n  - id: calculate_metrics\n    action: transform_data\n    parameters:\n      data: \"$results.merge_customer_data\"\n      operations:\n        - type: \"add_column\"\n          name: \"customer_lifetime_value\"\n          expression: \"sum(purchase_amounts) * retention_probability\"\n\n        - type: \"add_column\"\n          name: \"churn_risk_score\"\n          expression: \"calculate_churn_risk(days_since_last_activity, support_tickets, engagement_score)\"\n\n        - type: \"add_column\"\n          name: \"segment\"\n          expression: \"classify_customer_segment(clv, engagement, recency)\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 996,
    "line_end": 1062,
    "type": "yaml",
    "description": "----------------------------------",
    "content": "name: financial-data-pipeline\ndescription: Process financial transactions with compliance checks\n\ninputs:\n  transaction_sources:\n    type: array\n    required: true\n\n  compliance_rules:\n    type: object\n    required: true\n\nsteps:\n  # Extract transactions from multiple sources\n  - id: extract_transactions\n    for_each: \"{{ inputs.transaction_sources }}\"\n    as: source\n    action: extract_financial_data\n    parameters:\n      source_config: \"{{ source }}\"\n      date_range: \"{{ execution.date | date_range('-1d') }}\"\n\n  # Compliance screening\n  - id: screen_transactions\n    action: validate_data\n    parameters:\n      data: \"$results.extract_transactions\"\n      rules:\n        - name: \"aml_screening\"\n          type: \"anti_money_laundering\"\n          threshold: \"{{ inputs.compliance_rules.aml_threshold }}\"\n\n        - name: \"sanctions_check\"\n          type: \"sanctions_screening\"\n          watchlists: \"{{ inputs.compliance_rules.watchlists }}\"\n\n        - name: \"pep_screening\"\n          type: \"politically_exposed_person\"\n          databases: \"{{ inputs.compliance_rules.pep_databases }}\"\n\n  # Risk scoring\n  - id: calculate_risk_scores\n    action: transform_data\n    parameters:\n      data: \"$results.extract_transactions\"\n      operations:\n        - type: \"add_column\"\n          name: \"risk_score\"\n          expression: \"calculate_transaction_risk(amount, counterparty, geography, transaction_type)\"\n\n        - type: \"add_column\"\n          name: \"risk_category\"\n          expression: \"categorize_risk(risk_score)\"\n\n  # Generate compliance report\n  - id: create_compliance_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Generate daily compliance report:\n\n        Transactions processed: {{ results.extract_transactions | length }}\n        Screening results: {{ results.screen_transactions | json }}\n        Risk distribution: {{ results.calculate_risk_scores | group_by('risk_category') }}\n\n        Include regulatory compliance status and any required actions.",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 1073,
    "line_end": 1078,
    "type": "text",
    "description": "Build a pipeline that processes e-commerce data:",
    "content": "Your challenge:\n- Extract: Orders, customers, products, reviews\n- Transform: Calculate metrics, segment customers\n- Load: Create analytics-ready datasets\n- Quality: Validate business rules",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 1086,
    "line_end": 1091,
    "type": "text",
    "description": "Create a pipeline for IoT sensor data:",
    "content": "Requirements:\n- Handle high-volume time series data\n- Detect sensor anomalies\n- Aggregate by time windows\n- Generate maintenance alerts",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_data_processing.rst",
    "line_start": 1099,
    "line_end": 1104,
    "type": "yaml",
    "description": "Build a social media data processing pipeline:",
    "content": "# Features:\n# - Extract from multiple platforms\n# - Text analysis and sentiment\n# - Trend detection\n# - Influence measurement",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 36,
    "line_end": 91,
    "type": "yaml",
    "description": "Create a file called ``web_search.yaml``:",
    "content": "name: basic-web-search\ndescription: Search the web and compile results into a report\n\ninputs:\n  query:\n    type: string\n    description: \"Search query\"\n    required: true\n\n  max_results:\n    type: integer\n    description: \"Maximum number of results to return\"\n    default: 10\n    validation:\n      min: 1\n      max: 50\n\noutputs:\n  report:\n    type: string\n    value: \"search_results_{{ inputs.query | slugify }}.md\"\n\nsteps:\n  # Search the web\n  - id: search\n    action: search_web\n    parameters:\n      query: \"{{ inputs.query }}\"\n      max_results: \"{{ inputs.max_results }}\"\n      include_snippets: true\n\n  # Compile into markdown report\n  - id: compile_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Create a well-organized markdown report from these search results:\n\n        {{ results.search | json }}\n\n        Include:\n        - Executive summary\n        - Key findings\n        - Source links\n        - Relevant details from each result\n\n      style: \"professional\"\n      format: \"markdown\"\n\n  # Save the report\n  - id: save_report\n    action: write_file\n    parameters:\n      path: \"{{ outputs.report }}\"\n      content: \"$results.compile_report\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 97,
    "line_end": 117,
    "type": "python",
    "description": "------------------------",
    "content": "import orchestrator as orc\n\n# Initialize\norc.init_models()\n\n# Compile and run\npipeline = orc.compile(\"web_search.yaml\")\n\n# Search for different topics\nresult1 = pipeline.run(\n    query=\"artificial intelligence trends 2024\",\n    max_results=15\n)\n\nresult2 = pipeline.run(\n    query=\"sustainable energy solutions\",\n    max_results=10\n)\n\nprint(f\"Generated reports: {result1}, {result2}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 125,
    "line_end": 142,
    "type": "markdown",
    "description": "Your pipeline will create markdown files like:",
    "content": "# Search Results: Artificial Intelligence Trends 2024\n\n## Executive Summary\n\nRecent searches reveal significant developments in AI across multiple domains...\n\n## Key Findings\n\n1. **Large Language Models** - Continued advancement in reasoning capabilities\n2. **AI Safety** - Increased focus on alignment and control\n3. **Enterprise Adoption** - Growing integration in business processes\n\n## Detailed Results\n\n### 1. AI Breakthrough: New Model Achieves Human-Level Performance\n**Source**: [TechCrunch](https://techcrunch.com/...)\n**Summary**: Details about the latest AI advancement...",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 155,
    "line_end": 346,
    "type": "yaml",
    "description": "Create ``multi_source_research.yaml``:",
    "content": "name: multi-source-research\ndescription: Comprehensive research using web, news, and academic sources\n\ninputs:\n  topic:\n    type: string\n    required: true\n\n  depth:\n    type: string\n    description: \"Research depth\"\n    default: \"medium\"\n    validation:\n      enum: [\"light\", \"medium\", \"deep\"]\n\n  include_sources:\n    type: array\n    description: \"Sources to include\"\n    default: [\"web\", \"news\", \"academic\"]\n    validation:\n      enum_items: [\"web\", \"news\", \"academic\", \"patents\"]\n\noutputs:\n  comprehensive_report:\n    type: string\n    value: \"research/{{ inputs.topic | slugify }}_comprehensive.md\"\n\n  data_file:\n    type: string\n    value: \"research/{{ inputs.topic | slugify }}_data.json\"\n\n# Research depth configuration\nconfig:\n  research_params:\n    light:\n      web_results: 10\n      news_results: 5\n      academic_results: 3\n    medium:\n      web_results: 20\n      news_results: 10\n      academic_results: 8\n    deep:\n      web_results: 40\n      news_results: 20\n      academic_results: 15\n\nsteps:\n  # Parallel search across sources\n  - id: search_sources\n    parallel:\n      # Web search\n      - id: web_search\n        condition: \"'web' in inputs.include_sources\"\n        action: search_web\n        parameters:\n          query: \"{{ inputs.topic }} comprehensive overview\"\n          max_results: \"{{ config.research_params[inputs.depth].web_results }}\"\n          include_snippets: true\n\n      # News search\n      - id: news_search\n        condition: \"'news' in inputs.include_sources\"\n        action: search_news\n        parameters:\n          query: \"{{ inputs.topic }}\"\n          max_results: \"{{ config.research_params[inputs.depth].news_results }}\"\n          date_range: \"last_month\"\n\n      # Academic search\n      - id: academic_search\n        condition: \"'academic' in inputs.include_sources\"\n        action: search_academic\n        parameters:\n          query: \"{{ inputs.topic }}\"\n          max_results: \"{{ config.research_params[inputs.depth].academic_results }}\"\n          year_range: \"2020-2024\"\n          peer_reviewed: true\n\n  # Extract key information from each source\n  - id: extract_information\n    action: extract_information\n    parameters:\n      content: \"$results.search_sources\"\n      extract:\n        key_facts:\n          description: \"Important facts and findings\"\n        statistics:\n          description: \"Numerical data and metrics\"\n        expert_opinions:\n          description: \"Quotes and opinions from experts\"\n        trends:\n          description: \"Emerging trends and developments\"\n        challenges:\n          description: \"Problems and challenges mentioned\"\n        opportunities:\n          description: \"Opportunities and potential solutions\"\n\n  # Cross-validate information\n  - id: validate_facts\n    action: validate_data\n    parameters:\n      data: \"$results.extract_information\"\n      rules:\n        - name: \"source_diversity\"\n          condition: \"count(unique(sources)) >= 2\"\n          severity: \"warning\"\n          message: \"Information should be confirmed by multiple sources\"\n\n        - name: \"recent_information\"\n          field: \"date\"\n          condition: \"date_diff(value, today()) <= 365\"\n          severity: \"info\"\n          message: \"Information is from the last year\"\n\n  # Generate comprehensive analysis\n  - id: analyze_findings\n    action: generate_content\n    parameters:\n      prompt: |\n        Analyze the following research data about {{ inputs.topic }}:\n\n        {{ results.extract_information | json }}\n\n        Provide:\n        1. Current state analysis\n        2. Key trends identification\n        3. Challenge assessment\n        4. Future outlook\n        5. Recommendations\n\n        Base your analysis on the evidence provided and note any limitations.\n\n      style: \"analytical\"\n      max_tokens: 2000\n\n  # Create structured data export\n  - id: export_data\n    action: transform_data\n    parameters:\n      data:\n        topic: \"{{ inputs.topic }}\"\n        research_date: \"{{ execution.timestamp }}\"\n        depth: \"{{ inputs.depth }}\"\n        sources_used: \"{{ inputs.include_sources }}\"\n        extracted_info: \"$results.extract_information\"\n        validation_results: \"$results.validate_facts\"\n        analysis: \"$results.analyze_findings\"\n      operations:\n        - type: \"convert_format\"\n          to_format: \"json\"\n\n  # Save structured data\n  - id: save_data\n    action: write_file\n    parameters:\n      path: \"{{ outputs.data_file }}\"\n      content: \"$results.export_data\"\n\n  # Generate final report\n  - id: create_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Create a comprehensive research report about {{ inputs.topic }} using:\n\n        Analysis: {{ results.analyze_findings }}\n\n        Structure the report with:\n        1. Executive Summary\n        2. Methodology\n        3. Current State Analysis\n        4. Key Findings\n        5. Trends and Developments\n        6. Challenges and Limitations\n        7. Future Outlook\n        8. Recommendations\n        9. Sources and References\n\n        Include confidence levels for major claims.\n\n      style: \"professional\"\n      format: \"markdown\"\n      max_tokens: 3000\n\n  # Save final report\n  - id: save_report\n    action: write_file\n    parameters:\n      path: \"{{ outputs.comprehensive_report }}\"\n      content: \"$results.create_report\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 352,
    "line_end": 375,
    "type": "python",
    "description": "---------------------------------",
    "content": "import orchestrator as orc\n\n# Initialize\norc.init_models()\n\n# Compile pipeline\npipeline = orc.compile(\"multi_source_research.yaml\")\n\n# Run deep research on quantum computing\nresult = pipeline.run(\n    topic=\"quantum computing applications\",\n    depth=\"deep\",\n    include_sources=[\"web\", \"academic\", \"news\"]\n)\n\nprint(f\"Research complete: {result}\")\n\n# Run lighter research on emerging tech\nresult2 = pipeline.run(\n    topic=\"edge computing trends\",\n    depth=\"medium\",\n    include_sources=[\"web\", \"news\"]\n)",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 388,
    "line_end": 488,
    "type": "yaml",
    "description": "Create ``fact_checker.yaml``:",
    "content": "name: fact-checker\ndescription: Verify claims against multiple reliable sources\n\ninputs:\n  claims:\n    type: array\n    description: \"Claims to verify\"\n    required: true\n\n  confidence_threshold:\n    type: float\n    description: \"Minimum confidence level to accept claims\"\n    default: 0.7\n    validation:\n      min: 0.0\n      max: 1.0\n\noutputs:\n  fact_check_report:\n    type: string\n    value: \"fact_check_{{ execution.timestamp | strftime('%Y%m%d_%H%M') }}.md\"\n\nsteps:\n  # Research each claim\n  - id: research_claims\n    for_each: \"{{ inputs.claims }}\"\n    as: claim\n    action: search_web\n    parameters:\n      query: \"{{ claim }} verification facts evidence\"\n      max_results: 15\n      include_snippets: true\n\n  # Extract supporting/contradicting evidence\n  - id: analyze_evidence\n    for_each: \"{{ inputs.claims }}\"\n    as: claim\n    action: extract_information\n    parameters:\n      content: \"$results.research_claims[loop.index0]\"\n      extract:\n        supporting_evidence:\n          description: \"Evidence that supports the claim\"\n        contradicting_evidence:\n          description: \"Evidence that contradicts the claim\"\n        source_credibility:\n          description: \"Assessment of source reliability\"\n        expert_opinions:\n          description: \"Expert statements about the claim\"\n\n  # Assess credibility of each claim\n  - id: assess_claims\n    for_each: \"{{ inputs.claims }}\"\n    as: claim\n    action: generate_content\n    parameters:\n      prompt: |\n        Assess the veracity of this claim: \"{{ claim }}\"\n\n        Based on the evidence:\n        {{ results.analyze_evidence[loop.index0] | json }}\n\n        Provide:\n        1. Verdict: True/False/Partially True/Insufficient Evidence\n        2. Confidence level (0-1)\n        3. Supporting evidence summary\n        4. Contradicting evidence summary\n        5. Overall assessment\n\n        Be objective and cite specific sources.\n\n      style: \"analytical\"\n      format: \"structured\"\n\n  # Compile fact-check report\n  - id: create_fact_check_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Create a comprehensive fact-check report based on:\n\n        Claims assessed: {{ inputs.claims | json }}\n        Assessment results: {{ results.assess_claims | json }}\n\n        Format as a professional fact-checking article with:\n        1. Summary of findings\n        2. Individual claim assessments\n        3. Methodology used\n        4. Sources consulted\n        5. Limitations and caveats\n\n      style: \"journalistic\"\n      format: \"markdown\"\n\n  # Save report\n  - id: save_fact_check\n    action: write_file\n    parameters:\n      path: \"{{ outputs.fact_check_report }}\"\n      content: \"$results.create_fact_check_report\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 494,
    "line_end": 514,
    "type": "python",
    "description": "----------------------------",
    "content": "import orchestrator as orc\n\n# Initialize\norc.init_models()\n\n# Compile fact-checker\nfact_checker = orc.compile(\"fact_checker.yaml\")\n\n# Check various claims\nresult = fact_checker.run(\n    claims=[\n        \"Electric vehicles produce zero emissions\",\n        \"AI will replace 50% of jobs by 2030\",\n        \"Quantum computers can break all current encryption\",\n        \"Renewable energy is now cheaper than fossil fuels\"\n    ],\n    confidence_threshold=0.8\n)\n\nprint(f\"Fact-check report: {result}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 527,
    "line_end": 805,
    "type": "yaml",
    "description": "Create ``report_generator.yaml``:",
    "content": "name: automated-report-generator\ndescription: Generate professional reports from research data\n\ninputs:\n  topic:\n    type: string\n    required: true\n\n  report_type:\n    type: string\n    description: \"Type of report to generate\"\n    default: \"standard\"\n    validation:\n      enum: [\"executive\", \"technical\", \"standard\", \"briefing\"]\n\n  target_audience:\n    type: string\n    description: \"Primary audience for the report\"\n    default: \"general\"\n    validation:\n      enum: [\"executives\", \"technical\", \"general\", \"academic\"]\n\n  sections:\n    type: array\n    description: \"Sections to include in report\"\n    default: [\"summary\", \"introduction\", \"analysis\", \"conclusion\"]\n\noutputs:\n  report_markdown:\n    type: string\n    value: \"reports/{{ inputs.topic | slugify }}_{{ inputs.report_type }}.md\"\n\n  report_pdf:\n    type: string\n    value: \"reports/{{ inputs.topic | slugify }}_{{ inputs.report_type }}.pdf\"\n\n  report_html:\n    type: string\n    value: \"reports/{{ inputs.topic | slugify }}_{{ inputs.report_type }}.html\"\n\n# Report templates by type\nconfig:\n  report_templates:\n    executive:\n      style: \"executive\"\n      length: \"concise\"\n      focus: \"strategic\"\n      sections: [\"executive_summary\", \"key_findings\", \"recommendations\", \"appendix\"]\n\n    technical:\n      style: \"technical\"\n      length: \"detailed\"\n      focus: \"implementation\"\n      sections: [\"introduction\", \"technical_analysis\", \"methodology\", \"results\", \"conclusion\"]\n\n    standard:\n      style: \"professional\"\n      length: \"medium\"\n      focus: \"comprehensive\"\n      sections: [\"summary\", \"background\", \"analysis\", \"findings\", \"recommendations\"]\n\n    briefing:\n      style: \"concise\"\n      length: \"short\"\n      focus: \"actionable\"\n      sections: [\"situation\", \"assessment\", \"recommendations\"]\n\nsteps:\n  # Gather comprehensive research data\n  - id: research_topic\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }} comprehensive analysis research\"\n      max_results: 25\n      include_snippets: true\n\n  # Get recent news for current context\n  - id: current_context\n    action: search_news\n    parameters:\n      query: \"{{ inputs.topic }}\"\n      max_results: 10\n      date_range: \"last_week\"\n\n  # Extract structured information\n  - id: extract_report_data\n    action: extract_information\n    parameters:\n      content:\n        research: \"$results.research_topic\"\n        news: \"$results.current_context\"\n      extract:\n        key_points:\n          description: \"Main points and findings\"\n        statistics:\n          description: \"Important numbers and data\"\n        trends:\n          description: \"Current and emerging trends\"\n        implications:\n          description: \"Implications and consequences\"\n        expert_views:\n          description: \"Expert opinions and quotes\"\n        future_outlook:\n          description: \"Predictions and future scenarios\"\n\n  # Generate executive summary\n  - id: create_executive_summary\n    condition: \"'summary' in inputs.sections or 'executive_summary' in inputs.sections\"\n    action: generate_content\n    parameters:\n      prompt: |\n        Create an executive summary for {{ inputs.target_audience }} audience about {{ inputs.topic }}.\n\n        Based on: {{ results.extract_report_data.key_points | json }}\n\n        Style: {{ config.report_templates[inputs.report_type].style }}\n        Focus: {{ config.report_templates[inputs.report_type].focus }}\n\n        Include the most critical points in 200-400 words.\n\n      style: \"{{ config.report_templates[inputs.report_type].style }}\"\n      max_tokens: 500\n\n  # Generate introduction/background\n  - id: create_introduction\n    condition: \"'introduction' in inputs.sections or 'background' in inputs.sections\"\n    action: generate_content\n    parameters:\n      prompt: |\n        Write an introduction/background section about {{ inputs.topic }} for {{ inputs.target_audience }}.\n\n        Context: {{ results.extract_report_data | json }}\n\n        Provide necessary background and context for understanding the topic.\n\n      style: \"{{ config.report_templates[inputs.report_type].style }}\"\n      max_tokens: 800\n\n  # Generate main analysis\n  - id: create_analysis\n    condition: \"'analysis' in inputs.sections or 'technical_analysis' in inputs.sections\"\n    action: generate_content\n    parameters:\n      prompt: |\n        Create a comprehensive analysis section about {{ inputs.topic }}.\n\n        Data: {{ results.extract_report_data | json }}\n\n        Style: {{ config.report_templates[inputs.report_type].style }}\n        Audience: {{ inputs.target_audience }}\n\n        Include:\n        - Current state analysis\n        - Trend analysis\n        - Key factors and drivers\n        - Challenges and opportunities\n\n        Support points with specific data and examples.\n\n      style: \"{{ config.report_templates[inputs.report_type].style }}\"\n      max_tokens: 1500\n\n  # Generate findings and implications\n  - id: create_findings\n    condition: \"'findings' in inputs.sections or 'key_findings' in inputs.sections\"\n    action: generate_content\n    parameters:\n      prompt: |\n        Summarize key findings and implications regarding {{ inputs.topic }}.\n\n        Analysis: {{ results.create_analysis }}\n        Supporting data: {{ results.extract_report_data.implications | json }}\n\n        Present clear, actionable findings with implications.\n\n      style: \"{{ config.report_templates[inputs.report_type].style }}\"\n      max_tokens: 1000\n\n  # Generate recommendations\n  - id: create_recommendations\n    condition: \"'recommendations' in inputs.sections\"\n    action: generate_content\n    parameters:\n      prompt: |\n        Develop actionable recommendations based on the analysis of {{ inputs.topic }}.\n\n        Findings: {{ results.create_findings }}\n        Target audience: {{ inputs.target_audience }}\n\n        Provide specific, actionable recommendations with priorities and considerations.\n\n      style: \"{{ config.report_templates[inputs.report_type].style }}\"\n      max_tokens: 800\n\n  # Generate conclusion\n  - id: create_conclusion\n    condition: \"'conclusion' in inputs.sections\"\n    action: generate_content\n    parameters:\n      prompt: |\n        Write a strong conclusion for the {{ inputs.topic }} report.\n\n        Key findings: {{ results.create_findings }}\n        Recommendations: {{ results.create_recommendations }}\n\n        Synthesize the main points and end with a clear call to action.\n\n      style: \"{{ config.report_templates[inputs.report_type].style }}\"\n      max_tokens: 400\n\n  # Assemble complete report\n  - id: assemble_report\n    action: generate_content\n    parameters:\n      prompt: |\n        Compile a complete, professional report about {{ inputs.topic }}.\n\n        Report type: {{ inputs.report_type }}\n        Target audience: {{ inputs.target_audience }}\n\n        Sections to include:\n        {% if results.create_executive_summary %}\n        Executive Summary: {{ results.create_executive_summary }}\n        {% endif %}\n\n        {% if results.create_introduction %}\n        Introduction: {{ results.create_introduction }}\n        {% endif %}\n\n        {% if results.create_analysis %}\n        Analysis: {{ results.create_analysis }}\n        {% endif %}\n\n        {% if results.create_findings %}\n        Findings: {{ results.create_findings }}\n        {% endif %}\n\n        {% if results.create_recommendations %}\n        Recommendations: {{ results.create_recommendations }}\n        {% endif %}\n\n        {% if results.create_conclusion %}\n        Conclusion: {{ results.create_conclusion }}\n        {% endif %}\n\n        Format as a professional markdown document with:\n        - Proper headings and structure\n        - Table of contents\n        - Professional formatting\n        - Source citations where appropriate\n\n      style: \"professional\"\n      format: \"markdown\"\n      max_tokens: 4000\n\n  # Save markdown version\n  - id: save_markdown\n    action: write_file\n    parameters:\n      path: \"{{ outputs.report_markdown }}\"\n      content: \"$results.assemble_report\"\n\n  # Convert to PDF\n  - id: create_pdf\n    action: \"!pandoc {{ outputs.report_markdown }} -o {{ outputs.report_pdf }} --pdf-engine=xelatex\"\n    error_handling:\n      continue_on_error: true\n      fallback:\n        action: write_file\n        parameters:\n          path: \"{{ outputs.report_pdf }}.txt\"\n          content: \"PDF generation requires pandoc with xelatex\"\n\n  # Convert to HTML\n  - id: create_html\n    action: \"!pandoc {{ outputs.report_markdown }} -o {{ outputs.report_html }} --standalone --css=style.css\"\n    error_handling:\n      continue_on_error: true",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 811,
    "line_end": 843,
    "type": "python",
    "description": "------------------------------------",
    "content": "import orchestrator as orc\n\n# Initialize\norc.init_models()\n\n# Compile report generator\ngenerator = orc.compile(\"report_generator.yaml\")\n\n# Generate executive report\nexec_report = generator.run(\n    topic=\"artificial intelligence in healthcare\",\n    report_type=\"executive\",\n    target_audience=\"executives\",\n    sections=[\"executive_summary\", \"key_findings\", \"recommendations\"]\n)\n\n# Generate technical report\ntech_report = generator.run(\n    topic=\"blockchain scalability solutions\",\n    report_type=\"technical\",\n    target_audience=\"technical\",\n    sections=[\"introduction\", \"technical_analysis\", \"methodology\", \"results\"]\n)\n\n# Generate standard briefing\nbriefing = generator.run(\n    topic=\"cybersecurity threats 2024\",\n    report_type=\"briefing\",\n    target_audience=\"general\"\n)\n\nprint(f\"Generated reports: {exec_report}, {tech_report}, {briefing}\")",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 854,
    "line_end": 879,
    "type": "yaml",
    "description": "Create a pipeline that monitors a specific industry for news, updates, and trends:",
    "content": "# Hints for your solution:\ninputs:\n  - name: industry\n    type: string\n    description: \"Industry to monitor\"  # Examples: \"fintech\", \"biotech\", \"cleantech\"\n  - name: monitoring_period\n    type: string\n    description: \"daily\"  # Valid values: \"daily\", \"weekly\", \"monthly\"\n  - name: alert_keywords\n    type: list\n    description: Important terms to watch for\n\nsteps:\n  - id: search_news\n    action: search_web\n    # Multiple search strategies\n  - id: analyze_trends\n    action: analyze\n    # Trend analysis\n  - id: generate_alerts\n    action: filter\n    # Alert generation\n  - id: create_summary\n    action: generate_text\n    # Automated summaries",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 887,
    "line_end": 892,
    "type": "yaml",
    "description": "Build a system that researches competitors and market positioning:",
    "content": "# Structure your pipeline to:\n# 1. Research multiple companies\n# 2. Compare features and positioning\n# 3. Analyze market trends\n# 4. Generate competitive analysis",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/tutorials/tutorial_web_research.rst",
    "line_start": 900,
    "line_end": 907,
    "type": "python",
    "description": "Create a pipeline that combines multiple research pipelines for comprehensive analysis:",
    "content": "# Combine:\n# - Basic web search\n# - Multi-source research\n# - Fact-checking\n# - Report generation\n\n# Into a single meta-pipeline",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 14,
    "line_end": 67,
    "type": "yaml",
    "description": "A complete pipeline definition consists of several sections, each serving a specific purpose:",
    "content": "# Pipeline metadata\nname: pipeline-name           # Required: Unique identifier\ndescription: Pipeline purpose # Required: Human-readable description\nversion: \"1.0.0\"             # Optional: Version tracking\n\n# Input definitions\ninputs:\n  parameter_name:\n    type: string             # Required: string, integer, float, boolean, array, object\n    description: Purpose     # Required: What this input does\n    required: true          # Optional: Default is false\n    default: \"value\"        # Optional: Default value if not provided\n    validation:             # Optional: Input validation rules\n      pattern: \"^[a-z]+$\"   # Regex for strings\n      min: 0                # Minimum for numbers\n      max: 100              # Maximum for numbers\n      enum: [\"a\", \"b\"]      # Allowed values\n\n# Output definitions\noutputs:\n  result_name:\n    type: string            # Required: Output data type\n    value: \"expression\"     # Required: How to generate the output\n    description: Purpose    # Optional: What this output represents\n\n# Configuration\nconfig:\n  timeout: 3600             # Optional: Global timeout in seconds\n  parallel: true            # Optional: Enable parallel execution\n  checkpoint: true          # Optional: Enable checkpointing\n  error_mode: \"continue\"    # Optional: stop|continue|retry\n\n# Resource requirements\nresources:\n  gpu: false                # Optional: Require GPU\n  memory: \"8GB\"             # Optional: Memory requirement\n  model_size: \"large\"       # Optional: Preferred model size\n\n# Pipeline steps\nsteps:\n  - id: step_identifier     # Required: Unique step ID\n    action: action_name     # Required: What to do\n    description: Purpose    # Optional: Step description\n    parameters:             # Optional: Step parameters\n      key: value\n    depends_on: [step_id]   # Optional: Dependencies\n    condition: expression   # Optional: Conditional execution\n    error_handling:         # Optional: Error handling\n      retry:\n        max_attempts: 3\n        backoff: exponential\n      fallback:\n        action: alternate_action",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 78,
    "line_end": 87,
    "type": "yaml",
    "description": "The metadata section identifies and describes your pipeline:",
    "content": "name: advanced-research-pipeline\ndescription: |\n  Multi-stage research pipeline that:\n  - Searches multiple sources\n  - Validates information\n  - Generates comprehensive reports\nversion: \"2.1.0\"\nauthor: \"Your Name\"\ntags: [\"research\", \"automation\", \"reporting\"]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 97,
    "line_end": 148,
    "type": "yaml",
    "description": "**Basic Types**:",
    "content": "inputs:\n  # String input with validation\n  topic:\n    type: string\n    description: \"Research topic to investigate\"\n    required: true\n    validation:\n      pattern: \"^[A-Za-z0-9 ]+$\"\n      min_length: 3\n      max_length: 100\n\n  # Integer with range\n  depth:\n    type: integer\n    description: \"Research depth (1-5)\"\n    default: 3\n    validation:\n      min: 1\n      max: 5\n\n  # Boolean flag\n  include_images:\n    type: boolean\n    description: \"Include images in report\"\n    default: false\n\n  # Array of strings\n  sources:\n    type: array\n    description: \"Preferred information sources\"\n    default: [\"web\", \"academic\"]\n    validation:\n      min_items: 1\n      max_items: 10\n      item_type: string\n\n  # Complex object\n  config:\n    type: object\n    description: \"Advanced configuration\"\n    default:\n      language: \"en\"\n      format: \"pdf\"\n    validation:\n      properties:\n        language:\n          type: string\n          enum: [\"en\", \"es\", \"fr\", \"de\"]\n        format:\n          type: string\n          enum: [\"pdf\", \"html\", \"markdown\"]",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 156,
    "line_end": 184,
    "type": "yaml",
    "description": "Outputs define what the pipeline produces:",
    "content": "outputs:\n  # Simple file output\n  report:\n    type: string\n    value: \"reports/{{ inputs.topic | slugify }}_report.pdf\"\n    description: \"Generated PDF report\"\n\n  # Dynamic output using AUTO\n  summary:\n    type: string\n    value: <AUTO>Generate filename based on content</AUTO>\n    description: \"Executive summary document\"\n\n  # Computed output\n  metrics:\n    type: object\n    value:\n      word_count: \"{{ results.final_report.word_count }}\"\n      sources_used: \"{{ results.compile_sources.count }}\"\n      generation_time: \"{{ execution.duration }}\"\n\n  # Multiple file outputs\n  artifacts:\n    type: array\n    value:\n      - \"{{ outputs.report }}\"\n      - \"data/{{ inputs.topic }}_data.json\"\n      - \"images/{{ inputs.topic }}_charts.png\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 194,
    "line_end": 226,
    "type": "yaml",
    "description": "**Basic Actions**:",
    "content": "steps:\n  # Simple action\n  - id: fetch_data\n    action: fetch_url\n    parameters:\n      url: \"https://api.example.com/data\"\n\n  # Using input values\n  - id: search\n    action: search_web\n    parameters:\n      query: \"{{ inputs.topic }} {{ inputs.year }}\"\n      max_results: \"{{ inputs.depth * 5 }}\"\n\n  # Using previous results\n  - id: analyze\n    action: analyze_data\n    parameters:\n      data: \"$results.fetch_data\"\n      method: \"statistical\"\n\n  # Shell command (prefix with !)\n  - id: convert\n    action: \"!pandoc -f markdown -t pdf -o output.pdf input.md\"\n\n  # Using AUTO tags\n  - id: summarize\n    action: generate_summary\n    parameters:\n      content: \"$results.analyze\"\n      style: <AUTO>Choose style based on audience</AUTO>\n      length: <AUTO>Determine optimal length</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 231,
    "line_end": 257,
    "type": "yaml",
    "description": "**Dependencies and Flow Control**:",
    "content": "steps:\n  # Parallel execution (no dependencies)\n  - id: source1\n    action: fetch_source_a\n\n  - id: source2\n    action: fetch_source_b\n\n  # Sequential execution\n  - id: combine\n    action: merge_data\n    depends_on: [source1, source2]\n    parameters:\n      data1: \"$results.source1\"\n      data2: \"$results.source2\"\n\n  # Conditional execution\n  - id: premium_analysis\n    action: advanced_analysis\n    condition: \"{{ inputs.tier == 'premium' }}\"\n    parameters:\n      data: \"$results.combine\"\n\n  # Dynamic dependencies\n  - id: final_step\n    depends_on: \"{{ ['combine', 'premium_analysis'] if inputs.tier == 'premium' else ['combine'] }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 262,
    "line_end": 284,
    "type": "yaml",
    "description": "**Error Handling**:",
    "content": "steps:\n  - id: risky_operation\n    action: external_api_call\n    error_handling:\n      # Retry configuration\n      retry:\n        max_attempts: 3\n        backoff: exponential  # or: constant, linear\n        initial_delay: 1000   # milliseconds\n        max_delay: 30000\n\n      # Fallback action\n      fallback:\n        action: use_cached_data\n        parameters:\n          cache_key: \"{{ inputs.topic }}\"\n\n      # Continue on error\n      continue_on_error: true\n\n      # Custom error message\n      error_message: \"Failed to fetch external data, using cache\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 296,
    "line_end": 335,
    "type": "yaml",
    "description": "Variables can be referenced throughout your pipeline using Jinja2-style template expressions:",
    "content": "id: variable_demo\nname: Variable Access Demo\n\ninputs:\n  - name: user_topic\n    type: string\n    description: Topic to research\n\nsteps:\n  - id: initial_search\n    action: search_web\n    parameters:\n      # Reference input variables\n      query: \"{{ user_topic }} latest news\"\n\n  - id: analyze_results\n    action: analyze\n    parameters:\n      # Reference results from previous steps\n      data: \"{{ initial_search.results }}\"\n      # Can access nested fields\n      first_result: \"{{ initial_search.results[0].title }}\"\n    dependencies: [initial_search]\n\n  - id: final_report\n    action: generate_text\n    parameters:\n      # Combine multiple references\n      prompt: |\n        Create a report about {{ user_topic }}\n        Based on: {{ analyze_results.summary }}\n        Total results found: {{ initial_search.count }}\n    dependencies: [analyze_results]\n\noutputs:\n  # Define output variables\n  report: \"{{ final_report.result }}\"\n  summary: \"{{ analyze_results.summary }}\"\n  search_count: \"{{ initial_search.count }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 340,
    "line_end": 359,
    "type": "text",
    "description": "**Filters and Functions**:",
    "content": "# String manipulation\n\"{{ inputs.topic | lower }}\"\n\"{{ inputs.topic | upper }}\"\n\"{{ inputs.topic | slugify }}\"\n\"{{ inputs.topic | replace(' ', '_') }}\"\n\n# Date formatting\n\"{{ execution.timestamp | strftime('%Y-%m-%d') }}\"\n\n# Math operations\n\"{{ inputs.count * 2 }}\"\n\"{{ inputs.value | round(2) }}\"\n\n# Conditionals\n\"{{ 'premium' if inputs.tier == 'gold' else 'standard' }}\"\n\n# Lists and loops\n\"{{ inputs.items | join(', ') }}\"\n\"{{ inputs.sources | length }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 369,
    "line_end": 380,
    "type": "yaml",
    "description": "**Basic AUTO Tags**:",
    "content": "parameters:\n  # Simple decision\n  style: <AUTO>Choose appropriate writing style</AUTO>\n\n  # Context-aware decision\n  method: <AUTO>Based on the data type {{ results.fetch.type }}, choose the best analysis method</AUTO>\n\n  # Multiple choices\n  options:\n    visualization: <AUTO>Should we create visualizations for this data?</AUTO>\n    format: <AUTO>What's the best output format: json, csv, or parquet?</AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 385,
    "line_end": 406,
    "type": "yaml",
    "description": "**Advanced AUTO Patterns**:",
    "content": "# Conditional AUTO\nanalysis_depth: |\n  <AUTO>\n  Given:\n  - Data size: {{ results.fetch.size }}\n  - Time constraint: {{ inputs.deadline }}\n  - Importance: {{ inputs.priority }}\n\n  Determine the appropriate analysis depth (1-10)\n  </AUTO>\n\n# Structured AUTO\nreport_sections: |\n  <AUTO>\n  For a report about {{ inputs.topic }}, determine which sections to include:\n  - Executive Summary: yes/no\n  - Technical Details: yes/no\n  - Future Outlook: yes/no\n  - Recommendations: yes/no\n  Return as JSON object\n  </AUTO>",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 425,
    "line_end": 446,
    "type": "python",
    "description": "**User Control Points**:",
    "content": "import orchestrator as orc\n\n# Control compilation options\npipeline = orc.compile(\n    \"pipeline.yaml\",\n    # Override config values\n    config={\n        \"timeout\": 7200,\n        \"checkpoint\": True\n    },\n    # Set compilation flags\n    strict=True,           # Strict validation\n    optimize=True,         # Enable optimizations\n    dry_run=False,         # Actually compile (not just validate)\n    debug=True            # Include debug information\n)\n\n# Inspect compilation result\nprint(pipeline.get_required_tools())\nprint(pipeline.get_task_graph())\nprint(pipeline.get_estimated_cost())",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 451,
    "line_end": 461,
    "type": "yaml",
    "description": "**Runtime vs Compile-Time Resolution**:",
    "content": "# Compile-time (resolved during compilation)\nconfig:\n  timestamp: \"{{ compile_time.timestamp }}\"\n\n# Runtime (resolved during execution)\nsteps:\n  - id: dynamic\n    parameters:\n      query: \"{{ inputs.topic }}\"  # Runtime\n      results: \"$results.previous\"  # Runtime",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 472,
    "line_end": 491,
    "type": "yaml",
    "description": "Reuse common patterns:",
    "content": "imports:\n  # Import specific steps\n  - common/data_validation.yaml#validate_step as validate\n\n  # Import entire pipeline\n  - workflows/standard_analysis.yaml as analysis\n\nsteps:\n  # Use imported step\n  - id: validation\n    extends: validate\n    parameters:\n      data: \"$results.fetch\"\n\n  # Use imported pipeline\n  - id: analyze\n    pipeline: analysis\n    inputs:\n      data: \"$results.validation\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 497,
    "line_end": 525,
    "type": "yaml",
    "description": "-------------------------",
    "content": "steps:\n  # Define parallel group\n  - id: parallel_fetch\n    parallel:\n      - id: fetch_api\n        action: fetch_url\n        parameters:\n          url: \"{{ inputs.api_url }}\"\n\n      - id: fetch_db\n        action: query_database\n        parameters:\n          query: \"{{ inputs.db_query }}\"\n\n      - id: fetch_file\n        action: read_file\n        parameters:\n          path: \"{{ inputs.file_path }}\"\n\n  # Use results from parallel group\n  - id: merge\n    action: combine_data\n    depends_on: [parallel_fetch]\n    parameters:\n      sources:\n        - \"$results.parallel_fetch.fetch_api\"\n        - \"$results.parallel_fetch.fetch_db\"\n        - \"$results.parallel_fetch.fetch_file\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 531,
    "line_end": 548,
    "type": "yaml",
    "description": "-------------------",
    "content": "steps:\n  # For-each loop\n  - id: process_items\n    for_each: \"{{ inputs.items }}\"\n    as: item\n    action: process_single_item\n    parameters:\n      data: \"{{ item }}\"\n      index: \"{{ loop.index }}\"\n\n  # While loop\n  - id: iterative_refinement\n    while: \"{{ results.quality_check.score < 0.95 }}\"\n    max_iterations: 10\n    action: refine_result\n    parameters:\n      current: \"$results.previous_iteration\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 554,
    "line_end": 568,
    "type": "yaml",
    "description": "----------------",
    "content": "# Enable checkpointing\nconfig:\n  checkpoint:\n    enabled: true\n    frequency: \"after_each_step\"  # or: \"every_n_steps: 5\"\n    storage: \"postgresql\"         # or: \"redis\", \"filesystem\"\n\nsteps:\n  - id: long_running\n    action: expensive_computation\n    checkpoint: true  # Force checkpoint after this step\n    recovery:\n      strategy: \"retry\"  # or: \"skip\", \"use_cached\"\n      max_attempts: 3",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 605,
    "line_end": 665,
    "type": "yaml",
    "description": "------------------------",
    "content": "name: data-processing-pipeline\ndescription: ETL pipeline with validation\n\ninputs:\n  source_url:\n    type: string\n    required: true\n\n  output_format:\n    type: string\n    default: \"parquet\"\n    validation:\n      enum: [\"csv\", \"json\", \"parquet\"]\n\nsteps:\n  # Extract\n  - id: extract\n    action: fetch_data\n    parameters:\n      url: \"{{ inputs.source_url }}\"\n      format: <AUTO>Detect format from URL</AUTO>\n\n  # Transform\n  - id: clean\n    action: clean_data\n    parameters:\n      data: \"$results.extract\"\n      rules:\n        - remove_duplicates: true\n        - handle_missing: \"interpolate\"\n        - standardize_dates: true\n\n  - id: transform\n    action: transform_data\n    parameters:\n      data: \"$results.clean\"\n      operations:\n        - type: \"aggregate\"\n          group_by: [\"category\"]\n          metrics: [\"sum\", \"avg\"]\n\n  # Load\n  - id: validate\n    action: validate_data\n    parameters:\n      data: \"$results.transform\"\n      schema:\n        type: \"dataframe\"\n        columns:\n          - name: \"category\"\n            type: \"string\"\n          - name: \"total\"\n            type: \"float\"\n\n  - id: save\n    action: save_data\n    parameters:\n      data: \"$results.validate\"\n      path: \"output/processed_data.{{ inputs.output_format }}\"\n      format: \"{{ inputs.output_format }}\"",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  },
  {
    "file": "docs_sphinx/yaml_pipelines.rst",
    "line_start": 671,
    "line_end": 730,
    "type": "yaml",
    "description": "------------------------------",
    "content": "name: comprehensive-research\ndescription: Research from multiple sources with cross-validation\n\ninputs:\n  topic:\n    type: string\n    required: true\n\n  sources:\n    type: array\n    default: [\"web\", \"academic\", \"news\"]\n\nsteps:\n  # Parallel source fetching\n  - id: fetch_sources\n    parallel:\n      - id: web_search\n        condition: \"'web' in inputs.sources\"\n        action: search_web\n        parameters:\n          query: \"{{ inputs.topic }}\"\n          max_results: 20\n\n      - id: academic_search\n        condition: \"'academic' in inputs.sources\"\n        action: search_academic\n        parameters:\n          query: \"{{ inputs.topic }}\"\n          databases: [\"arxiv\", \"pubmed\", \"scholar\"]\n\n      - id: news_search\n        condition: \"'news' in inputs.sources\"\n        action: search_news\n        parameters:\n          query: \"{{ inputs.topic }}\"\n          date_range: \"last_30_days\"\n\n  # Process and validate\n  - id: extract_facts\n    action: extract_information\n    parameters:\n      sources: \"$results.fetch_sources\"\n      extract:\n        - facts\n        - claims\n        - statistics\n\n  - id: cross_validate\n    action: validate_claims\n    parameters:\n      claims: \"$results.extract_facts.claims\"\n      require_sources: 2  # Need 2+ sources to confirm\n\n  # Generate report\n  - id: synthesize\n    action: generate_synthesis\n    parameters:\n      validated_facts: \"$results.cross_validate\"\n      style: \"analytical\"\n      include_confidence: true",
    "test_status": "unverified",
    "test_file": "",
    "notes": ""
  }
]